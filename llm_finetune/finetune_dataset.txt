Roland Abou Younes is an accomplished engineering manager with a rich background spanning over 24 years in technology, particularly in tech management and software development. Over the past two years, Roland has developed a deep interest in Generative AI and Python, culminating in the development of an innovative open-source solution named Nur.
**Nur** is a self-actualizing documentation chatbot designed to address the challenges of scattered and inconsistent corporate knowledge. Developed by Roland, Nur leverages AI to interact with documentation in a conversational manner, healing its knowledge gaps as naturally as a ray of light. This advanced system integrates smoothly with Slack and Confluence, providing a dynamic platform where documentation can evolve through user interactions.
The system is particularly adept at identifying knowledge gaps within existing documentation and engages users on Slack to fill these gaps effectively. It not only asks questions and captures feedback but also summarizes conversations and updates Confluence documents automatically. Future enhancements for Nur include gamification elements to further engage users, advanced knowledge gap identification, and the integration of AI-driven content management to optimize storage and retrieval processes.
Roland's vision for Nur is to create an environment where documentation is not just a static repository of information but a living, evolving dialogue that grows with the contributions of its community. This approach not only improves the accessibility and reliability of information but also adapts to the diverse needs of its users, including those with neuro-divergent conditions, by making the documentation more understandable and engaging.
Nur Feature List
Nur learns your documentation and chat
Nur joins your team in your chat application
Nur answers questions in public channels and private conversation
Nur integrates in your process
Nur keeps track of information missing in your documentation
Upon request, Nur triggers a quiz to collect the missing information
Nur rewards people who engage in the process:
Asking questions
Contributing to growing the knowledge
If you mark a conversation as important, Nur will learn its content and use it to answer questions.
Nur can integrate with any documentation system and communication tool, we tested it extensively with Confluence and Slack and will continue to support more integrations.
Contact us soon so we can prioritize integrating your tech stack.
Nur: Your Dynamic Documentation and Knowledge Assistant
Empowering Your Documentation: Nur doesn't just learn from your existing documentation; it becomes an active participant in its evolution. By seamlessly integrating with your knowledge bases, Nur ensures that the wealth of information your team has accumulated is always at your fingertips, ready to be accessed and expanded upon.
Integrating with Team Communications: Nur is more than a silent observer; it actively engages in your team chats, both public and private. Imagine having a knowledgeable assistant ready to provide answers directly from your documentation, facilitating a smoother, more informed collaboration process.
Proactively Identifying Knowledge Gaps: Nur's intelligence extends to recognizing what's missing in your documentation. It doesn't stop at identifying gaps; it actively seeks to fill them by prompting team members for input, ensuring that your knowledge base remains comprehensive and up-to-date.
Interactive Quizzes for Knowledge Gathering: Nur takes an engaging approach to fill documentation gaps. Through interactive quizzes, it not only identifies missing information but makes the process of contributing knowledge fun and rewarding for the team, encouraging participation and continuous learning.
Rewarding Participation: Every question asked and every piece of knowledge contributed earns recognition. Nur's reward system motivates team members to engage, ask more, and share more, turning the process of building and maintaining a rich knowledge base into a rewarding experience.
Incorporating Valuable Conversations: With Nur, valuable knowledge shared in conversations doesn't evaporate once the chat ends. You can highlight significant interactions, and Nur will assimilate them into the collective knowledge pool, ensuring no insight is lost.
A Note from the Nur Team
As we embark on this journey with Nur, our innovative documentation and knowledge management assistant, we want to take a moment to talk about our current status and plans, especially regarding pricing and service levels.
Our Commitment to You: Nur is currently in its beta phase, a period of active development and learning. During this time, Nur is being maintained in our free time, reflecting our passion for creating a tool that genuinely addresses the challenges of documentation management and team knowledge sharing.
Beta Phase: In recognition of your support and feedback during this phase, we are offering Nur at no cost. This is our way of saying thank you for being early adopters and for helping us shape Nur's future.
Service Levels During Beta: We are committed to providing a reliable and valuable service during this phase. However, as Nur is being maintained on a part-time basis, there might be limitations in real-time support or updates. Rest assured, any such limitations will not compromise the core functionality and value Nur brings to your team.
Special Note on OpenAI Costs: To provide you with the best possible service, Nur leverages the power of OpenAI. During this beta phase and as we move forward, it will be necessary for customers to cover the direct costs associated with their OpenAI usage. This approach ensures that you have complete transparency and control over the services you use and their associated costs. We believe this is the fairest way to deliver exceptional value while also fostering trust and sustainability.
Looking Ahead: As Nur evolves, we anticipate refining our business model to ensure its sustainability and the continuous improvement of the service. This might involve introducing pricing changes or updates to our service levels. Our goal is to create a model that reflects the value Nur provides, is accessible to teams of all sizes, and supports ongoing innovation and enhancement.
Our Promise of Transparency and Flexibility:
Transparency: Any future changes to our pricing or service model will be communicated well in advance, providing clear explanations and justifications.
Flexibility: We understand that changes can affect different users in different ways. We promise to listen to your feedback and consider it carefully as we make decisions about Nur's future.
Open Dialogue: We encourage you to share your thoughts, suggestions, and concerns with us at any time. Your input is invaluable in guiding the direction of Nur.
We are here to build not just a tool but a community around better documentation practices. Your trust and support mean everything to us, and we are excited to see where this journey takes us together.
Thank you for being part of Nur's early days. Let's make documentation a seamless, integrated part of our work, together.
Warmest regards,
Roland Abou Younes is an accomplished engineering manager with a rich background spanning over 24 years in technology, particularly in tech management and software development. Over the past two years, Roland has developed a deep interest in Generative AI and Python, culminating in the development of an innovative open-source solution named Nur.

Nur is a self-actualizing documentation chatbot designed to address the challenges of scattered and inconsistent corporate knowledge. Developed by Roland, Nur leverages AI to interact with documentation in a conversational manner, healing its knowledge gaps as naturally as a ray of light. This advanced system integrates smoothly with Slack and Confluence, providing a dynamic platform where documentation can evolve through user interactions.

Nur Feature List:

Learning and Integration: Nur not only learns your documentation but becomes an integral part of your team, directly interacting within your chat applications.
Active Participation: It engages actively in both public channels and private conversations, answering questions and providing guidance.
Knowledge Tracking and Quizzes: Nur tracks missing information in your documentation and can trigger quizzes to collect this essential data, making the process interactive and engaging.
Rewarding System: People who ask questions and contribute to knowledge growth are rewarded, fostering a proactive knowledge-sharing environment.
Learning from Conversations: If a conversation is marked important, Nur learns from it and integrates the information to enhance its response accuracy.
Broad Integration Capability: While it has been extensively tested with Confluence and Slack, Nur is designed to integrate with various documentation systems and communication tools.
Empowering Your Documentation:
Nur doesn't just access your existing documentation; it actively contributes to its evolution, ensuring that the collective knowledge is always current, accessible, and actionable.

Integrating with Team Communications:
Nur serves as a proactive participant in your team's communications, offering immediate access to documented knowledge and facilitating more informed discussions.

Proactively Identifying and Filling Knowledge Gaps:
Beyond identifying what's missing, Nur actively gathers the necessary knowledge through interactive quizzes, fostering a culture of continuous improvement and learning.

Rewarding Engagement:
By rewarding every interaction, whether asking questions or providing answers, Nur encourages a more engaged and collaborative team environment.

Incorporating Valuable Conversations into Documentation:
Significant discussions do not go unnoticed; Nur integrates valuable insights from conversations into your knowledge base, ensuring no critical information is lost.

A Note from the Nur Team:
As Nur continues to develop, the team remains committed to transparency and flexibility regarding the evolution of service terms and pricing, particularly as it relates to direct costs from OpenAI usage during this beta phase. Future plans include refining Nur’s business model to ensure sustainability while enhancing the service to meet user needs effectively.

Contact Information:

Roland Abou Younes
Phone
Email
LinkedIn
Join our discussion on Discord
Roland and his team are dedicated to transforming how teams manage and interact with documentation, turning it into a dynamic, evolving tool that grows with the organization.
The mdgrey33/nur repository, specifically on the nur-v0.2 branch, is a complex and multifaceted project that integrates various technologies and programming paradigms to achieve its objectives. Below is a detailed overview of the codebase, based on the information available:

Project Structure
The project is structured into several key directories, each serving a distinct purpose within the overall architecture:

/nurai: This directory appears to be the core of the project, containing the main application logic and components. It is structured in a way that suggests a modular design, with subdirectories for different aspects of the application such as knowledge_sources, migrations, logger, events, users, and chat_services. This organization implies a separation of concerns, where each module handles a specific part of the application's functionality.

/shams_gpt and /open_ai: These directories suggest the integration of AI or machine learning models, possibly for natural language processing or other AI-driven tasks. The presence of a gpt.txt file and directories named after AI services indicates the use of generative pre-trained transformers (GPT) and possibly other OpenAI technologies.

/api and /slack: These directories likely contain the interfaces for external communication. The api directory suggests a set of endpoints for web-based interactions, while the slack directory implies integration with Slack for chat or bot functionalities.

/context and /git: These directories might be used for context management and version control integration within the project, although their specific roles are not clear without further details.

Key Components
Confluence Integration: The project includes specific functionalities for interacting with Confluence, a collaboration software, as indicated by files and directories related to knowledge_sources/confluence. This suggests that the project may be involved in knowledge management or document processing tasks.

AI and Machine Learning: The mention of GPT and embedding within the project indicates a significant focus on AI and machine learning, particularly in natural language processing. This is further supported by the presence of directories and files related to AI models and embeddings.

Chat Services: The integration with Slack and the structure of directories related to chat services suggest that the project includes functionalities for real-time communication, possibly for notifications, alerts, or interactive bots.

Licensing: The project is under the Apache License, Version 2.0, as indicated by the LICENSE file. This choice of license allows for considerable freedom in using, modifying, and distributing the software, making it suitable for both open-source and proprietary applications.

Observations
Modular Design: The project's structure indicates a modular design, allowing for the independent development and scaling of different components. This is a common practice in software engineering to enhance maintainability and facilitate collaboration.

Empty Initialization Files: Several __init__.py files and other directories are noted to be empty. This could indicate placeholders for future development or modular scaffolding that has not yet been fully implemented.

Documentation and Migrations: The presence of a migrations directory and a lack of detailed documentation in some areas suggest that the project is in a state of active development and evolution. Migrations are typically used in projects that rely on databases to manage changes in the database schema over time.

Conclusion
The mdgrey33/nur repository is a sophisticated project that combines modern software engineering practices with advanced AI technologies. Its modular architecture and integration with external services like Confluence and Slack indicate a versatile platform capable of handling complex tasks, particularly in the realm of knowledge management and AI-driven automation. The use of GPT and other machine learning models underscores the project's focus on leveraging cutting-edge AI technologies to fulfill its objectives.

The nurai package and the open_ai/assistants directory within the mdgrey33/nur repository represent two distinct facets of the project, each with its own approach to integrating Slack and Confluence, as well as its overall design and structure. Here's a detailed comparison based on their functionalities, design philosophies, and how they integrate with Slack and Confluence.

nurai Package
Overview
The nurai package is a comprehensive suite designed for managing assistants, chat services, events, knowledge sources, tasks, users, and database operations. It's structured to support a wide range of functionalities within a chat application context, including but not limited to, interactions with Slack and Confluence APIs.

Structure and Design
Modular Design: The package is organized into subdirectories based on functionality, promoting a modular architecture. This design facilitates scalability and maintainability, allowing developers to update or extend specific functionalities without impacting the entire system.
Integration with Slack and Confluence: It includes specific modules for interacting with the Slack API (slack directory) and Confluence API (confluence_integration directory), showcasing a direct approach to integrating these external services. The Slack integration handles bot functionalities, while the Confluence integration focuses on extracting and storing data from Confluence.
Database and Event Handling: The package also emphasizes database operations and event handling, with SQLAlchemy models for database tables and Pydantic models for different types of events, indicating a robust backend infrastructure designed to support complex workflows.
open_ai/assistants Directory
Overview
The open_ai/assistants directory, as part of the proof of concept, focuses on managing AI assistants within the GPT-4-Turbo-Assistant environment. It's tailored to handle interactions with AI models, particularly for generating responses and managing context files.

Structure and Design
Focused Functionality: Unlike the broad scope of nurai, this directory has a more focused aim—managing AI assistants. It includes scripts for creating, listing, loading, updating, and deleting assistants, as well as handling file management operations related to these assistants.
Integration Approach: The integration with Slack and Confluence is not directly handled within this directory but is implied through the functionalities that allow AI assistants to generate responses based on the context, which could include data from Slack and Confluence. The actual integration points are likely managed elsewhere in the project, with this directory focusing on the AI aspect.
Simpler, Proof of Concept Style: The style here is more indicative of a proof of concept, with scripts designed to demonstrate the capabilities of AI assistants rather than a fully-fledged application structure. It's more about showcasing the potential of integrating AI into workflows rather than providing a comprehensive solution.
Differences and Style
Scope and Complexity: nurai is broader in scope and complexity, designed as a more complete solution with a modular architecture. In contrast, open_ai/assistants is more focused and serves as a proof of concept for AI assistant management.
Integration with Slack and Confluence: nurai directly integrates with Slack and Confluence through dedicated modules, indicating a tightly coupled design. The open_ai/assistants directory, however, focuses on AI assistants, with integration points likely abstracted away or handled in conjunction with other parts of the project.
Design Philosophy: The nurai package follows a modular design philosophy, emphasizing separation of concerns and scalability. The open_ai/assistants directory, while also modular to an extent, prioritizes demonstrating the capabilities of AI assistants, suggesting a different focus in its design philosophy.
In summary, the nurai package represents a more mature, modular version of the project, designed to handle a wide range of functionalities including direct integration with Slack and Confluence. The open_ai/assistants directory, on the other hand, serves as a focused proof of concept primarily concerned with AI assistant management, with integration aspects being handled more indirectly or in conjunction with other parts of the project.

The nurai package within the mdgrey33/nur repository is a comprehensive framework designed to facilitate chat services, integrating functionalities such as managing assistants, events, knowledge sources, tasks, users, and database operations. Its structure is modular, with each subdirectory focusing on a specific aspect of the application, promoting a clean separation of concerns and scalability. Here's a breakdown of its structure and the key components:

Structure and Key Components
/nurai: This is the root directory of the nurai package. It likely contains the main package initialization file (__init__.py), which might import essential modules and subpackages, making them accessible to the rest of the application.

/nurai/database: This directory is dedicated to database operations, including the setup and management of database connections using SQLAlchemy, a Python SQL toolkit and Object-Relational Mapping (ORM) library. The presence of a chroma.py file suggests the use of a specific chromadb.HttpClient for database interactions, possibly for a service named "Chroma". The database.py file likely contains the core database connection logic, and the mixins directory provides a CRUDMixin class for CRUD operations, indicating a well-structured approach to database management.

/nurai/chat_services: This directory contains the logic for implementing chat services, with an emphasis on Slack integration. It includes:

An abstract base class ChatServiceInterface that outlines the interface for chat services.
A ChatServiceFactory class for creating instances of chat services based on provided names, showcasing a factory design pattern.
A FastAPI router (routing directory) for starting chat services, indicating the use of FastAPI, a modern, fast web framework for building APIs with Python 3.7+.
The slack subdirectory, which contains the SlackService class implementing the ChatServiceInterface, and additional classes for handling events received from the Slack API. This structure suggests a robust implementation designed to facilitate interactions with Slack, including message and reaction handling.
/nurai/openai: Although the __init__.py file in this directory is empty, suggesting it might be a placeholder or a namespace package, the presence of this directory indicates integration with OpenAI services, possibly for leveraging AI models like GPT for chatbot functionalities.

Design Philosophy
The nurai package demonstrates a modular and scalable design philosophy, with clear separation between different functionalities. This structure facilitates maintenance and future development, allowing for easy extension or modification of individual components without affecting the entire system. The use of modern technologies and design patterns, such as FastAPI for routing and the factory pattern for service instantiation, further underscores the package's commitment to robust, scalable software design.

Integration Points
Slack: The detailed implementation within the chat_services/slack directory, including event handlers and a dedicated service class, highlights a deep integration with Slack, enabling rich interaction capabilities within Slack channels.

Database Operations: The structured approach to database management, with SQLAlchemy models and a CRUD mixin, indicates a solid backend foundation capable of supporting complex data management tasks.

OpenAI: The mention of an openai directory, despite the lack of detailed information, suggests the incorporation of AI functionalities, potentially for natural language processing or automated response generation.

In summary, the nurai package within the mdgrey33/nur repository is a well-structured, modular framework designed to support complex chat service functionalities, with a particular focus on Slack integration and robust database management. Its architecture facilitates easy expansion and maintenance, making it a solid foundation for building scalable chat applications.

The nurai package within the mdgrey33/nur repository provides a structured approach to handling various functionalities through FastAPI routers, focusing on chat services, assistant management, knowledge source integration, and event handling. Below is a summary of the routes available in nurai based on the provided information:

Chat Services (/nurai/chat_services/routing)
Start Chat Service (/start-chat-service/): A POST route that initiates a chat service based on the provided service name. It utilizes the ChatServiceFactory to get and start the specified chat service. If the service name is invalid, it returns a 400 error.

Fetch Chat Thread (/fetch-chat-thread/): A GET route that retrieves messages from a specified chat thread in a service like Slack. It requires the service name, channel ID, and thread timestamp to fetch the thread messages. If successful, it transforms these messages into a structured format for interaction; otherwise, it returns error messages for failure scenarios.

Assistants (/nurai/assistants/routing)
Create Assistant (/assistants/create): A POST route for creating a new assistant. It takes an AssistantCreateRequest object and a database dependency, processes the request through the AssistantCreateController, and returns the response to the client.
Knowledge Sources (/nurai/knowledge_sources/routing)
Load Knowledge from Confluence (/load-knowledge/confluence): A POST route that initiates the background loading of knowledge from Confluence. It takes ConfluenceOptions as input, starts the loading process in the background, and returns a message indicating that processing has started.
Events (/nurai/events/routing)
While specific routes are not detailed in the provided information, the directory is designed to handle different types of events related to a question-and-answer platform. It likely includes routes for processing various event types such as QuestionEvent, FollowUpEvent, ReplyEvent, BotQuestionEvent, CheckmarkEvent, and BookmarkEvent.

Summary
The nurai package offers a comprehensive set of routes catering to different aspects of the application, from managing chat services and assistants to integrating knowledge sources and handling events. Each route is designed with specific functionalities in mind, leveraging FastAPI's capabilities for asynchronous request handling, dependency injection, and data validation to create a robust and scalable backend architecture.

The nurai package within the mdgrey33/nur repository utilizes a combination of SQLAlchemy ORM models for database interactions and Pydantic models for data validation and serialization/deserialization processes. Below is a summary of the models based on the provided information:

SQLAlchemy ORM Models
User Model (/nurai/nurai/users/models/user.py)
Table Name: users
Fields:
id: Integer, Primary Key
name: String(256)
Relationships:
knowledgeSources: Establishes a relationship with the KnowledgeSource model, indicating that a user can have multiple knowledge sources.
Interaction Model (/nurai/nurai/interactions/models/interaction.py)
Table Name: interactions
Fields:
id: Integer, Primary Key
thread_ts: String(255), Indexed
question_text: Text
assistant_thread_id: String(255), Indexed
answer_text: Text
channel_id: String(255)
slack_user_id: String(255)
question_timestamp: DateTime, Default=now()
answer_timestamp: DateTime
comments: Text
Methods:
get_filter_attributes: Returns thread_ts as the unique identifier for an interaction.
Task Model (/nurai/nurai/tasks/models/task.py)
Table Name: tasks
Fields:
id: Integer, Primary Key
status: String(256), Default="waiting"
start_date: DateTime, Default=now()
end_date: DateTime
Methods:
get_filter_attributes: Returns id as the filter attribute.
Assistant Model (/nurai/nurai/assistants/models/assistant.py)
Table Name: assistants
Fields:
id: String(256), Primary Key
created_at: Integer
description: String(256)
file_ids: JSON
instructions: String(256)
external_metadata: JSON
model: String(256)
name: String(256)
object: String(256)
tools: JSON
Methods:
get_filter_attributes: Returns id as the filter attribute.
Knowledge Source Model (/nurai/nurai/knowledge_sources/models/knowledge_source.py)
Table Name: knowledge_source
Fields:
id: Integer, Primary Key
source_type: String(256), Not Nullable
source_external_name: String(256), Not Nullable
user_id: Integer, ForeignKey to users.id
Methods:
get_filter_attributes: Returns user_id and source_type as the filter attributes.
Pydantic Models (/nurai/nurai/events/models)
The events directory contains Pydantic models for different types of events that can occur in a chat application, including:

BaseEventModel
QuestionEvent
FollowUpEvent
ReplyEvent
BotQuestionEvent
ReactionEvent
CheckmarkEvent
BookmarkEvent
These models are designed to provide a structured way to represent and handle different types of events, with fields such as ts, thread_ts, channel, user, text, bot_mention, channel_message, user_id, reaction, and more, depending on the specific event type.

Database Migration Scripts (/nurai/nurai/migrations/versions)
Two migration files are highlighted:

95bd8de6aa22_initialize_the_db.py: Initializes the database by creating tables for tasks, users, knowledge_source, and knowledge_source_item.
d69322653621_add_assistant.py: Adds the assistants table to the database, with fields including id, created_at, description, file_ids, instructions, external_metadata, model, name, object, and tools.
These models and migration scripts collectively form the backbone of the nurai package's data handling and storage mechanisms, providing a robust framework for managing users, interactions, tasks, assistants, and knowledge sources within the application.

The environment and configuration setup for the nurai application within the mdgrey33/nur repository is designed to ensure that the application can be configured to run in different environments smoothly, leveraging environment variables for flexibility and security. Here's a walkthrough of how the environment and configuration packages are structured and used:

settings.py
Located in /nurai/nurai/settings/settings.py, this file defines a Settings class that inherits from BaseSettings, a feature provided by the pydantic_settings package. This class is used to define and automatically load environment variables into the application, ensuring type safety and easy access throughout the codebase.

Properties: The Settings class defines four properties:

CONFLUENCE_TEST_URL: The URL for the Confluence API endpoint.
CONFLUENCE_TEST_USERNAME: The username for authenticating with the Confluence API.
CONFLUENCE_TEST_API_TOKEN: The API token for authenticating with the Confluence API.
OPEN_AI_KEY: The API key for accessing OpenAI services.
Configuration: The inner Config class within Settings specifies that the environment variables should be loaded from a file located at ./.env relative to the settings.py file. This approach centralizes the configuration and makes it easy to manage environment-specific settings.

Caching: The get_settings function, decorated with @lru_cache, ensures that the loaded settings are cached for the lifetime of the application. This optimization prevents the need to reload and parse the environment file on every access, improving performance.

.env.example
The .env.example file serves as a template for the required environment variables. It's a best practice to include such a file in projects to document the necessary environment variables without exposing sensitive information. Users of the application should copy this file to .env and fill in the actual values for their environment.

Contents: This file includes placeholders for the application's host and port (NUR_API_HOST and NUR_API_PORT), among other environment-specific settings. While it doesn't directly include the variables used in settings.py, it serves as a guide for what variables might be needed.
Docker Compose (compose.yml)
The Docker Compose file, compose.yml, further illustrates how environment variables are used to configure services within the application. It defines two services: web and slack, both of which use environment variables to configure the API host and port, among other settings.

Service Configuration: Each service in the Docker Compose file is configured to use specific environment variables, such as NUR_API_HOST and NUR_API_PORT, to determine how the application should run within its container. This setup allows for easy adjustments to the application's runtime environment without modifying the code.

Volume Sharing: The shared volume shared_content between services indicates a design where both the web API and the Slack bot can access and share data, demonstrating the application's modular but integrated architecture.

Summary
The environment and configuration setup in the nurai application is designed to be flexible, secure, and easy to manage. By leveraging pydantic's BaseSettings for type-safe environment variable loading, providing a clear .env.example for documentation, and utilizing Docker Compose for service configuration, the application ensures that it can be easily configured and deployed across different environments.


Git Commits:
+
+
+def get_last_updated_timestamp(page_id):
+    session = Session()
+    page_record = session.query(PageData).filter_by(page_id=page_id).first()
+    session.close()
+
+    if page_record:
+        return page_record.lastUpdated
+    else:
+        return None

c505d914c8885bd167d3931fd215c943a6aa41bb
Author: Roland Abou Younes
Date: Sat Dec 16 00:36:29 2023 +0200
Message: extracted db operations out of the chroma module

diff --git a/database/confluence_database.py b/database/confluence_database.py
index c8542c0..4c583e5 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -3,6 +3,8 @@ from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
 from sqlalchemy.orm import sessionmaker, declarative_base
 from datetime import datetime
 from configuration import sql_file_path
+import sqlite3
+

 # Define the base class for SQLAlchemy models
 Base = declarative_base()
@@ -110,3 +112,37 @@ def store_pages_data(space_key, pages_data):
     session.commit()


+def get_page_data_from_db():
+    # Connect to the SQLite database
+    conn = sqlite3.connect(sql_file_path)
+    cursor = conn.cursor()
+
+    # Retrieve records from the "page_data" table where "lastUpdated" is newer than "last_embedded"
+    cursor.execute("SELECT * FROM page_data WHERE lastUpdated > last_embedded OR last_embedded IS NULL")
+    records = cursor.fetchall()
+
+    # Process each record into a string
+    all_documents = []
+    page_ids = []
+    for record in records:
+        document = (
+            f"Page id: {record[1]}, space key: {record[2]}, title: {record[3]}, "
+            f"author: {record[4]}, created date: {record[5]}, last updated: {record[6]}, "
+            f"content: {record[7]}, comments: {record[8]}"
+        )
+        all_documents.append(document)
+        page_ids.append(record[1])
+
+    # Close the SQLite connection
+    conn.close()
+    return all_documents, page_ids
+
+
+def update_embed_date(page_ids):
+    conn = sqlite3.connect(sql_file_path)
+    cursor = conn.cursor()
+    current_time = datetime.now()
+    for page_id in page_ids:
+        cursor.execute("UPDATE page_data SET last_embedded = ? WHERE page_id = ?", (current_time, page_id))
+    conn.commit()
+    conn.close()
\ No newline at end of file
diff --git a/vector/chroma.py b/vector/chroma.py
index cb88316..569aa59 100644
--- a/vector/chroma.py
+++ b/vector/chroma.py
@@ -1,35 +1,9 @@
-import sqlite3
 from langchain.embeddings.openai import OpenAIEmbeddings
 from langchain.vectorstores import Chroma
 from credentials import oai_api_key
-from configuration import sql_file_path, vector_folder_path
-from datetime import datetime
-
-
-def get_data_from_db():
-    # Connect to the SQLite database
-    conn = sqlite3.connect(sql_file_path)
-    cursor = conn.cursor()
-
-    # Retrieve records from the "page_data" table where "lastUpdated" is newer than "last_embedded"
-    cursor.execute("SELECT * FROM page_data WHERE lastUpdated > last_embedded OR last_embedded IS NULL")
-    records = cursor.fetchall()
-
-    # Process each record into a string
-    all_documents = []
-    page_ids = []
-    for record in records:
-        document = (
-            f"Page id: {record[1]}, space key: {record[2]}, title: {record[3]}, "
-            f"author: {record[4]}, created date: {record[5]}, last updated: {record[6]}, "
-            f"content: {record[7]}, comments: {record[8]}"
-        )
-        all_documents.append(document)
-        page_ids.append(record[1])
-
-    # Close the SQLite connection
-    conn.close()
-    return all_documents, page_ids
+from configuration import vector_folder_path
+from database.confluence_database import get_page_data_from_db
+from database.confluence_database import update_embed_date


 def vectorize_documents(all_documents, page_ids):
@@ -49,17 +23,10 @@ def vectorize_documents(all_documents, page_ids):
     vectordb.persist()

     # Update the last_embedded timestamp in the database
-    conn = sqlite3.connect(sql_file_path)
-    cursor = conn.cursor()
-    current_time = datetime.now()
-    for page_id in page_ids:
-        cursor.execute("UPDATE page_data SET last_embedded = ? WHERE page_id = ?", (current_time, page_id))
-    conn.commit()
-    conn.close()
-
+    update_embed_date(page_ids)

 def add_to_vector():
-    all_documents, page_ids = get_data_from_db()
+    all_documents, page_ids = get_page_data_from_db()

     # Check if the lists are empty
     if not all_documents or not page_ids:

0357178001eab5eb12628ab3424a45eef5bdc525
Author: Roland Abou Younes
Date: Sat Dec 16 00:24:22 2023 +0200
Message: Will add the date the document was retrieved from confluence

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index 07c59a7..445d4ce 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -198,6 +198,7 @@ def get_space_content(update_date=None):
     page_content_map = {}  # For storing page data for database

     for page_id in all_page_ids:
+        current_time = datetime.now()
         page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
         page_title = strip_html_tags(page['title'])
         page_author = page['history']['createdBy']['displayName']
@@ -220,7 +221,8 @@ def get_space_content(update_date=None):
             'createdDate': created_date,
             'lastUpdated': last_updated,
             'content': page_content,
-            'comments': page_comments_content
+            'comments': page_comments_content,
+            'datePulledFromConfluence': current_time
         }

         # Store data for database
diff --git a/database/confluence_database.py b/database/confluence_database.py
index f03587f..c8542c0 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -39,6 +39,7 @@ class PageData(Base):
     content = Column(Text)
     comments = Column(Text)
     last_embedded = Column(DateTime)
+    date_pulled_from_confluence = Column(DateTime)


 # Setup the database engine and create tables if they don't exist
@@ -92,6 +93,7 @@ def store_pages_data(space_key, pages_data):
     for page_id, page_info in pages_data.items():
         created_date = parse_datetime(page_info['createdDate'])
         last_updated = parse_datetime(page_info['lastUpdated'])
+        date_pulled_from_confluence = page_info['datePulledFromConfluence']

         new_page = PageData(page_id=page_id,
                             space_key=space_key,
@@ -100,7 +102,9 @@ def store_pages_data(space_key, pages_data):
                             createdDate=created_date,
                             lastUpdated=last_updated,
                             content=page_info['content'],
-                            comments=page_info['comments'])
+                            comments=page_info['comments'],
+                            date_pulled_from_confluence=date_pulled_from_confluence
+                            )
         session.add(new_page)
         print(f"Page with ID {page_id} written to database")
     session.commit()
diff --git a/vector/chroma.py b/vector/chroma.py
index 7f92e2c..cb88316 100644
--- a/vector/chroma.py
+++ b/vector/chroma.py
@@ -67,6 +67,8 @@ def add_to_vector():
         return []

     vectorize_documents(all_documents, page_ids)
+    print(f'Vectorized {len(all_documents)} documents.')
+    print(f'Vectorized page ids: {page_ids}')
     return page_ids



fb2bfa44a2ab41fe89ea36aa0505bf19ad5827b7
Author: Roland Abou Younes
Date: Fri Dec 15 23:53:50 2023 +0200
Message: Only vectorize a page if it is updated after the last embed date of the specific page.

diff --git a/database/confluence_database.py b/database/confluence_database.py
index bfe4a5f..f03587f 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -1,11 +1,9 @@
 # ./database/confluence_database.py
-from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, LargeBinary
-from sqlalchemy.ext.declarative import declarative_base
-from sqlalchemy.orm import sessionmaker
+from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
+from sqlalchemy.orm import sessionmaker, declarative_base
 from datetime import datetime
 from configuration import sql_file_path

-
 # Define the base class for SQLAlchemy models
 Base = declarative_base()

@@ -40,6 +38,7 @@ class PageData(Base):
     lastUpdated = Column(DateTime)
     content = Column(Text)
     comments = Column(Text)
+    last_embedded = Column(DateTime)


 # Setup the database engine and create tables if they don't exist
diff --git a/database/view_page_data.py b/database/view_page_data.py
index 92a4247..a51af6a 100644
--- a/database/view_page_data.py
+++ b/database/view_page_data.py
@@ -5,22 +5,26 @@ from configuration import sql_file_path

 def display_page_data():
     """
-    Display all records in the "page_data" table of the SQLite database.
+    Display all records in the 'page_data' table of the SQLite database.
     """
     # Connect to the SQLite database
     conn = sqlite3.connect(sql_file_path)
     cursor = conn.cursor()

-    # Retrieve all records from the "page_data" table
+    # Retrieve all records from the 'page_data' table
     cursor.execute("SELECT * FROM page_data")
     records = cursor.fetchall()

     # Create a PrettyTable
     table = PrettyTable()
-    table.field_names = ["ID", "Page ID", "Space Key", "Title", "Author", "Created Date", "Last Updated", "Content", "Comments"]
+    table.field_names = ["ID", "Page ID", "Space Key", "Title", "Author", "Created Date", "Last Updated", "Content", "Comments", "Last Embedded"]

     for record in records:
-        table.add_row(record)
+        formatted_record = list(record)
+        # Assuming createdDate, lastUpdated, and lastEmbedded are already in a readable string format
+        formatted_record[7] = (formatted_record[7][:75] + '...') if formatted_record[7] and len(formatted_record[7]) > 75 else formatted_record[7]  # Content (truncated)
+
+        table.add_row(formatted_record)

     # Close the SQLite connection
     conn.close()
diff --git a/vector/chroma.py b/vector/chroma.py
index 66e8275..7f92e2c 100644
--- a/vector/chroma.py
+++ b/vector/chroma.py
@@ -1,9 +1,9 @@
-# ./vector/chroma.py
 import sqlite3
 from langchain.embeddings.openai import OpenAIEmbeddings
 from langchain.vectorstores import Chroma
 from credentials import oai_api_key
 from configuration import sql_file_path, vector_folder_path
+from datetime import datetime


 def get_data_from_db():
@@ -11,12 +11,13 @@ def get_data_from_db():
     conn = sqlite3.connect(sql_file_path)
     cursor = conn.cursor()

-    # Retrieve all records from the "page_data" table
-    cursor.execute("SELECT * FROM page_data")
+    # Retrieve records from the "page_data" table where "lastUpdated" is newer than "last_embedded"
+    cursor.execute("SELECT * FROM page_data WHERE lastUpdated > last_embedded OR last_embedded IS NULL")
     records = cursor.fetchall()

     # Process each record into a string
     all_documents = []
+    page_ids = []
     for record in records:
         document = (
             f"Page id: {record[1]}, space key: {record[2]}, title: {record[3]}, "
@@ -24,18 +25,14 @@ def get_data_from_db():
             f"content: {record[7]}, comments: {record[8]}"
         )
         all_documents.append(document)
-    page_ids = [record[1] for record in records]
+        page_ids.append(record[1])

     # Close the SQLite connection
     conn.close()
-    print('###################################################')
-    print(page_ids)
-    print('###################################################')
     return all_documents, page_ids


 def vectorize_documents(all_documents, page_ids):
-
     # Initialize OpenAI embeddings with the API key
     embedding = OpenAIEmbeddings(openai_api_key=oai_api_key)

@@ -43,20 +40,32 @@ def vectorize_documents(all_documents, page_ids):
     vectordb = Chroma(embedding_function=embedding, persist_directory=vector_folder_path)

     # Prepare page_ids to be added as metadata
-    metadatas = [
-        {"page_id": page_id} for page_id in page_ids
-    ]
+    metadatas = [{"page_id": page_id} for page_id in page_ids]

     # Add texts to the vectorstore
     vectordb.add_texts(texts=all_documents, metadatas=metadatas)
-    # vectordb.upsert_texts(texts=all_documents, metadatas=metadatas)

     # Persist the database
     vectordb.persist()

+    # Update the last_embedded timestamp in the database
+    conn = sqlite3.connect(sql_file_path)
+    cursor = conn.cursor()
+    current_time = datetime.now()
+    for page_id in page_ids:
+        cursor.execute("UPDATE page_data SET last_embedded = ? WHERE page_id = ?", (current_time, page_id))
+    conn.commit()
+    conn.close()
+

 def add_to_vector():
     all_documents, page_ids = get_data_from_db()
+
+    # Check if the lists are empty
+    if not all_documents or not page_ids:
+        print("No new or updated documents to vectorize.")
+        return []
+
     vectorize_documents(all_documents, page_ids)
     return page_ids

@@ -94,4 +103,3 @@ if __name__ == '__main__':
     for result in relevant_document_ids:
         print(result)
         print("---------------------------------------------------")
-

ef29371c3f716c95f5e46372edada88020de750c
Author: Roland Abou Younes
Date: Fri Dec 15 17:15:01 2023 +0200
Message: moved the print statement to print all ids from within the loop in the database add. Added soaceID and page ID to the page_data

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index 3795132..07c59a7 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -213,6 +213,8 @@ def get_space_content(update_date=None):
             page_comments_content += strip_html_tags(comment_content)

         page_data = {
+            'spaceKey': space_key,
+            'pageId': page_id,
             'title': page_title,
             'author': page_author,
             'createdDate': created_date,
diff --git a/database/confluence_database.py b/database/confluence_database.py
index 4f42a4f..bfe4a5f 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -103,7 +103,7 @@ def store_pages_data(space_key, pages_data):
                             content=page_info['content'],
                             comments=page_info['comments'])
         session.add(new_page)
+        print(f"Page with ID {page_id} written to database")
     session.commit()
-    print(f"Page with ID {page_id} written to database")



e916658ed795b7d6e339c2f79a8e16f6b4f9ecc2
Author: Roland Abou Younes
Date: Fri Dec 15 16:53:55 2023 +0200
Message: Revert "Getting all pages IDs, publishing to pulasar, consuming from pulsar, downloading content and adding to db and file system then aknowledging on pulsar."

diff --git a/configuration.py b/configuration.py
index 3efb943..01f6313 100644
--- a/configuration.py
+++ b/configuration.py
@@ -11,5 +11,3 @@ sql_file_path = database_path + "/confluence_pages_sql.db"
 # Slack Bot User ID
 bot_user_id = "U069C17DCE5"

-# Pulsar client
-pulsar_client_url = "pulsar://localhost:6650"
diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index 9f6d6ee..3795132 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -198,8 +198,6 @@ def get_space_content(update_date=None):
     page_content_map = {}  # For storing page data for database

     for page_id in all_page_ids:
-        # publish IDs to pulsar
-
         page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
         page_title = strip_html_tags(page['title'])
         page_author = page['history']['createdBy']['displayName']
diff --git a/confluence_integration/retrieve_space_pulsar.py b/confluence_integration/retrieve_space_pulsar.py
deleted file mode 100644
index 34b5940..0000000
--- a/confluence_integration/retrieve_space_pulsar.py
+++ /dev/null
@@ -1,235 +0,0 @@
-# ./confluence_integration/retrieve_space.py
-from datetime import datetime
-from bs4 import BeautifulSoup
-from atlassian import Confluence
-from credentials import confluence_credentials
-from database.confluence_database import store_space_data, store_page_data
-from file_system.file_manager import FileManager
-from messaging_service.pulsar_client import PulsarClient
-from configuration import pulsar_client_url
-
-pulsar_client = PulsarClient(pulsar_client_url)
-
-# Initialize Confluence API
-confluence = Confluence(
-    url=confluence_credentials['base_url'],
-    username=confluence_credentials['username'],
-    password=confluence_credentials['api_token']
-)
-
-
-# Get top level pages from a space
-def get_top_level_ids(space_key):
-    """
-    Retrieve IDs of top-level pages in a specified Confluence space.
-
-    Args:
-    space_key (str): The key of the Confluence space.
-
-    Returns:
-    list: A list of page IDs for the top-level pages in the space.
-    """
-    top_level_pages = confluence.get_all_pages_from_space(space_key)
-    return [page['id'] for page in top_level_pages]
-
-
-# Get child pages from a page
-def get_child_ids(item_id, content_type):
-    """
-    Retrieve IDs of child items (pages or comments) for a given Confluence item.
-
-    Args:
-    item_id (str): The ID of the Confluence page or comment.
-    content_type (str): Type of content to retrieve ('page' or 'comment').
-
-    Returns:
-    list: A list of IDs for child items.
-    """
-    child_items = confluence.get_page_child_by_type(item_id, type=content_type)
-    return [child['id'] for child in child_items]
-
-
-def get_all_page_ids_recursive(space_key):
-    def get_child_pages_recursively(page_id):
-        child_pages = []
-        child_page_ids = get_child_ids(page_id, content_type='page')
-        for child_id in child_page_ids:
-            child_pages.append(child_id)
-            child_pages.extend(get_child_pages_recursively(child_id))
-        return child_pages
-
-    all_page_ids = []
-    top_level_ids = get_top_level_ids(space_key)
-    topic_name = "confluence_page_ids"  # Define the topic name here
-
-    for top_level_id in top_level_ids:
-        all_page_ids.append(top_level_id)
-        all_page_ids.extend(get_child_pages_recursively(top_level_id))
-
-    for page_id in all_page_ids:
-        pulsar_client.publish_message(topic_name, str(page_id))  # Include the topic name
-
-    return all_page_ids
-
-
-def get_all_comment_ids_recursive(page_id):
-    """
-    Recursively retrieves all comment IDs for a given Confluence page.
-
-    Args:
-    page_id (str): The ID of the Confluence page.
-
-    Returns:
-    list: A list of all comment IDs for the page.
-    """
-
-    def get_child_comment_ids_recursively(comment_id):
-        # Inner function to recursively get child comment IDs
-        child_comment_ids = []  # Use a separate list to accumulate child comment IDs
-        immediate_child_ids = get_child_ids(comment_id, content_type='comment')
-        for child_id in immediate_child_ids:
-            child_comment_ids.append(child_id)
-            child_comment_ids.extend(get_child_comment_ids_recursively(child_id))
-        return child_comment_ids
-
-    all_comment_ids = []
-    top_level_comment_ids = get_child_ids(page_id, content_type='comment')
-    for comment_id in top_level_comment_ids:
-        all_comment_ids.append(comment_id)
-        all_comment_ids.extend(get_child_comment_ids_recursively(comment_id))
-    return all_comment_ids
-
-
-def choose_space():
-    """
-    Prompt the user to choose a Confluence space from a list of available spaces.
-
-    Returns:
-    str: The key of the chosen Confluence space.
-    """
-    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
-    for i, space in enumerate(spaces['results']):
-        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
-    choice = int(input("Choose a space (number): ")) - 1
-    space_key = spaces['results'][choice]['key']
-    space_data = {'space_key': space_key,
-                  'url': confluence_credentials['base_url'],
-                  'login': confluence_credentials['username'],
-                  'token': confluence_credentials['api_token']
-                  }
-    store_space_data(space_data)
-    return spaces['results'][choice]['key']
-
-
-def strip_html_tags(content):
-    """
-    Remove HTML tags from a string.
-
-    Args:
-    content (str): The string with HTML content.
-
-    Returns:
-    str: The string with HTML tags removed.
-    """
-    soup = BeautifulSoup(content, 'html.parser')
-    return soup.get_text()
-
-
-def check_date_filter(update_date, all_page_ids):
-    """
-    Filter pages based on their last updated date.
-
-    Args:
-    update_date (datetime): The threshold date for filtering. Pages updated after this date will be included.
-    all_page_ids (list): A list of page IDs to be filtered.
-
-    Returns:
-    list: A list of page IDs that were last updated on or after the specified update_date.
-    """
-    updated_pages = []
-    for page_id in all_page_ids:
-        page_history = confluence.history(page_id)  # directly use page_id
-        last_updated = datetime.strptime(page_history['lastUpdated']['when'], '%Y-%m-%dT%H:%M:%S.%fZ')
-        if last_updated >= update_date:
-            updated_pages.append(page_id)  # append the page_id to the list
-    return updated_pages
-
-
-def format_page_content_for_llm(page_data):
-    """
-        Format page data into a string of key-value pairs suitable for LLM (Language Learning Models) context.
-
-        This function converts page data into a text format that can be easily consumed by language models,
-        with each key-value pair on a separate line.
-
-        Args:
-        page_data (dict): A dictionary containing page data with keys like title, author, createdDate, etc.
-
-        Returns:
-        str: A string representation of the page data in key-value format.
-        """
-    content = ""
-    for key, value in page_data.items():
-        content += f"{key}: {value}\n"
-    return content
-
-
-def publish_space_ids(space_key, update_date=None):
-    topic_name = "confluence_page_ids"
-    all_page_ids = get_all_page_ids_recursive(space_key)
-    if update_date is not None:
-        all_page_ids = check_date_filter(update_date, all_page_ids)
-
-    for page_id in all_page_ids:
-        pulsar_client.publish_message(topic_name, str(page_id))
-
-    print("Page IDs published to Pulsar.")
-    return all_page_ids
-
-
-def retrieve_and_process_page_data(page_id, space_key):
-    page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
-    if not page:
-        print(f"No data found for page ID: {page_id}")
-        return
-
-    page_title = page.get('title', '')
-    page_author = page.get('history', {}).get('createdBy', {}).get('displayName', '')
-    created_date = page.get('history', {}).get('createdDate', '')
-    last_updated = page.get('version', {}).get('when', '')
-    page_content = strip_html_tags(page.get('body', {}).get('storage', {}).get('value', ''))
-
-    page_comments_content = ""
-    page_comment_ids = get_all_comment_ids_recursive(page_id)
-    for comment_id in page_comment_ids:
-        comment = confluence.get_page_by_id(comment_id, expand='body.storage')
-        comment_content = strip_html_tags(comment.get('body', {}).get('storage', {}).get('value', ''))
-        page_comments_content += comment_content + '\n'
-
-    page_data = {
-        'title': page_title,
-        'author': page_author,
-        'createdDate': created_date,
-        'lastUpdated': last_updated,
-        'content': page_content,
-        'comments': page_comments_content
-    }
-
-    store_page_data(page_id, space_key, page_data)
-    formatted_content = format_page_content_for_llm(page_data)
-    file_manager = FileManager()
-    file_manager.create(f"{page_id}.txt", formatted_content)
-
-
-def consume_pages_and_store_data(topic_name, subscription_name, space_key):
-    while True:
-        page_id = pulsar_client.consume_message(topic_name, subscription_name)
-        if page_id:
-            retrieve_and_process_page_data(page_id, space_key)
-    pulsar_client.close()
-
-
-if __name__ == "__main__":
-    chosen_space_key = choose_space()
-    publish_space_ids(chosen_space_key)
-    consume_pages_and_store_data("confluence_page_ids", "default_processor", chosen_space_key)
diff --git a/database/confluence_database.py b/database/confluence_database.py
index a112536..4f42a4f 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -1,16 +1,22 @@
-from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
+# ./database/confluence_database.py
+from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, LargeBinary
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import sessionmaker
 from datetime import datetime
 from configuration import sql_file_path

+
 # Define the base class for SQLAlchemy models
 Base = declarative_base()


 # Define the SpaceData model
 class SpaceData(Base):
+    """
+    SQLAlchemy model for storing Confluence space data.
+    """
     __tablename__ = 'space_data'
+
     id = Column(Integer, primary_key=True)
     space_key = Column(String)
     url = Column(String)
@@ -20,7 +26,11 @@ class SpaceData(Base):

 # Define the PageData model
 class PageData(Base):
+    """
+    SQLAlchemy model for storing Confluence page data.
+    """
     __tablename__ = 'page_data'
+
     id = Column(Integer, primary_key=True)
     page_id = Column(String)
     space_key = Column(String)
@@ -39,29 +49,51 @@ Base.metadata.create_all(engine)

 # Create a sessionmaker object to manage database sessions
 Session = sessionmaker(bind=engine)
-
-
-def parse_datetime(date_string):
-    return datetime.fromisoformat(date_string.replace('Z', '+00:00'))
+session = Session()


 def store_space_data(space_data):
-    session = Session()
+    """
+    Store Confluence space data into the database.
+
+    Args:
+    space_data (dict): A dictionary containing space data to store.
+    """
+    # Create a new SpaceData object and add it to the session
     new_space = SpaceData(space_key=space_data['space_key'],
                           url=space_data['url'],
                           login=space_data['login'],
                           token=space_data['token'])
     session.add(new_space)
     session.commit()
-    session.close()
     print(f"Space with key {space_data['space_key']} written to database")


+def parse_datetime(date_string):
+    """
+    Convert an ISO format datetime string to a datetime object.
+
+    Args:
+    date_string (str): ISO format datetime string.
+
+    Returns:
+    datetime: A datetime object.
+    """
+    return datetime.fromisoformat(date_string.replace('Z', '+00:00'))
+
+
 def store_pages_data(space_key, pages_data):
-    session = Session()
+    """
+    Store Confluence page data into the database.
+
+    Args:
+    space_key (str): The key of the Confluence space.
+    pages_data (dict): A dictionary of page data, keyed by page ID.
+    """
     for page_id, page_info in pages_data.items():
         created_date = parse_datetime(page_info['createdDate'])
         last_updated = parse_datetime(page_info['lastUpdated'])
+
         new_page = PageData(page_id=page_id,
                             space_key=space_key,
                             title=page_info['title'],
@@ -72,24 +104,6 @@ def store_pages_data(space_key, pages_data):
                             comments=page_info['comments'])
         session.add(new_page)
     session.commit()
-    session.close()
-    print(f"Pages from space {space_key} written to database")
-
-
-def store_page_data(page_id, space_key, page_data):
-    session = Session()
-    created_date = parse_datetime(page_data['createdDate'])
-    last_updated = parse_datetime(page_data['lastUpdated'])
-    new_page = PageData(page_id=page_id,
-                        space_key=space_key,
-                        title=page_data['title'],
-                        author=page_data['author'],
-                        createdDate=created_date,
-                        lastUpdated=last_updated,
-                        content=page_data['content'],
-                        comments=page_data['comments'])
-
-    session.add(new_page)
-    session.commit()
-    session.close()
     print(f"Page with ID {page_id} written to database")
+
+
diff --git a/messaging_service/__init__.py b/messaging_service/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/messaging_service/pulsar_client.py b/messaging_service/pulsar_client.py
deleted file mode 100644
index 904fe75..0000000
--- a/messaging_service/pulsar_client.py
+++ /dev/null
@@ -1,56 +0,0 @@
-import pulsar
-import time
-
-
-class PulsarClient:
-    def __init__(self, service_url):
-        self.client = pulsar.Client(service_url)
-
-    def publish_message(self, topic, message):
-        producer = self.client.create_producer(topic)
-        producer.send(message.encode('utf-8'))
-        print(f"Message published to topic '{topic}': {message}")
-        producer.close()
-
-    def consume_message(self, topic, subscription_name):
-        consumer = self.client.subscribe(topic, subscription_name)
-        msg = None  # Initialize msg before the try block
-
-        try:
-            msg = consumer.receive(timeout_millis=5000)  # waits for 5 seconds for a message
-            if msg:
-                print("Received message: '{}'".format(msg.data().decode('utf-8')))
-                consumer.acknowledge(msg)
-                return msg.data().decode('utf-8')  # Return the message content
-            else:
-                print("No message received within the timeout period.")
-                return None
-        except Exception as e:
-            print("Failed to process message: ", e)
-            if msg:  # Check if msg is not None
-                consumer.negative_acknowledge(msg)
-            return None
-        finally:
-            consumer.close()
-
-    def close(self):
-        self.client.close()
-
-
-if __name__ == "__main__":
-    client = PulsarClient("pulsar://localhost:6650")
-    topic = "my-topic"
-    subscription_name = "my-subscription"
-
-    # Publishing a message
-    client.publish_message(topic, "Hello, Pulsar!")
-
-    # Waiting for a short period
-    print("Waiting for 10 seconds before consuming the message...")
-    time.sleep(10)
-
-    # Consuming the message
-    client.consume_message(topic, subscription_name)
-
-    # Closing the Pulsar client
-    client.close()
diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
index d35fb16..43fd787 100755
--- a/setup/setup_and_run.sh
+++ b/setup/setup_and_run.sh
@@ -61,7 +61,7 @@ else
     echo "No requirements.txt found in $setup_path. Skipping Python dependencies installation."
 fi

-# python "$project_root_path/main.py"
+python "$project_root_path/main.py"
 # Install Python dependencies from the setup directory
 # requirements_file="$setup_path/requirements.txt"
 # if [ -f "$requirements_file" ]; then
@@ -74,5 +74,5 @@ fi
 # Add run create_db.sh to this script
 # Start the Docker containers
 # echo "Starting Docker containers from $project_root_path."
-docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone
+# docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone


7efc06dd683c246992aee2c1a9c43c30b6cb357b
Author: Roland Abou Younes
Date: Thu Dec 14 00:19:35 2023 +0200
Message: Getting all pages IDs, publishing to pulasar, consuming from pulsar, downloading content and adding to db and file system then aknowledging on pulsar. Successfully tested.

diff --git a/configuration.py b/configuration.py
index 01f6313..3efb943 100644
--- a/configuration.py
+++ b/configuration.py
@@ -11,3 +11,5 @@ sql_file_path = database_path + "/confluence_pages_sql.db"
 # Slack Bot User ID
 bot_user_id = "U069C17DCE5"

+# Pulsar client
+pulsar_client_url = "pulsar://localhost:6650"
diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index 3795132..9f6d6ee 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -198,6 +198,8 @@ def get_space_content(update_date=None):
     page_content_map = {}  # For storing page data for database

     for page_id in all_page_ids:
+        # publish IDs to pulsar
+
         page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
         page_title = strip_html_tags(page['title'])
         page_author = page['history']['createdBy']['displayName']
diff --git a/confluence_integration/retrieve_space_pulsar.py b/confluence_integration/retrieve_space_pulsar.py
new file mode 100644
index 0000000..34b5940
--- /dev/null
+++ b/confluence_integration/retrieve_space_pulsar.py
@@ -0,0 +1,235 @@
+# ./confluence_integration/retrieve_space.py
+from datetime import datetime
+from bs4 import BeautifulSoup
+from atlassian import Confluence
+from credentials import confluence_credentials
+from database.confluence_database import store_space_data, store_page_data
+from file_system.file_manager import FileManager
+from messaging_service.pulsar_client import PulsarClient
+from configuration import pulsar_client_url
+
+pulsar_client = PulsarClient(pulsar_client_url)
+
+# Initialize Confluence API
+confluence = Confluence(
+    url=confluence_credentials['base_url'],
+    username=confluence_credentials['username'],
+    password=confluence_credentials['api_token']
+)
+
+
+# Get top level pages from a space
+def get_top_level_ids(space_key):
+    """
+    Retrieve IDs of top-level pages in a specified Confluence space.
+
+    Args:
+    space_key (str): The key of the Confluence space.
+
+    Returns:
+    list: A list of page IDs for the top-level pages in the space.
+    """
+    top_level_pages = confluence.get_all_pages_from_space(space_key)
+    return [page['id'] for page in top_level_pages]
+
+
+# Get child pages from a page
+def get_child_ids(item_id, content_type):
+    """
+    Retrieve IDs of child items (pages or comments) for a given Confluence item.
+
+    Args:
+    item_id (str): The ID of the Confluence page or comment.
+    content_type (str): Type of content to retrieve ('page' or 'comment').
+
+    Returns:
+    list: A list of IDs for child items.
+    """
+    child_items = confluence.get_page_child_by_type(item_id, type=content_type)
+    return [child['id'] for child in child_items]
+
+
+def get_all_page_ids_recursive(space_key):
+    def get_child_pages_recursively(page_id):
+        child_pages = []
+        child_page_ids = get_child_ids(page_id, content_type='page')
+        for child_id in child_page_ids:
+            child_pages.append(child_id)
+            child_pages.extend(get_child_pages_recursively(child_id))
+        return child_pages
+
+    all_page_ids = []
+    top_level_ids = get_top_level_ids(space_key)
+    topic_name = "confluence_page_ids"  # Define the topic name here
+
+    for top_level_id in top_level_ids:
+        all_page_ids.append(top_level_id)
+        all_page_ids.extend(get_child_pages_recursively(top_level_id))
+
+    for page_id in all_page_ids:
+        pulsar_client.publish_message(topic_name, str(page_id))  # Include the topic name
+
+    return all_page_ids
+
+
+def get_all_comment_ids_recursive(page_id):
+    """
+    Recursively retrieves all comment IDs for a given Confluence page.
+
+    Args:
+    page_id (str): The ID of the Confluence page.
+
+    Returns:
+    list: A list of all comment IDs for the page.
+    """
+
+    def get_child_comment_ids_recursively(comment_id):
+        # Inner function to recursively get child comment IDs
+        child_comment_ids = []  # Use a separate list to accumulate child comment IDs
+        immediate_child_ids = get_child_ids(comment_id, content_type='comment')
+        for child_id in immediate_child_ids:
+            child_comment_ids.append(child_id)
+            child_comment_ids.extend(get_child_comment_ids_recursively(child_id))
+        return child_comment_ids
+
+    all_comment_ids = []
+    top_level_comment_ids = get_child_ids(page_id, content_type='comment')
+    for comment_id in top_level_comment_ids:
+        all_comment_ids.append(comment_id)
+        all_comment_ids.extend(get_child_comment_ids_recursively(comment_id))
+    return all_comment_ids
+
+
+def choose_space():
+    """
+    Prompt the user to choose a Confluence space from a list of available spaces.
+
+    Returns:
+    str: The key of the chosen Confluence space.
+    """
+    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
+    for i, space in enumerate(spaces['results']):
+        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
+    choice = int(input("Choose a space (number): ")) - 1
+    space_key = spaces['results'][choice]['key']
+    space_data = {'space_key': space_key,
+                  'url': confluence_credentials['base_url'],
+                  'login': confluence_credentials['username'],
+                  'token': confluence_credentials['api_token']
+                  }
+    store_space_data(space_data)
+    return spaces['results'][choice]['key']
+
+
+def strip_html_tags(content):
+    """
+    Remove HTML tags from a string.
+
+    Args:
+    content (str): The string with HTML content.
+
+    Returns:
+    str: The string with HTML tags removed.
+    """
+    soup = BeautifulSoup(content, 'html.parser')
+    return soup.get_text()
+
+
+def check_date_filter(update_date, all_page_ids):
+    """
+    Filter pages based on their last updated date.
+
+    Args:
+    update_date (datetime): The threshold date for filtering. Pages updated after this date will be included.
+    all_page_ids (list): A list of page IDs to be filtered.
+
+    Returns:
+    list: A list of page IDs that were last updated on or after the specified update_date.
+    """
+    updated_pages = []
+    for page_id in all_page_ids:
+        page_history = confluence.history(page_id)  # directly use page_id
+        last_updated = datetime.strptime(page_history['lastUpdated']['when'], '%Y-%m-%dT%H:%M:%S.%fZ')
+        if last_updated >= update_date:
+            updated_pages.append(page_id)  # append the page_id to the list
+    return updated_pages
+
+
+def format_page_content_for_llm(page_data):
+    """
+        Format page data into a string of key-value pairs suitable for LLM (Language Learning Models) context.
+
+        This function converts page data into a text format that can be easily consumed by language models,
+        with each key-value pair on a separate line.
+
+        Args:
+        page_data (dict): A dictionary containing page data with keys like title, author, createdDate, etc.
+
+        Returns:
+        str: A string representation of the page data in key-value format.
+        """
+    content = ""
+    for key, value in page_data.items():
+        content += f"{key}: {value}\n"
+    return content
+
+
+def publish_space_ids(space_key, update_date=None):
+    topic_name = "confluence_page_ids"
+    all_page_ids = get_all_page_ids_recursive(space_key)
+    if update_date is not None:
+        all_page_ids = check_date_filter(update_date, all_page_ids)
+
+    for page_id in all_page_ids:
+        pulsar_client.publish_message(topic_name, str(page_id))
+
+    print("Page IDs published to Pulsar.")
+    return all_page_ids
+
+
+def retrieve_and_process_page_data(page_id, space_key):
+    page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
+    if not page:
+        print(f"No data found for page ID: {page_id}")
+        return
+
+    page_title = page.get('title', '')
+    page_author = page.get('history', {}).get('createdBy', {}).get('displayName', '')
+    created_date = page.get('history', {}).get('createdDate', '')
+    last_updated = page.get('version', {}).get('when', '')
+    page_content = strip_html_tags(page.get('body', {}).get('storage', {}).get('value', ''))
+
+    page_comments_content = ""
+    page_comment_ids = get_all_comment_ids_recursive(page_id)
+    for comment_id in page_comment_ids:
+        comment = confluence.get_page_by_id(comment_id, expand='body.storage')
+        comment_content = strip_html_tags(comment.get('body', {}).get('storage', {}).get('value', ''))
+        page_comments_content += comment_content + '\n'
+
+    page_data = {
+        'title': page_title,
+        'author': page_author,
+        'createdDate': created_date,
+        'lastUpdated': last_updated,
+        'content': page_content,
+        'comments': page_comments_content
+    }
+
+    store_page_data(page_id, space_key, page_data)
+    formatted_content = format_page_content_for_llm(page_data)
+    file_manager = FileManager()
+    file_manager.create(f"{page_id}.txt", formatted_content)
+
+
+def consume_pages_and_store_data(topic_name, subscription_name, space_key):
+    while True:
+        page_id = pulsar_client.consume_message(topic_name, subscription_name)
+        if page_id:
+            retrieve_and_process_page_data(page_id, space_key)
+    pulsar_client.close()
+
+
+if __name__ == "__main__":
+    chosen_space_key = choose_space()
+    publish_space_ids(chosen_space_key)
+    consume_pages_and_store_data("confluence_page_ids", "default_processor", chosen_space_key)
diff --git a/database/confluence_database.py b/database/confluence_database.py
index 4f42a4f..a112536 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -1,22 +1,16 @@
-# ./database/confluence_database.py
-from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, LargeBinary
+from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import sessionmaker
 from datetime import datetime
 from configuration import sql_file_path

-
 # Define the base class for SQLAlchemy models
 Base = declarative_base()


 # Define the SpaceData model
 class SpaceData(Base):
-    """
-    SQLAlchemy model for storing Confluence space data.
-    """
     __tablename__ = 'space_data'
-
     id = Column(Integer, primary_key=True)
     space_key = Column(String)
     url = Column(String)
@@ -26,11 +20,7 @@ class SpaceData(Base):

 # Define the PageData model
 class PageData(Base):
-    """
-    SQLAlchemy model for storing Confluence page data.
-    """
     __tablename__ = 'page_data'
-
     id = Column(Integer, primary_key=True)
     page_id = Column(String)
     space_key = Column(String)
@@ -49,51 +39,29 @@ Base.metadata.create_all(engine)

 # Create a sessionmaker object to manage database sessions
 Session = sessionmaker(bind=engine)
-session = Session()


-def store_space_data(space_data):
-    """
-    Store Confluence space data into the database.
+def parse_datetime(date_string):
+    return datetime.fromisoformat(date_string.replace('Z', '+00:00'))
+

-    Args:
-    space_data (dict): A dictionary containing space data to store.
-    """
-    # Create a new SpaceData object and add it to the session
+def store_space_data(space_data):
+    session = Session()
     new_space = SpaceData(space_key=space_data['space_key'],
                           url=space_data['url'],
                           login=space_data['login'],
                           token=space_data['token'])
     session.add(new_space)
     session.commit()
+    session.close()
     print(f"Space with key {space_data['space_key']} written to database")


-def parse_datetime(date_string):
-    """
-    Convert an ISO format datetime string to a datetime object.
-
-    Args:
-    date_string (str): ISO format datetime string.
-
-    Returns:
-    datetime: A datetime object.
-    """
-    return datetime.fromisoformat(date_string.replace('Z', '+00:00'))
-
-
 def store_pages_data(space_key, pages_data):
-    """
-    Store Confluence page data into the database.
-
-    Args:
-    space_key (str): The key of the Confluence space.
-    pages_data (dict): A dictionary of page data, keyed by page ID.
-    """
+    session = Session()
     for page_id, page_info in pages_data.items():
         created_date = parse_datetime(page_info['createdDate'])
         last_updated = parse_datetime(page_info['lastUpdated'])
-
         new_page = PageData(page_id=page_id,
                             space_key=space_key,
                             title=page_info['title'],
@@ -104,6 +72,24 @@ def store_pages_data(space_key, pages_data):
                             comments=page_info['comments'])
         session.add(new_page)
     session.commit()
+    session.close()
+    print(f"Pages from space {space_key} written to database")
+
+
+def store_page_data(page_id, space_key, page_data):
+    session = Session()
+    created_date = parse_datetime(page_data['createdDate'])
+    last_updated = parse_datetime(page_data['lastUpdated'])
+    new_page = PageData(page_id=page_id,
+                        space_key=space_key,
+                        title=page_data['title'],
+                        author=page_data['author'],
+                        createdDate=created_date,
+                        lastUpdated=last_updated,
+                        content=page_data['content'],
+                        comments=page_data['comments'])
+
+    session.add(new_page)
+    session.commit()
+    session.close()
     print(f"Page with ID {page_id} written to database")
-
-
diff --git a/messaging_service/__init__.py b/messaging_service/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/messaging_service/pulsar_client.py b/messaging_service/pulsar_client.py
new file mode 100644
index 0000000..904fe75
--- /dev/null
+++ b/messaging_service/pulsar_client.py
@@ -0,0 +1,56 @@
+import pulsar
+import time
+
+
+class PulsarClient:
+    def __init__(self, service_url):
+        self.client = pulsar.Client(service_url)
+
+    def publish_message(self, topic, message):
+        producer = self.client.create_producer(topic)
+        producer.send(message.encode('utf-8'))
+        print(f"Message published to topic '{topic}': {message}")
+        producer.close()
+
+    def consume_message(self, topic, subscription_name):
+        consumer = self.client.subscribe(topic, subscription_name)
+        msg = None  # Initialize msg before the try block
+
+        try:
+            msg = consumer.receive(timeout_millis=5000)  # waits for 5 seconds for a message
+            if msg:
+                print("Received message: '{}'".format(msg.data().decode('utf-8')))
+                consumer.acknowledge(msg)
+                return msg.data().decode('utf-8')  # Return the message content
+            else:
+                print("No message received within the timeout period.")
+                return None
+        except Exception as e:
+            print("Failed to process message: ", e)
+            if msg:  # Check if msg is not None
+                consumer.negative_acknowledge(msg)
+            return None
+        finally:
+            consumer.close()
+
+    def close(self):
+        self.client.close()
+
+
+if __name__ == "__main__":
+    client = PulsarClient("pulsar://localhost:6650")
+    topic = "my-topic"
+    subscription_name = "my-subscription"
+
+    # Publishing a message
+    client.publish_message(topic, "Hello, Pulsar!")
+
+    # Waiting for a short period
+    print("Waiting for 10 seconds before consuming the message...")
+    time.sleep(10)
+
+    # Consuming the message
+    client.consume_message(topic, subscription_name)
+
+    # Closing the Pulsar client
+    client.close()
diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
index 43fd787..d35fb16 100755
--- a/setup/setup_and_run.sh
+++ b/setup/setup_and_run.sh
@@ -61,7 +61,7 @@ else
     echo "No requirements.txt found in $setup_path. Skipping Python dependencies installation."
 fi

-python "$project_root_path/main.py"
+# python "$project_root_path/main.py"
 # Install Python dependencies from the setup directory
 # requirements_file="$setup_path/requirements.txt"
 # if [ -f "$requirements_file" ]; then
@@ -74,5 +74,5 @@ python "$project_root_path/main.py"
 # Add run create_db.sh to this script
 # Start the Docker containers
 # echo "Starting Docker containers from $project_root_path."
-# docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone
+docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone


c7c7193764903fa7315bff03e7dbcf63635277e9
Author: Roland Abou Younes
Date: Tue Dec 12 21:27:28 2023 +0200
Message: Tested functional with GPT-4t and assistant

diff --git a/configuration.py b/configuration.py
index 1752ec6..01f6313 100644
--- a/configuration.py
+++ b/configuration.py
@@ -5,6 +5,7 @@ project_path = "/Users/roland/code/Nur"
 file_system_path = project_path + "/content/file_system"
 database_path = project_path + "/content/database"
 vector_folder_path = database_path + "/confluence_page_vectors"
+vector_chunk_folder_path = database_path + "/confluence_page_vectors"
 sql_file_path = database_path + "/confluence_pages_sql.db"

 # Slack Bot User ID
diff --git a/gpt_4t_response/__init__.py b/gpt_4t/__init__.py
similarity index 100%
rename from gpt_4t_response/__init__.py
rename to gpt_4t/__init__.py
diff --git a/gpt_4t/old/load_document.py b/gpt_4t/old/load_document.py
new file mode 100644
index 0000000..90f833f
--- /dev/null
+++ b/gpt_4t/old/load_document.py
@@ -0,0 +1,43 @@
+from langchain.document_loaders import TextLoader, PythonLoader, BSHTMLLoader, PDFPlumberLoader
+
+
+def load_text_file(file_path):
+    loader = TextLoader(file_path)
+    documents = loader.load_and_split()
+    return documents
+
+
+def load_python_file(file_path):
+    loader = PythonLoader(file_path)
+    documents = loader.load_and_split()
+    return documents
+
+
+def load_bshtml_file(file_path):
+    loader = BSHTMLLoader(file_path)
+    documents = loader.load_and_split()
+    return documents
+
+
+def load_pdf_file(file_path):
+    loader = PDFPlumberLoader(file_path)
+    documents = loader.load_and_split()
+    return documents
+
+
+def process_document(file_path):
+    # Determine the file extension
+    file_extension = file_path.split('.')[-1]
+
+    # Call the appropriate function based on file extension
+    if file_extension == 'txt':
+        return load_text_file(file_path)
+    elif file_extension == 'py':
+        return load_python_file(file_path)
+    elif file_extension == 'html' or file_extension == 'htm':
+        return load_bshtml_file(file_path)
+    elif file_extension == 'pdf':
+        return load_pdf_file(file_path)
+    else:
+        print(f"Warning: Unsupported file extension: {file_extension} for file {file_path}. Skipping...")
+        return []
diff --git a/gpt_4t/old/query_gpt_4t_with_context.py b/gpt_4t/old/query_gpt_4t_with_context.py
new file mode 100644
index 0000000..c730125
--- /dev/null
+++ b/gpt_4t/old/query_gpt_4t_with_context.py
@@ -0,0 +1,107 @@
+import os
+import openai
+from langchain.chains import RetrievalQA
+from langchain.vectorstores import Chroma
+from langchain.embeddings import OpenAIEmbeddings
+from langchain.text_splitter import RecursiveCharacterTextSplitter
+from langchain.llms import LLM
+from gpt_4t_response.load_document import process_document
+from credentials import oai_api_key
+from configuration import file_system_path, vector_chunk_folder_path
+
+# Set API key in environment variables
+os.environ["OPENAI_API_KEY"] = oai_api_key
+
+# Create OpenAI client and wrapper class
+client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])
+
+class OpenAIWrapper(LLM):
+    def __init__(self, client):
+        self.client = client
+
+    def generate(self, prompt, **kwargs):
+        # Adjust this method to match LangChain's expectations
+        return self.client.Completion.create(prompt=prompt, **kwargs)
+
+# Instantiate the OpenAI wrapper
+openai_wrapper = OpenAIWrapper(client)
+
+
+def get_all_files_in_directory(directory):
+    """
+    Returns a list of all file paths within the specified directory and its subdirectories.
+
+    Args:
+    - directory (str): The path to the directory.
+
+    Returns:
+    - list: A list of file paths.
+    """
+    file_paths = []
+
+    # Walk through directory and subdirectories
+    for dirpath, _, filenames in os.walk(directory):
+        for filename in filenames:
+            if not filename.endswith('.DS_Store'):
+                file_paths.append(os.path.join(dirpath, filename))
+
+    return file_paths
+
+
+def initial_load_context():
+    """
+    Initially load context into the vector database.
+    """
+    # Get a list of all files in the 'context/' directory and subdirectories
+    file_paths = get_all_files_in_directory(file_system_path)
+
+    # Process each file using the process_document function
+    all_documents = []
+    for file_path in file_paths:
+        documents = process_document(file_path)
+        all_documents.extend(documents)  # Combine all documents into a single list
+    # split the document into chunks
+    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
+    texts = text_splitter.split_documents(all_documents)
+    # create the vector database from the context folder
+    embedding = OpenAIEmbeddings()
+    vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=vector_chunk_folder_path)
+    vectordb.persist()
+
+
+def load_context_and_answer(question, debug=True):
+    vectordb = Chroma(persist_directory=vector_chunk_folder_path, embedding_function=OpenAIEmbeddings())
+    retriever = vectordb.as_retriever(search_kwargs={"k": 4})
+    qa_chain = RetrievalQA.from_chain_type(llm=openai_wrapper, chain_type="stuff", retriever=retriever, return_source_documents=True)
+
+    llm_response = qa_chain(question)
+    answer = llm_response['result']
+
+    if debug:
+        print('\n\nSources:')
+        for source in llm_response["source_documents"]:
+            print(source.metadata['source'])
+
+    return answer
+
+def load_chat():
+    debug = True
+    while True:
+        print("You: (type 'done' on a new line when finished)")
+        user_input = ""
+        while True:
+            line = input()
+            if line.lower() == 'done':
+                break
+            user_input += line + '\n'
+        if user_input.lower() in ['exit\n', 'quit\n']:
+            break
+        print('Processing...')
+        answer = load_context_and_answer(user_input, debug)
+        print(f'GPT: {answer}')
+
+
+if __name__ == "__main__":
+    initial_load_context()
+    load_chat()
+
diff --git a/gpt_4t/old/test.py b/gpt_4t/old/test.py
new file mode 100644
index 0000000..836b0bd
--- /dev/null
+++ b/gpt_4t/old/test.py
@@ -0,0 +1,51 @@
+import os
+import openai
+from langchain.embeddings import OpenAIEmbeddings
+from langchain.vectorstores import Chroma
+from langchain.text_splitter import RecursiveCharacterTextSplitter
+from langchain.chains import RetrievalQA
+from langchain.llms import LLM
+
+# Set your OpenAI API key
+os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
+
+# Function to load and split a text file into chunks
+def load_and_split_documents(file_paths):
+    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
+    documents = []
+    for file_path in file_paths:
+        with open(file_path, 'r') as file:
+            text = file.read()
+            chunks = splitter.split_text(text)
+            documents.extend(chunks)
+    return documents
+
+# Set up OpenAI and LangChain
+embedding = OpenAIEmbeddings()
+vectordb = Chroma("your_vector_store_name", embedding)
+
+# Load and process your documents
+file_paths = ["path/to/your/textfile1.txt", "path/to/your/textfile2.txt"]
+all_documents = load_and_split_documents(file_paths)
+
+# Index documents in the vector store
+for doc in all_documents:
+    vectordb.add(doc)
+
+# Test function
+def test_query(query):
+    similar_docs = vectordb.similarity_search(query)
+    context = "\n".join(similar_docs)
+    response = openai.Completion.create(prompt=context + query, engine="davinci", max_tokens=150)
+    return response.choices[0].text
+
+# Running tests
+test_queries = [
+    "What is the main theme of the first document?",
+    "Explain the process described in the second document."
+]
+
+for query in test_queries:
+    print(f"Query: {query}")
+    print("Response:", test_query(query))
+    print("-------------")
diff --git a/gpt_4t/query_from_documents.py b/gpt_4t/query_from_documents.py
new file mode 100644
index 0000000..f58fb47
--- /dev/null
+++ b/gpt_4t/query_from_documents.py
@@ -0,0 +1,84 @@
+# ./oai_assistants/query_gpt_4t_from_documents.py
+from openai import OpenAI
+from credentials import oai_api_key
+
+client = OpenAI(api_key=oai_api_key)
+
+
+def get_response_from_gpt_4t(question, context):
+    """
+    Queries the GPT-4T model with a specific question and context.
+
+    Args:
+    question (str): The question to be asked.
+    context (str): The context to be used for answering the question.
+
+    Returns:
+    str: The response from the GPT-4T model.
+    """
+    response = client.chat.completions.create(
+        model="gpt-4-1106-preview",
+        messages=[
+            {
+                "role": "system",
+                "content": "You are the Q&A based on knowledge base assistant.\nYou will always review and refer to the pages included as context. \nYou will always answer from the pages.\nYou will never improvise or create content from outside the files.\nIf you do not have the answer based on the files you will clearly state that and abstain from answering.\nIf you use your knowledge to explain some information from outside the file, you will clearly state that.\n"
+            },
+            {
+                "role": "user",
+                "content": f"You will answer the following question with a summary, then provide a comprehensive answer, then provide the references aliasing them as Technical trace: \nquestion: {question}\npages:{context}"
+            }
+        ],
+        temperature=0.5,
+        max_tokens=4095,
+        top_p=1,
+        frequency_penalty=0,
+        presence_penalty=0
+    )
+    answer = response.choices[0].message.content
+    return answer
+
+
+def format_pages_as_context(file_ids):
+    """
+    Adds specified files to the question's context for referencing in responses.
+
+    Args:
+    file_ids (list of str): List of file IDs to be added to the assistant.
+    """
+    context = ""
+    for file_id in file_ids:
+        chosen_file_path = f"/Users/roland/code/Nur/content/file_system/{file_id}.txt"
+        # Open file and append file to context
+        with open(chosen_file_path, 'r') as file:
+            context += file.read()
+        print(f"File {file_id} appended to context successfully")
+
+    return context
+
+
+def query_gpt_4t_with_context(question, page_ids):
+    """
+    Queries the assistant with a specific question, after setting up the necessary context by adding relevant files.
+
+    Args:
+    question (str): The question to be asked.
+    page_ids (list): A list of page IDs representing the files to be added to the assistant's context.
+
+    Returns:
+    list: A list of messages, including the assistant's response to the question.
+    """
+    # Format the context
+    # Ensure page_ids is a list
+    if not isinstance(page_ids, list):
+        page_ids = [page_ids]
+    context = format_pages_as_context(page_ids)
+    # Query the assistant
+    response = get_response_from_gpt_4t(question, context)
+    return response
+
+
+if __name__ == "__main__":
+    response = query_gpt_4t_with_context("Do we support payment matching in our solution? and if the payment is not matched "
+                                 "do we already have a way to notify the client that they have a delayed payment?",
+                                 ["458841", "491570"])
+    print(response)
\ No newline at end of file
diff --git a/gpt_4t_response/query_gpt_4t_with_context.py b/gpt_4t_response/query_gpt_4t_with_context.py
deleted file mode 100644
index 6fc9ca8..0000000
--- a/gpt_4t_response/query_gpt_4t_with_context.py
+++ /dev/null
@@ -1,2 +0,0 @@
-def query_gpt_4t_with_context(question, page_ids):
-    return f"this is a mock response from query_gpt_4t_with_context"
diff --git a/main.py b/main.py
index 394415d..06a9bf0 100644
--- a/main.py
+++ b/main.py
@@ -1,6 +1,7 @@
 from confluence_integration.retrieve_space import get_space_content
 from vector.chroma import add_to_vector, retrieve_relevant_documents
 from oai_assistants.query_assistant_from_documents import query_assistant_with_context
+from gpt_4t.query_from_documents import query_gpt_4t_with_context
 from slack.channel_reaction import load_slack_bot


@@ -10,11 +11,15 @@ def add_space():
     return retrieved_page_ids, indexed_page_ids


-def answer_question(question):
+def answer_question_with_assistant(question):
     relevant_document_ids = retrieve_relevant_documents(question)
     response = query_assistant_with_context(question, relevant_document_ids)
     return response

+def answer_question_with_gpt_4t(question):
+    relevant_document_ids = retrieve_relevant_documents(question)
+    response = query_gpt_4t_with_context(question, relevant_document_ids)
+    return response

 question1 = "Do we support payment matching in our solution? and if the payment is not matched do we already have a way to notify the client that they have a delayed payment?"

@@ -32,8 +37,9 @@ def main_menu():
     while True:
         print("\nMain Menu:")
         print("1. Load New Documentation Space")
-        print("2. Ask a Question to Existing Documentation")
-        print("3. Start Slack Bot")
+        print("2. Ask a Question to Existing Documentation with Assistant")
+        print("3. Ask a question to Existing Documentation with GPT-4T")
+        print("4. Start Slack Bot")
         print("0. Cancel/Quit")
         choice = input("Enter your choice (0-3): ")

@@ -43,9 +49,14 @@ def main_menu():
         elif choice == "2":
             question = ask_question()
             if question:
-                answer = answer_question(question)
+                answer = answer_question_with_assistant(question)
                 print("\nAnswer:", answer)
         elif choice == "3":
+            question = ask_question()
+            if question:
+                answer = answer_question_with_gpt_4t(question)
+                print("\nAnswer:", answer)
+        elif choice == "4":
             print("Starting Slack Bot...")
             load_slack_bot()
             print("Slack Bot is running.")
diff --git a/slack/channel_reaction.py b/slack/channel_reaction.py
index e033722..e7ce68a 100644
--- a/slack/channel_reaction.py
+++ b/slack/channel_reaction.py
@@ -11,6 +11,7 @@ from file_system.file_manager import FileManager
 from vector.chroma import retrieve_relevant_documents
 from oai_assistants.query_assistant_from_documents import query_assistant_with_context
 from configuration import bot_user_id
+from gpt_4t.query_from_documents import query_gpt_4t_with_context


 # Abstract base class for Slack event handlers
@@ -95,19 +96,22 @@ class ChannelMessageHandler(SlackEventHandler):
         # Use FileManager's create method to save the file
         file_manager.create(file_name, context)

-    def generate_response(self, question, file_name):
+    def generate_response(self, question, file_name, respond_with_assistant=False):
         """ Generate a response for a question """
         file_id = file_name[:-4]
         relevant_document_ids = retrieve_relevant_documents(question)
-        relevant_document_ids.append(file_id)
-        response_obj = query_assistant_with_context(question, relevant_document_ids)
+        if respond_with_assistant:
+            relevant_document_ids.append(file_id)
+            response_obj = query_assistant_with_context(question, relevant_document_ids)
+            response_text = "No valid response found."  # Default response
+            for message in response_obj.data:
+                if message.role == "assistant":
+                    response_text = message.content[0].text.value
+                    break  # Assuming you only need the first assistant message
+        else:
+            response_text = query_gpt_4t_with_context(question, relevant_document_ids)

         # Extract the text from the first assistant message
-        response_text = "No valid response found."  # Default response
-        for message in response_obj.data:
-            if message.role == "assistant":
-                response_text = message.content[0].text.value
-                break  # Assuming you only need the first assistant message

         return response_text


a267a19b8d255aa57be91137cbcc037693b84e91
Author: Roland Abou Younes
Date: Sat Dec 9 18:00:40 2023 +0200
Message: Added loading a slack bot to the main menu and extracted the Bot ID variable to configuration

diff --git a/README.md b/README.md
index d1521db..257d1b2 100644
--- a/README.md
+++ b/README.md
@@ -11,6 +11,7 @@ The self actualizing documentation framework that heals its knowledge gaps as na
 - Listens on specific slack channels for questions relevant to its domain

 ### Todo:
+- Delete Open AI assistant after use
 - Implement fast response using Gpt-4 Turbo without assistant
 - Implement Pulsar message infrastructure for scalability and resilience
 - Gets user feedback to either increase confidence or decrease confidence
@@ -30,5 +31,6 @@ Setup script not functional at this point.
 ## Usage
 1. Add openai api key to credentials
 2. Add confluence credentials to ./credentials
-3. Add project absolute path to ./confiduration
-4. Run the main.py file
+3. Add project absolute path to ./confiduration
+4. To listen to slack create slack app and add bot_user_id to ./configuration
+5. Run the main.py file and follow the menu
diff --git a/configuration.py b/configuration.py
index aed219e..1752ec6 100644
--- a/configuration.py
+++ b/configuration.py
@@ -6,3 +6,7 @@ file_system_path = project_path + "/content/file_system"
 database_path = project_path + "/content/database"
 vector_folder_path = database_path + "/confluence_page_vectors"
 sql_file_path = database_path + "/confluence_pages_sql.db"
+
+# Slack Bot User ID
+bot_user_id = "U069C17DCE5"
+
diff --git a/main.py b/main.py
index a39b4f8..394415d 100644
--- a/main.py
+++ b/main.py
@@ -1,6 +1,7 @@
 from confluence_integration.retrieve_space import get_space_content
 from vector.chroma import add_to_vector, retrieve_relevant_documents
-from oai_assistants.query_assistant_from_documents import get_response_from_assistant
+from oai_assistants.query_assistant_from_documents import query_assistant_with_context
+from slack.channel_reaction import load_slack_bot


 def add_space():
@@ -11,7 +12,7 @@ def add_space():

 def answer_question(question):
     relevant_document_ids = retrieve_relevant_documents(question)
-    response = get_response_from_assistant(question, relevant_document_ids)
+    response = query_assistant_with_context(question, relevant_document_ids)
     return response


@@ -32,8 +33,9 @@ def main_menu():
         print("\nMain Menu:")
         print("1. Load New Documentation Space")
         print("2. Ask a Question to Existing Documentation")
+        print("3. Start Slack Bot")
         print("0. Cancel/Quit")
-        choice = input("Enter your choice (0-2): ")
+        choice = input("Enter your choice (0-3): ")

         if choice == "1":
             add_space()
@@ -43,11 +45,15 @@ def main_menu():
             if question:
                 answer = answer_question(question)
                 print("\nAnswer:", answer)
+        elif choice == "3":
+            print("Starting Slack Bot...")
+            load_slack_bot()
+            print("Slack Bot is running.")
         elif choice == "0":
             print("Exiting program.")
             break
         else:
-            print("Invalid choice. Please enter 0, 1, or 2.")
+            print("Invalid choice. Please enter 0, 1, 2, or 3.")


 def ask_question():
diff --git a/slack/channel_reaction.py b/slack/channel_reaction.py
index 408e456..e033722 100644
--- a/slack/channel_reaction.py
+++ b/slack/channel_reaction.py
@@ -7,11 +7,10 @@ from slack_sdk.socket_mode.response import SocketModeResponse
 from slack_sdk.socket_mode.request import SocketModeRequest
 from typing import List
 from credentials import slack_bot_user_oauth_token, slack_app_level_token
-from configuration import file_system_path
 from file_system.file_manager import FileManager
 from vector.chroma import retrieve_relevant_documents
 from oai_assistants.query_assistant_from_documents import query_assistant_with_context
-import os
+from configuration import bot_user_id


 # Abstract base class for Slack event handlers
@@ -151,10 +150,13 @@ class SlackBot:
             logging.critical("Bot stopped due to an exception", exc_info=True)


-# Start the bot if this script is run directly
-if __name__ == "__main__":
+def load_slack_bot():
     logging.basicConfig(level=logging.DEBUG)
-    bot_user_id = "U069C17DCE5"  # Replace with your bot's actual user ID
     event_handlers = [ChannelMessageHandler(), ReactionHandler()]
     bot = SlackBot(slack_bot_user_oauth_token, slack_app_level_token, bot_user_id, event_handlers)
     bot.start()
+
+
+# Start the bot if this script is run directly
+if __name__ == "__main__":
+    load_slack_bot()

8abb71e732227cd00e31631c29460b2ed0d52df2
Author: Roland Abou Younes
Date: Sat Dec 9 16:34:18 2023 +0200
Message: Refactoring channel reaction Created query gpt4

diff --git a/gpt_4t_response/__init__.py b/gpt_4t_response/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/gpt_4t_response/query_gpt_4t_with_context.py b/gpt_4t_response/query_gpt_4t_with_context.py
new file mode 100644
index 0000000..6fc9ca8
--- /dev/null
+++ b/gpt_4t_response/query_gpt_4t_with_context.py
@@ -0,0 +1,2 @@
+def query_gpt_4t_with_context(question, page_ids):
+    return f"this is a mock response from query_gpt_4t_with_context"
diff --git a/oai_assistants/query_assistant_from_documents.py b/oai_assistants/query_assistant_from_documents.py
index 8247e44..fde66f7 100644
--- a/oai_assistants/query_assistant_from_documents.py
+++ b/oai_assistants/query_assistant_from_documents.py
@@ -8,7 +8,10 @@ from oai_assistants.assistant_manager import AssistantManager

 def create_new_assistant():
     """
-    Creates a new assistant with the specified parameters.
+    Creates and initializes a new assistant with predefined parameters.
+
+    Returns:
+    Assistant: An instance of the newly created assistant.
     """
     client = initiate_client()

@@ -25,7 +28,7 @@ def create_new_assistant():
         "description": "The ultimate librarian",
         "file_ids": []
     }
-
+    # Create the assistant and print its details
     assistant = create_assistant(client, new_assistant)
     print(assistant)
     return assistant
@@ -33,7 +36,11 @@ def create_new_assistant():

 def add_files_to_assistant(assistant, file_ids):
     """
-    Adds multiple files to an assistant's list of files.
+    Adds specified files to the assistant's context for referencing in responses.
+
+    Args:
+    assistant (Assistant): The assistant to which files will be added.
+    file_ids (list of str): List of file IDs to be added to the assistant.
     """
     client = initiate_client()
     file_manager = FileManager(client)
@@ -49,30 +56,38 @@ def add_files_to_assistant(assistant, file_ids):
         print(f"File {chosen_file_path} added to assistant {assistant.id}")


-def ask_assistant(assistant, question):
-    """
-    Asks an assistant a question.
+def query_assistant_with_context(question, page_ids):
     """
-    client = initiate_client()
-    thread_manager = ThreadManager(client, assistant.id)
-    thread_manager.create_thread()
-    question = (f"You will answer the following question with a summary, then provide a comprehensive answer, "
-                f"then provide the references aliasing them as Technical trace: {question}")
-    messages = thread_manager.add_message_and_wait_for_reply(question, [])
-    return messages
+    Queries the assistant with a specific question, after setting up the necessary context by adding relevant files.

+    Args:
+    question (str): The question to be asked.
+    page_ids (list): A list of page IDs representing the files to be added to the assistant's context.

-def get_response_from_assistant(question, page_ids):
+    Returns:
+    list: A list of messages, including the assistant's response to the question.
+    """
+    # Create a new assistant instance
     assistant = create_new_assistant()

+    # Ensure page_ids is a list
     if not isinstance(page_ids, list):
         page_ids = [page_ids]

+    # Add relevant files to the assistant
     add_files_to_assistant(assistant, page_ids)
-    messages = ask_assistant(assistant, question)
-    # print(messages)
+
+    # Format the question and query the assistant
+    client = initiate_client()
+    thread_manager = ThreadManager(client, assistant.id)
+    thread_manager.create_thread()
+    formatted_question = (f"You will answer the following question with a summary, then provide a comprehensive answer, "
+                          f"then provide the references aliasing them as Technical trace: {question}")
+    messages = thread_manager.add_message_and_wait_for_reply(formatted_question, [])
     return messages


 if __name__ == "__main__":
-    get_response_from_assistant("Do we support payment matching in our solution? and if the payment is not matched do we already have a way to notify the client that they have a delayed payment?", ["458841", "491570"])
+    query_assistant_with_context("Do we support payment matching in our solution? and if the payment is not matched "
+                                 "do we already have a way to notify the client that they have a delayed payment?",
+                                 ["458841", "491570"])
diff --git a/slack/channel_message_handler.py b/slack/channel_message_handler.py
deleted file mode 100644
index 1976c5e..0000000
--- a/slack/channel_message_handler.py
+++ /dev/null
@@ -1,38 +0,0 @@
-
-
-class ChannelMessageHandler(SlackEventHandler):
-    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
-        try:
-            if req.type == "events_api":
-                event = req.payload.get("event", {})
-                if event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id:
-                    text = event.get("text", "")
-                    channel_id = event["channel"]
-
-                    if "?" in text:
-                        file_name = f"{channel_id}_context.txt"
-                        self.answer_question(file_name, text, event.get("ts"), web_client, channel_id)
-                    else:
-                        # Respond to the message as usual
-                        response_message = "Your usual response message here"
-                        web_client.chat_postMessage(channel=channel_id, text=response_message)
-
-        except Exception as e:
-            logging.error(f"Error processing event: {e}", exc_info=True)
-
-    def answer_question(self, file_name, question, message_id_to_reply_under, web_client, channel_id):
-        # Fetch context from recent messages
-        recent_messages = self.fetch_recent_messages(channel_id, web_client)
-        context = "\n".join(recent_messages)
-        with open(file_name, 'w') as file:  # Save context to file
-            file.write(context)
-
-        # Combine context with the question for the LLM
-        combined_input = f"{context}\n\nQuestion: {question}"
-        response = get_response_from_assistant(combined_input, [file_name])
-        web_client.chat_postMessage(channel=channel_id, text=response, thread_ts=message_id_to_reply_under)
-
-    def fetch_recent_messages(self, channel_id, web_client):
-        response = web_client.conversations_history(channel=channel_id, limit=100)
-        return [msg.get('text') for msg in response.get('messages', [])]
-
diff --git a/slack/channel_reaction.py b/slack/channel_reaction.py
index e268fe8..408e456 100644
--- a/slack/channel_reaction.py
+++ b/slack/channel_reaction.py
@@ -7,10 +7,11 @@ from slack_sdk.socket_mode.response import SocketModeResponse
 from slack_sdk.socket_mode.request import SocketModeRequest
 from typing import List
 from credentials import slack_bot_user_oauth_token, slack_app_level_token
+from configuration import file_system_path
 from file_system.file_manager import FileManager
 from vector.chroma import retrieve_relevant_documents
-from oai_assistants.query_assistant_from_documents import get_response_from_assistant
-
+from oai_assistants.query_assistant_from_documents import query_assistant_with_context
+import os


 # Abstract base class for Slack event handlers
@@ -22,28 +23,95 @@ class SlackEventHandler(ABC):

 # Handler for messages sent in Slack channels
 class ChannelMessageHandler(SlackEventHandler):
+    processed_messages = set()  # To keep track of processed message IDs
+
     def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        # Acknowledge the event immediately
+        client.send_socket_mode_response(SocketModeResponse(envelope_id=req.envelope_id))
+
+        event = req.payload.get("event", {})
+        message_id = event.get("client_msg_id")  # Unique identifier for each message
+
+        # Check if the message is a retry and already processed
+        if message_id in self.processed_messages:
+            return
+
         try:
-            logging.debug(f"Received event: {req.payload}")
             if req.type == "events_api":
-                event = req.payload.get("event", {})
-                logging.info(f"Event details: {event}")
-
-                # Ignore messages from the bot itself
-                if event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id:
+                if self.is_valid_message(event, bot_user_id):
                     text = event.get("text", "")
                     channel_id = event["channel"]
-                    logging.info(f"Message received: '{text}' in channel {channel_id}")

-                    # Respond to the message
-                    response_message = f"I got a message from you saying \"{text}\""
-                    web_client.chat_postMessage(channel=channel_id, text=response_message)
-                    logging.info(f"Sent response in channel {channel_id}")
+                    if "?" in text:
+                        # Handle question message
+                        self.answer_question(channel_id, text, event.get("ts"), web_client)
+                    else:
+                        # Handle non-question message
+                        self.send_default_response(text, channel_id, event.get("ts"), web_client)
+
+                    # Add the message ID to the processed set
+                    self.processed_messages.add(message_id)

-                client.send_socket_mode_response(SocketModeResponse(envelope_id=req.envelope_id))
         except Exception as e:
             logging.error(f"Error processing event: {e}", exc_info=True)

+    def is_valid_message(self, event, bot_user_id):
+        """ Check if the event is a valid user message """
+        return event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id
+
+    def send_default_response(self, text, channel_id, thread_ts, web_client):
+        """ Send a default response to a non-question message """
+        response_message = f"I got a message from you saying \"{text}\""
+        web_client.chat_postMessage(channel=channel_id, text=response_message, thread_ts=thread_ts)
+
+    def answer_question(self, channel_id, question, message_id_to_reply_under, web_client):
+        """ Handle a question message """
+        file_name = f"{channel_id}_context.txt"
+        context = self.fetch_recent_messages(channel_id, web_client)
+        self.save_context_to_file(context, file_name)
+
+        response_text = self.generate_response(question, file_name)
+        print("*"*100)
+        print(response_text)
+        print("*"*100)
+        # Send the extracted text as a response in Slack
+        web_client.chat_postMessage(channel=channel_id, text=response_text, thread_ts=message_id_to_reply_under)
+
+    def fetch_recent_messages(self, channel_id, web_client):
+        """ Fetch recent messages for context """
+        response = web_client.conversations_history(channel=channel_id, limit=100)
+        return "\n".join([msg.get('text') for msg in response.get('messages', [])])
+
+    def save_context_to_file(self, context, file_name):
+        """
+        Save the fetched context to a file using the FileManager class.
+
+        Args:
+        context (str): The context string to be saved to a file.
+        file_name (str): The name of the file to save the context in.
+        """
+        # Initialize FileManager instance
+        file_manager = FileManager()
+
+        # Use FileManager's create method to save the file
+        file_manager.create(file_name, context)
+
+    def generate_response(self, question, file_name):
+        """ Generate a response for a question """
+        file_id = file_name[:-4]
+        relevant_document_ids = retrieve_relevant_documents(question)
+        relevant_document_ids.append(file_id)
+        response_obj = query_assistant_with_context(question, relevant_document_ids)
+
+        # Extract the text from the first assistant message
+        response_text = "No valid response found."  # Default response
+        for message in response_obj.data:
+            if message.role == "assistant":
+                response_text = message.content[0].text.value
+                break  # Assuming you only need the first assistant message
+
+        return response_text
+

 # Handler for reactions added to messages
 class ReactionHandler(SlackEventHandler):
diff --git a/slack/channel_reaction2.py b/slack/channel_reaction2.py
deleted file mode 100644
index d5c7fa0..0000000
--- a/slack/channel_reaction2.py
+++ /dev/null
@@ -1,150 +0,0 @@
-import logging
-import time
-from abc import ABC, abstractmethod
-from slack_sdk import WebClient
-from slack_sdk.socket_mode import SocketModeClient
-from slack_sdk.socket_mode.response import SocketModeResponse
-from slack_sdk.socket_mode.request import SocketModeRequest
-from typing import List
-from credentials import slack_bot_user_oauth_token, slack_app_level_token
-from configuration import file_system_path
-from file_system.file_manager import FileManager
-from vector.chroma import retrieve_relevant_documents
-from oai_assistants.query_assistant_from_documents import get_response_from_assistant
-import os
-
-
-# Abstract base class for Slack event handlers
-class SlackEventHandler(ABC):
-    @abstractmethod
-    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
-        pass
-
-
-# Handler for messages sent in Slack channels
-class ChannelMessageHandler(SlackEventHandler):
-    processed_messages = set()  # To keep track of processed message IDs
-
-    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
-        # Acknowledge the event immediately
-        client.send_socket_mode_response(SocketModeResponse(envelope_id=req.envelope_id))
-
-        event = req.payload.get("event", {})
-        message_id = event.get("client_msg_id")  # Unique identifier for each message
-
-        # Check if the message is a retry and already processed
-        if message_id in self.processed_messages:
-            return
-
-        try:
-            if req.type == "events_api":
-                if self.is_valid_message(event, bot_user_id):
-                    text = event.get("text", "")
-                    channel_id = event["channel"]
-
-                    if "?" in text:
-                        # Handle question message
-                        self.answer_question(channel_id, text, event.get("ts"), web_client)
-                    else:
-                        # Handle non-question message
-                        self.send_default_response(text, channel_id, event.get("ts"), web_client)
-
-                    # Add the message ID to the processed set
-                    self.processed_messages.add(message_id)
-
-        except Exception as e:
-            logging.error(f"Error processing event: {e}", exc_info=True)
-
-    def is_valid_message(self, event, bot_user_id):
-        """ Check if the event is a valid user message """
-        return event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id
-
-    def send_default_response(self, text, channel_id, thread_ts, web_client):
-        """ Send a default response to a non-question message """
-        response_message = f"I got a message from you saying \"{text}\""
-        web_client.chat_postMessage(channel=channel_id, text=response_message, thread_ts=thread_ts)
-
-    def answer_question(self, channel_id, question, message_id_to_reply_under, web_client):
-        """ Handle a question message """
-        file_name = f"{channel_id}_context.txt"
-        context = self.fetch_recent_messages(channel_id, web_client)
-        self.save_context_to_file(context, file_name)
-
-        response_text = self.generate_response(question, file_name)
-        print("*"*100)
-        print(response_text)
-        print("*"*100)
-        # Send the extracted text as a response in Slack
-        web_client.chat_postMessage(channel=channel_id, text=response_text, thread_ts=message_id_to_reply_under)
-
-    def fetch_recent_messages(self, channel_id, web_client):
-        """ Fetch recent messages for context """
-        response = web_client.conversations_history(channel=channel_id, limit=100)
-        return "\n".join([msg.get('text') for msg in response.get('messages', [])])
-
-    def save_context_to_file(self, context, file_name):
-        """ Save the fetched context to a file """
-        with open(os.path.join(file_system_path, file_name), 'w') as file:
-            file.write(context)
-
-    def generate_response(self, question, file_name):
-        """ Generate a response for a question """
-        file_id = file_name[:-4]
-        relevant_document_ids = retrieve_relevant_documents(question)
-        relevant_document_ids.append(file_id)
-        response_obj = get_response_from_assistant(question, relevant_document_ids)
-
-        # Extract the text from the first assistant message
-        response_text = "No valid response found."  # Default response
-        for message in response_obj.data:
-            if message.role == "assistant":
-                response_text = message.content[0].text.value
-                break  # Assuming you only need the first assistant message
-
-        return response_text
-
-# Handler for reactions added to messages
-class ReactionHandler(SlackEventHandler):
-    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
-        if req.type == "events_api":
-            event = req.payload.get("event", {})
-            if event.get("type") == "reaction_added" and event.get("item_user") == bot_user_id:
-                reaction = event.get("reaction")
-                channel_id = event.get("item", {}).get("channel")
-                message_ts = event.get("item", {}).get("ts")
-                response_message = f"I saw a :{reaction}: reaction on my message with timestamp {message_ts}"
-                web_client.chat_postMessage(channel=channel_id, text=response_message)
-
-
-# Main SlackBot class to initialize and manage the bot
-class SlackBot:
-    def __init__(self, token: str, app_token: str, bot_user_id: str, event_handlers: List[SlackEventHandler]):
-        self.web_client = WebClient(token=token)
-        self.socket_mode_client = SocketModeClient(app_token=app_token, web_client=self.web_client)
-        self.bot_user_id = bot_user_id
-        self.event_handlers = event_handlers
-
-    def start(self):
-        from functools import partial
-        for event_handler in self.event_handlers:
-            event_handler_func = partial(event_handler.handle, web_client=self.web_client, bot_user_id=self.bot_user_id)
-            self.socket_mode_client.socket_mode_request_listeners.append(event_handler_func)
-
-        self.socket_mode_client.connect()
-        try:
-            while True:
-                logging.debug("Bot is running...")
-                time.sleep(10)
-        except KeyboardInterrupt:
-            logging.info("Bot stopped by the user")
-        except Exception as e:
-            logging.critical("Bot stopped due to an exception", exc_info=True)
-
-
-# Start the bot if this script is run directly
-if __name__ == "__main__":
-    logging.basicConfig(level=logging.DEBUG)
-    bot_user_id = "U069C17DCE5"  # Replace with your bot's actual user ID
-    event_handlers = [ChannelMessageHandler(), ReactionHandler()]
-    bot = SlackBot(slack_bot_user_oauth_token, slack_app_level_token, bot_user_id, event_handlers)
-    bot.start()

67363b15ed1ccf691e11cfa9fa89ecf4e5a9fd2b
Author: Roland Abou Younes
Date: Sat Dec 9 15:15:36 2023 +0200
Message: Added docstrings to the File manager module

diff --git a/file_system/file_manager.py b/file_system/file_manager.py
index 0a500a4..54f2114 100644
--- a/file_system/file_manager.py
+++ b/file_system/file_manager.py
@@ -2,40 +2,86 @@
 import os
 from configuration import file_system_path

-"""
-A module to manage file system operations.
-Creating files, deleting files, moving files, listing files in file_system_path directory.=
-"""
+# A module to manage file system operations.
+# Includes functionalities for creating, deleting, adding to, and listing files in the file_system_path directory.
+# The file_system_path directory is specified in the configuration.py file.


 class FileManager:
-    """ Class to manage file system operations."""
+    """
+    Class to manage file system operations in a specified directory.
+
+    Attributes:
+    file_system_path (str): The path to the directory where file operations are performed.
+    """
     def __init__(self):
+        """
+        Initializes the FileManager with a specific directory path.
+        """
         self.file_system_path = file_system_path

     def create(self, file_name, file_content):
-        """ Create a file in file_system_path directory."""
+        """
+        Create a file with specified content in the file system path.
+
+        Args:
+        file_name (str): The name of the file to be created.
+        file_content (str): The content to be written to the file.
+
+        Returns:
+        str: Confirmation message that the file has been created.
+        """
         with open(os.path.join(self.file_system_path, file_name), 'w') as file_object:
             file_object.write(file_content)
             return f"File {file_name} has been created."

     def delete(self, file_name):
-        """ Delete a file in file_system_path directory."""
+        """
+        Delete a file from the file system path.
+
+        Args:
+        file_name (str): The name of the file to be deleted.
+
+        Returns:
+        str: Confirmation message that the file has been deleted.
+        """
         os.remove(os.path.join(self.file_system_path, file_name))
         return f"File {file_name} has been deleted."

     def list(self):
-        """ List all files in file_system_path directory."""
+        """
+        List all files in the file system path directory.
+
+        Returns:
+        list: A list of file names in the directory.
+        """
         return os.listdir(self.file_system_path)

     def add_content(self, file_name, file_content):
-        """ Add a file in file_system_path directory."""
+        """
+        Add content to an existing file in the file system path.
+
+        Args:
+        file_name (str): The name of the file to be appended to.
+        file_content (str): The content to be added to the file.
+
+        Returns:
+        str: Confirmation message that the file has been updated.
+        """
         with open(os.path.join(self.file_system_path, file_name), 'a') as file_object:
             file_object.write(file_content)
             return f"File {file_name} has been added."

     def read(self, file_name):
-        """ Read a file in file_system_path directory."""
+        """
+        Read and return the content of a file in the file system path.
+
+        Args:
+        file_name (str): The name of the file to be read.
+
+        Returns:
+        str: The content of the file.
+        """
         with open(os.path.join(self.file_system_path, file_name), 'r') as file_object:
             return file_object.read()


ef7211dc620ed5d5f621120dfb5bf6b487e84672
Author: Roland Abou Younes
Date: Sat Dec 9 14:50:20 2023 +0200
Message: Updated README.md

diff --git a/README.md b/README.md
index e5144d3..d1521db 100644
--- a/README.md
+++ b/README.md
@@ -7,9 +7,12 @@ The self actualizing documentation framework that heals its knowledge gaps as na
 - Pulls the confluence space and stores it in a sqlite database
 - Vectorizes the confluence space pages and stores the embeds in a chroma db collection
 - Uses the vectorized embeds to find the most similar pages to a question
-- Creates an assistant with the relevant pages and allows it to engage to provide the answer if confident enough
-### Todo:
+- Creates an assistant with the relevant pages and allows it to engage to provide the answer
 - Listens on specific slack channels for questions relevant to its domain
+
+### Todo:
+- Implement fast response using Gpt-4 Turbo without assistant
+- Implement Pulsar message infrastructure for scalability and resilience
 - Gets user feedback to either increase confidence or decrease confidence
 - If confidence is below a certain threashold the assistant will add the question to a trivia quizz and runs it with the specialist team and recommends the update in a confluence comment


e054cadbef7f60eac76f7c05c35116e215bdcda1
Author: Roland Abou Younes
Date: Sat Dec 9 14:42:43 2023 +0200
Message: Refactored database modules to add docstrings and commenbts

diff --git a/database/confluence_database.py b/database/confluence_database.py
index 3a885b7..4f42a4f 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -6,12 +6,15 @@ from datetime import datetime
 from configuration import sql_file_path


-# Define the base class
+# Define the base class for SQLAlchemy models
 Base = declarative_base()


 # Define the SpaceData model
 class SpaceData(Base):
+    """
+    SQLAlchemy model for storing Confluence space data.
+    """
     __tablename__ = 'space_data'

     id = Column(Integer, primary_key=True)
@@ -23,6 +26,9 @@ class SpaceData(Base):

 # Define the PageData model
 class PageData(Base):
+    """
+    SQLAlchemy model for storing Confluence page data.
+    """
     __tablename__ = 'page_data'

     id = Column(Integer, primary_key=True)
@@ -36,29 +42,54 @@ class PageData(Base):
     comments = Column(Text)


-# Setup database connection
+# Setup the database engine and create tables if they don't exist
 engine = create_engine('sqlite:///' + sql_file_path)
 Base.metadata.bind = engine
 Base.metadata.create_all(engine)
+
+# Create a sessionmaker object to manage database sessions
 Session = sessionmaker(bind=engine)
 session = Session()


 def store_space_data(space_data):
+    """
+    Store Confluence space data into the database.
+
+    Args:
+    space_data (dict): A dictionary containing space data to store.
+    """
+    # Create a new SpaceData object and add it to the session
     new_space = SpaceData(space_key=space_data['space_key'],
                           url=space_data['url'],
                           login=space_data['login'],
                           token=space_data['token'])
     session.add(new_space)
     session.commit()
-    print(f"Space data written to database")
+    print(f"Space with key {space_data['space_key']} written to database")


 def parse_datetime(date_string):
+    """
+    Convert an ISO format datetime string to a datetime object.
+
+    Args:
+    date_string (str): ISO format datetime string.
+
+    Returns:
+    datetime: A datetime object.
+    """
     return datetime.fromisoformat(date_string.replace('Z', '+00:00'))


 def store_pages_data(space_key, pages_data):
+    """
+    Store Confluence page data into the database.
+
+    Args:
+    space_key (str): The key of the Confluence space.
+    pages_data (dict): A dictionary of page data, keyed by page ID.
+    """
     for page_id, page_info in pages_data.items():
         created_date = parse_datetime(page_info['createdDate'])
         last_updated = parse_datetime(page_info['lastUpdated'])
@@ -73,6 +104,6 @@ def store_pages_data(space_key, pages_data):
                             comments=page_info['comments'])
         session.add(new_page)
     session.commit()
-    print("Page content written to database.")
+    print(f"Page with ID {page_id} written to database")


diff --git a/database/view_page_data.py b/database/view_page_data.py
index 5c3a54c..92a4247 100644
--- a/database/view_page_data.py
+++ b/database/view_page_data.py
@@ -4,6 +4,9 @@ from configuration import sql_file_path


 def display_page_data():
+    """
+    Display all records in the "page_data" table of the SQLite database.
+    """
     # Connect to the SQLite database
     conn = sqlite3.connect(sql_file_path)
     cursor = conn.cursor()
diff --git a/database/view_space_data.py b/database/view_space_data.py
index f163bfe..bfcc2c6 100644
--- a/database/view_space_data.py
+++ b/database/view_space_data.py
@@ -4,6 +4,9 @@ from configuration import sql_file_path


 def display_space_data():
+    """
+    Display all records in the "space_data" table of the SQLite database.
+    """
     # Connect to the SQLite database
     conn = sqlite3.connect(sql_file_path)
     cursor = conn.cursor()

543602af8be3843b9bd0d560c0d56cc97e5d7cec
Author: Roland Abou Younes
Date: Fri Dec 8 16:15:03 2023 +0200
Message: Refactored retrieve space to add docstrings and proper variable naming

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index bfc9b46..3795132 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -16,18 +16,48 @@ confluence = Confluence(

 # Get top level pages from a space
 def get_top_level_ids(space_key):
+    """
+    Retrieve IDs of top-level pages in a specified Confluence space.
+
+    Args:
+    space_key (str): The key of the Confluence space.
+
+    Returns:
+    list: A list of page IDs for the top-level pages in the space.
+    """
     top_level_pages = confluence.get_all_pages_from_space(space_key)
     return [page['id'] for page in top_level_pages]


 # Get child pages from a page
 def get_child_ids(item_id, content_type):
+    """
+    Retrieve IDs of child items (pages or comments) for a given Confluence item.
+
+    Args:
+    item_id (str): The ID of the Confluence page or comment.
+    content_type (str): Type of content to retrieve ('page' or 'comment').
+
+    Returns:
+    list: A list of IDs for child items.
+    """
     child_items = confluence.get_page_child_by_type(item_id, type=content_type)
     return [child['id'] for child in child_items]


-def get_all_pages_recursive(space_key):
+def get_all_page_ids_recursive(space_key):
+    """
+    Recursively retrieves all page IDs in a given space, including child pages.
+
+    Args:
+    space_key (str): The key of the Confluence space.
+
+    Returns:
+    list: A list of all page IDs in the space.
+    """
+
     def get_child_pages_recursively(page_id):
+        # Inner function to recursively get child page IDs
         child_pages = []
         child_page_ids = get_child_ids(page_id, content_type='page')
         for child_id in child_page_ids:
@@ -44,44 +74,80 @@ def get_all_pages_recursive(space_key):
     return all_pages


-def get_all_comments_recursive(page_id):
-    def get_child_comments_recursively(comment_id):
-        child_comments = []
-        child_comment_ids = get_child_ids(comment_id, content_type='comment')
-        for child_id in child_comment_ids:
-            child_comments.append(child_id)
-            child_comments.extend(get_child_comments_recursively(child_id))
-        return child_comments
+def get_all_comment_ids_recursive(page_id):
+    """
+    Recursively retrieves all comment IDs for a given Confluence page.
+
+    Args:
+    page_id (str): The ID of the Confluence page.
+
+    Returns:
+    list: A list of all comment IDs for the page.
+    """

-    all_comments = []
+    def get_child_comment_ids_recursively(comment_id):
+        # Inner function to recursively get child comment IDs
+        child_comment_ids = []  # Use a separate list to accumulate child comment IDs
+        immediate_child_ids = get_child_ids(comment_id, content_type='comment')
+        for child_id in immediate_child_ids:
+            child_comment_ids.append(child_id)
+            child_comment_ids.extend(get_child_comment_ids_recursively(child_id))
+        return child_comment_ids
+
+    all_comment_ids = []
     top_level_comment_ids = get_child_ids(page_id, content_type='comment')
     for comment_id in top_level_comment_ids:
-        all_comments.append(comment_id)
-        all_comments.extend(get_child_comments_recursively(comment_id))
-    return all_comments
+        all_comment_ids.append(comment_id)
+        all_comment_ids.extend(get_child_comment_ids_recursively(comment_id))
+    return all_comment_ids


 def choose_space():
+    """
+    Prompt the user to choose a Confluence space from a list of available spaces.
+
+    Returns:
+    str: The key of the chosen Confluence space.
+    """
     spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
     for i, space in enumerate(spaces['results']):
         print(f"{i + 1}. {space['name']} (Key: {space['key']})")
     choice = int(input("Choose a space (number): ")) - 1
     space_key = spaces['results'][choice]['key']
-    space_data={'space_key': space_key,
-                'url': confluence_credentials['base_url'],
-                'login': confluence_credentials['username'],
-                'token': confluence_credentials['api_token']
-                }
+    space_data = {'space_key': space_key,
+                  'url': confluence_credentials['base_url'],
+                  'login': confluence_credentials['username'],
+                  'token': confluence_credentials['api_token']
+                  }
     store_space_data(space_data)
     return spaces['results'][choice]['key']


 def strip_html_tags(content):
+    """
+    Remove HTML tags from a string.
+
+    Args:
+    content (str): The string with HTML content.
+
+    Returns:
+    str: The string with HTML tags removed.
+    """
     soup = BeautifulSoup(content, 'html.parser')
     return soup.get_text()


 def check_date_filter(update_date, all_page_ids):
+    """
+    Filter pages based on their last updated date.
+
+    Args:
+    update_date (datetime): The threshold date for filtering. Pages updated after this date will be included.
+    all_page_ids (list): A list of page IDs to be filtered.
+
+    Returns:
+    list: A list of page IDs that were last updated on or after the specified update_date.
+    """
     updated_pages = []
     for page_id in all_page_ids:
         page_history = confluence.history(page_id)  # directly use page_id
@@ -92,7 +158,18 @@ def check_date_filter(update_date, all_page_ids):


 def format_page_content_for_llm(page_data):
-    """ Format page data into a string of key-value pairs for LLM context. """
+    """
+        Format page data into a string of key-value pairs suitable for LLM (Language Learning Models) context.
+
+        This function converts page data into a text format that can be easily consumed by language models,
+        with each key-value pair on a separate line.
+
+        Args:
+        page_data (dict): A dictionary containing page data with keys like title, author, createdDate, etc.
+
+        Returns:
+        str: A string representation of the page data in key-value format.
+        """
     content = ""
     for key, value in page_data.items():
         content += f"{key}: {value}\n"
@@ -100,8 +177,20 @@ def format_page_content_for_llm(page_data):


 def get_space_content(update_date=None):
+    """
+    Retrieve content from a specified Confluence space and process it.
+
+    This function allows the user to choose a Confluence space, retrieves all relevant page and comment data,
+    formats it, and stores it both in files and a database.
+
+    Args:
+    update_date (datetime, optional): If provided, only pages updated after this date will be retrieved. Default is None.
+
+    Returns:
+    list: A list of IDs of all pages that were processed.
+    """
     space_key = choose_space()
-    all_page_ids = get_all_pages_recursive(space_key)
+    all_page_ids = get_all_page_ids_recursive(space_key)
     if update_date is not None:
         all_page_ids = check_date_filter(update_date, all_page_ids)

@@ -116,9 +205,9 @@ def get_space_content(update_date=None):
         last_updated = page['version']['when']
         page_content = strip_html_tags(page.get('body', {}).get('storage', {}).get('value', ''))
         page_comments_content = ""
-        page_comments = get_all_comments_recursive(page_id)
+        page_comment_ids = get_all_comment_ids_recursive(page_id)

-        for comment_id in page_comments:
+        for comment_id in page_comment_ids:
             comment = confluence.get_page_by_id(comment_id, expand='body.storage')
             comment_content = comment.get('body', {}).get('storage', {}).get('value', '')
             page_comments_content += strip_html_tags(comment_content)
@@ -150,4 +239,3 @@ if __name__ == "__main__":
     get_space_content()
     # Space update retrieve
     # get_space_content(update_date=datetime(2023, 12, 1, 0, 0, 0))
-

8e3fef2f56dff466383d690363a7a3f1e9419bf3
Author: Roland Abou Younes
Date: Fri Dec 8 02:41:14 2023 +0200
Message: # testing git on OAI Assistants

diff --git a/oai_assistants/thread_manager.py b/oai_assistants/thread_manager.py
index 087f6da..390bcfa 100644
--- a/oai_assistants/thread_manager.py
+++ b/oai_assistants/thread_manager.py
@@ -117,3 +117,4 @@ class ThreadManager:
         for message in messages.data:
             if message.role == "assistant":
                 print(f"Assistant: {message.content[0].text.value}")
+

d5875bb706d92bfe3c28500fe9b92af3438995ec
Author: Roland Abou Younes
Date: Fri Dec 8 02:17:29 2023 +0200
Message: Channel reaction 2 works with questions and gets a response and posts to slack in thread

diff --git a/slack/channel_reaction2.py b/slack/channel_reaction2.py
index a01ba63..d5c7fa0 100644
--- a/slack/channel_reaction2.py
+++ b/slack/channel_reaction2.py
@@ -43,9 +43,11 @@ class ChannelMessageHandler(SlackEventHandler):
                     channel_id = event["channel"]

                     if "?" in text:
+                        # Handle question message
                         self.answer_question(channel_id, text, event.get("ts"), web_client)
                     else:
-                        self.send_default_response(text, channel_id, web_client)
+                        # Handle non-question message
+                        self.send_default_response(text, channel_id, event.get("ts"), web_client)

                     # Add the message ID to the processed set
                     self.processed_messages.add(message_id)
@@ -57,24 +59,23 @@ class ChannelMessageHandler(SlackEventHandler):
         """ Check if the event is a valid user message """
         return event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id

-    def send_default_response(self, text, channel_id, web_client):
+    def send_default_response(self, text, channel_id, thread_ts, web_client):
         """ Send a default response to a non-question message """
         response_message = f"I got a message from you saying \"{text}\""
-        web_client.chat_postMessage(channel=channel_id, text=response_message)
+        web_client.chat_postMessage(channel=channel_id, text=response_message, thread_ts=thread_ts)

     def answer_question(self, channel_id, question, message_id_to_reply_under, web_client):
         """ Handle a question message """
         file_name = f"{channel_id}_context.txt"
         context = self.fetch_recent_messages(channel_id, web_client)
         self.save_context_to_file(context, file_name)
-        response = self.generate_response(question, file_name)
-        web_client.chat_postMessage(channel=channel_id, text=response, thread_ts=message_id_to_reply_under)
-        logging.debug(f"Sent question response to channel {channel_id}")

-    def send_default_response(self, text, channel_id, web_client):
-        """ Send a default response to a non-question message """
-        response_message = f"I got a message from you saying \"{text}\""
-        web_client.chat_postMessage(channel=channel_id, text=response_message)
+        response_text = self.generate_response(question, file_name)
+        print("*"*100)
+        print(response_text)
+        print("*"*100)
+        # Send the extracted text as a response in Slack
+        web_client.chat_postMessage(channel=channel_id, text=response_text, thread_ts=message_id_to_reply_under)

     def fetch_recent_messages(self, channel_id, web_client):
         """ Fetch recent messages for context """
@@ -91,10 +92,16 @@ class ChannelMessageHandler(SlackEventHandler):
         file_id = file_name[:-4]
         relevant_document_ids = retrieve_relevant_documents(question)
         relevant_document_ids.append(file_id)
-        return get_response_from_assistant(question, relevant_document_ids)
-
+        response_obj = get_response_from_assistant(question, relevant_document_ids)

+        # Extract the text from the first assistant message
+        response_text = "No valid response found."  # Default response
+        for message in response_obj.data:
+            if message.role == "assistant":
+                response_text = message.content[0].text.value
+                break  # Assuming you only need the first assistant message

+        return response_text

 # Handler for reactions added to messages
 class ReactionHandler(SlackEventHandler):

527a5122a4327c6be2691ef86bc8e8a5809eaa70
Author: Roland Abou Younes
Date: Fri Dec 8 00:44:13 2023 +0200
Message: Channel reaction 2 works with questions and gets a response but cant post it in reply

diff --git a/slack/channel_message_handler.py b/slack/channel_message_handler.py
new file mode 100644
index 0000000..1976c5e
--- /dev/null
+++ b/slack/channel_message_handler.py
@@ -0,0 +1,38 @@
+
+
+class ChannelMessageHandler(SlackEventHandler):
+    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        try:
+            if req.type == "events_api":
+                event = req.payload.get("event", {})
+                if event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id:
+                    text = event.get("text", "")
+                    channel_id = event["channel"]
+
+                    if "?" in text:
+                        file_name = f"{channel_id}_context.txt"
+                        self.answer_question(file_name, text, event.get("ts"), web_client, channel_id)
+                    else:
+                        # Respond to the message as usual
+                        response_message = "Your usual response message here"
+                        web_client.chat_postMessage(channel=channel_id, text=response_message)
+
+        except Exception as e:
+            logging.error(f"Error processing event: {e}", exc_info=True)
+
+    def answer_question(self, file_name, question, message_id_to_reply_under, web_client, channel_id):
+        # Fetch context from recent messages
+        recent_messages = self.fetch_recent_messages(channel_id, web_client)
+        context = "\n".join(recent_messages)
+        with open(file_name, 'w') as file:  # Save context to file
+            file.write(context)
+
+        # Combine context with the question for the LLM
+        combined_input = f"{context}\n\nQuestion: {question}"
+        response = get_response_from_assistant(combined_input, [file_name])
+        web_client.chat_postMessage(channel=channel_id, text=response, thread_ts=message_id_to_reply_under)
+
+    def fetch_recent_messages(self, channel_id, web_client):
+        response = web_client.conversations_history(channel=channel_id, limit=100)
+        return [msg.get('text') for msg in response.get('messages', [])]
+
diff --git a/slack/channel_reaction.py b/slack/channel_reaction.py
index ecb5e99..e268fe8 100644
--- a/slack/channel_reaction.py
+++ b/slack/channel_reaction.py
@@ -5,15 +5,22 @@ from slack_sdk import WebClient
 from slack_sdk.socket_mode import SocketModeClient
 from slack_sdk.socket_mode.response import SocketModeResponse
 from slack_sdk.socket_mode.request import SocketModeRequest
+from typing import List
 from credentials import slack_bot_user_oauth_token, slack_app_level_token
+from file_system.file_manager import FileManager
+from vector.chroma import retrieve_relevant_documents
+from oai_assistants.query_assistant_from_documents import get_response_from_assistant


+
+# Abstract base class for Slack event handlers
 class SlackEventHandler(ABC):
     @abstractmethod
     def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
         pass


+# Handler for messages sent in Slack channels
 class ChannelMessageHandler(SlackEventHandler):
     def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
         try:
@@ -22,11 +29,13 @@ class ChannelMessageHandler(SlackEventHandler):
                 event = req.payload.get("event", {})
                 logging.info(f"Event details: {event}")

+                # Ignore messages from the bot itself
                 if event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id:
                     text = event.get("text", "")
                     channel_id = event["channel"]
                     logging.info(f"Message received: '{text}' in channel {channel_id}")

+                    # Respond to the message
                     response_message = f"I got a message from you saying \"{text}\""
                     web_client.chat_postMessage(channel=channel_id, text=response_message)
                     logging.info(f"Sent response in channel {channel_id}")
@@ -36,19 +45,34 @@ class ChannelMessageHandler(SlackEventHandler):
             logging.error(f"Error processing event: {e}", exc_info=True)


+# Handler for reactions added to messages
+class ReactionHandler(SlackEventHandler):
+    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        if req.type == "events_api":
+            event = req.payload.get("event", {})
+            if event.get("type") == "reaction_added" and event.get("item_user") == bot_user_id:
+                reaction = event.get("reaction")
+                channel_id = event.get("item", {}).get("channel")
+                message_ts = event.get("item", {}).get("ts")
+                response_message = f"I saw a :{reaction}: reaction on my message with timestamp {message_ts}"
+                web_client.chat_postMessage(channel=channel_id, text=response_message)
+
+
+# Main SlackBot class to initialize and manage the bot
 class SlackBot:
-    def __init__(self, token: str, app_token: str, bot_user_id: str, event_handler: SlackEventHandler):
+    def __init__(self, token: str, app_token: str, bot_user_id: str, event_handlers: List[SlackEventHandler]):
         self.web_client = WebClient(token=token)
         self.socket_mode_client = SocketModeClient(app_token=app_token, web_client=self.web_client)
         self.bot_user_id = bot_user_id
-        self.event_handler = event_handler
+        self.event_handlers = event_handlers

     def start(self):
         from functools import partial
-        event_handler = partial(self.event_handler.handle, web_client=self.web_client, bot_user_id=self.bot_user_id)
-        self.socket_mode_client.socket_mode_request_listeners.append(event_handler)
-        self.socket_mode_client.connect()
+        for event_handler in self.event_handlers:
+            event_handler_func = partial(event_handler.handle, web_client=self.web_client, bot_user_id=self.bot_user_id)
+            self.socket_mode_client.socket_mode_request_listeners.append(event_handler_func)

+        self.socket_mode_client.connect()
         try:
             while True:
                 logging.debug("Bot is running...")
@@ -59,9 +83,10 @@ class SlackBot:
             logging.critical("Bot stopped due to an exception", exc_info=True)


+# Start the bot if this script is run directly
 if __name__ == "__main__":
     logging.basicConfig(level=logging.DEBUG)
     bot_user_id = "U069C17DCE5"  # Replace with your bot's actual user ID
-    event_handler = ChannelMessageHandler()
-    bot = SlackBot(slack_bot_user_oauth_token, slack_app_level_token, bot_user_id, event_handler)
+    event_handlers = [ChannelMessageHandler(), ReactionHandler()]
+    bot = SlackBot(slack_bot_user_oauth_token, slack_app_level_token, bot_user_id, event_handlers)
     bot.start()
diff --git a/slack/channel_reaction2.py b/slack/channel_reaction2.py
new file mode 100644
index 0000000..a01ba63
--- /dev/null
+++ b/slack/channel_reaction2.py
@@ -0,0 +1,143 @@
+import logging
+import time
+from abc import ABC, abstractmethod
+from slack_sdk import WebClient
+from slack_sdk.socket_mode import SocketModeClient
+from slack_sdk.socket_mode.response import SocketModeResponse
+from slack_sdk.socket_mode.request import SocketModeRequest
+from typing import List
+from credentials import slack_bot_user_oauth_token, slack_app_level_token
+from configuration import file_system_path
+from file_system.file_manager import FileManager
+from vector.chroma import retrieve_relevant_documents
+from oai_assistants.query_assistant_from_documents import get_response_from_assistant
+import os
+
+
+# Abstract base class for Slack event handlers
+class SlackEventHandler(ABC):
+    @abstractmethod
+    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        pass
+
+
+# Handler for messages sent in Slack channels
+class ChannelMessageHandler(SlackEventHandler):
+    processed_messages = set()  # To keep track of processed message IDs
+
+    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        # Acknowledge the event immediately
+        client.send_socket_mode_response(SocketModeResponse(envelope_id=req.envelope_id))
+
+        event = req.payload.get("event", {})
+        message_id = event.get("client_msg_id")  # Unique identifier for each message
+
+        # Check if the message is a retry and already processed
+        if message_id in self.processed_messages:
+            return
+
+        try:
+            if req.type == "events_api":
+                if self.is_valid_message(event, bot_user_id):
+                    text = event.get("text", "")
+                    channel_id = event["channel"]
+
+                    if "?" in text:
+                        self.answer_question(channel_id, text, event.get("ts"), web_client)
+                    else:
+                        self.send_default_response(text, channel_id, web_client)
+
+                    # Add the message ID to the processed set
+                    self.processed_messages.add(message_id)
+
+        except Exception as e:
+            logging.error(f"Error processing event: {e}", exc_info=True)
+
+    def is_valid_message(self, event, bot_user_id):
+        """ Check if the event is a valid user message """
+        return event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id
+
+    def send_default_response(self, text, channel_id, web_client):
+        """ Send a default response to a non-question message """
+        response_message = f"I got a message from you saying \"{text}\""
+        web_client.chat_postMessage(channel=channel_id, text=response_message)
+
+    def answer_question(self, channel_id, question, message_id_to_reply_under, web_client):
+        """ Handle a question message """
+        file_name = f"{channel_id}_context.txt"
+        context = self.fetch_recent_messages(channel_id, web_client)
+        self.save_context_to_file(context, file_name)
+        response = self.generate_response(question, file_name)
+        web_client.chat_postMessage(channel=channel_id, text=response, thread_ts=message_id_to_reply_under)
+        logging.debug(f"Sent question response to channel {channel_id}")
+
+    def send_default_response(self, text, channel_id, web_client):
+        """ Send a default response to a non-question message """
+        response_message = f"I got a message from you saying \"{text}\""
+        web_client.chat_postMessage(channel=channel_id, text=response_message)
+
+    def fetch_recent_messages(self, channel_id, web_client):
+        """ Fetch recent messages for context """
+        response = web_client.conversations_history(channel=channel_id, limit=100)
+        return "\n".join([msg.get('text') for msg in response.get('messages', [])])
+
+    def save_context_to_file(self, context, file_name):
+        """ Save the fetched context to a file """
+        with open(os.path.join(file_system_path, file_name), 'w') as file:
+            file.write(context)
+
+    def generate_response(self, question, file_name):
+        """ Generate a response for a question """
+        file_id = file_name[:-4]
+        relevant_document_ids = retrieve_relevant_documents(question)
+        relevant_document_ids.append(file_id)
+        return get_response_from_assistant(question, relevant_document_ids)
+
+
+
+
+# Handler for reactions added to messages
+class ReactionHandler(SlackEventHandler):
+    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        if req.type == "events_api":
+            event = req.payload.get("event", {})
+            if event.get("type") == "reaction_added" and event.get("item_user") == bot_user_id:
+                reaction = event.get("reaction")
+                channel_id = event.get("item", {}).get("channel")
+                message_ts = event.get("item", {}).get("ts")
+                response_message = f"I saw a :{reaction}: reaction on my message with timestamp {message_ts}"
+                web_client.chat_postMessage(channel=channel_id, text=response_message)
+
+
+# Main SlackBot class to initialize and manage the bot
+class SlackBot:
+    def __init__(self, token: str, app_token: str, bot_user_id: str, event_handlers: List[SlackEventHandler]):
+        self.web_client = WebClient(token=token)
+        self.socket_mode_client = SocketModeClient(app_token=app_token, web_client=self.web_client)
+        self.bot_user_id = bot_user_id
+        self.event_handlers = event_handlers
+
+    def start(self):
+        from functools import partial
+        for event_handler in self.event_handlers:
+            event_handler_func = partial(event_handler.handle, web_client=self.web_client, bot_user_id=self.bot_user_id)
+            self.socket_mode_client.socket_mode_request_listeners.append(event_handler_func)
+
+        self.socket_mode_client.connect()
+        try:
+            while True:
+                logging.debug("Bot is running...")
+                time.sleep(10)
+        except KeyboardInterrupt:
+            logging.info("Bot stopped by the user")
+        except Exception as e:
+            logging.critical("Bot stopped due to an exception", exc_info=True)
+
+
+# Start the bot if this script is run directly
+if __name__ == "__main__":
+    logging.basicConfig(level=logging.DEBUG)
+    bot_user_id = "U069C17DCE5"  # Replace with your bot's actual user ID
+    event_handlers = [ChannelMessageHandler(), ReactionHandler()]
+    bot = SlackBot(slack_bot_user_oauth_token, slack_app_level_token, bot_user_id, event_handlers)
+    bot.start()

d0948447da1a6ae391c5b5863aa37bbc3de18fe9
Author: Roland Abou Younes
Date: Thu Dec 7 17:18:34 2023 +0200
Message: working slack event handler with SOLID refactor

diff --git a/slack/channel_reaction.py b/slack/channel_reaction.py
index c8cd317..ecb5e99 100644
--- a/slack/channel_reaction.py
+++ b/slack/channel_reaction.py
@@ -1,61 +1,67 @@
 import logging
 import time
+from abc import ABC, abstractmethod
 from slack_sdk import WebClient
 from slack_sdk.socket_mode import SocketModeClient
 from slack_sdk.socket_mode.response import SocketModeResponse
 from slack_sdk.socket_mode.request import SocketModeRequest
 from credentials import slack_bot_user_oauth_token, slack_app_level_token

-# Configure detailed logging
-logging.basicConfig(level=logging.DEBUG)

-# Initialize a Web client
-web_client = WebClient(token=slack_bot_user_oauth_token)
+class SlackEventHandler(ABC):
+    @abstractmethod
+    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        pass

-# Your bot's user ID
-bot_user_id = "U069C17DCE5"  # Replace with your bot's actual user ID

+class ChannelMessageHandler(SlackEventHandler):
+    def handle(self, client: SocketModeClient, req: SocketModeRequest, web_client: WebClient, bot_user_id: str):
+        try:
+            logging.debug(f"Received event: {req.payload}")
+            if req.type == "events_api":
+                event = req.payload.get("event", {})
+                logging.info(f"Event details: {event}")

-# Function to handle events
-def process(client: SocketModeClient, req: SocketModeRequest):
-    try:
-        logging.debug(f"Received event: {req.payload}")
-        if req.type == "events_api":
-            event = req.payload.get("event", {})
-            logging.info(f"Event details: {event}")
+                if event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id:
+                    text = event.get("text", "")
+                    channel_id = event["channel"]
+                    logging.info(f"Message received: '{text}' in channel {channel_id}")

-            # Respond to any message that is not from a bot
-            if event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id:
-                text = event.get("text", "")
-                channel_id = event["channel"]
-                logging.info(f"Message received: '{text}' in channel {channel_id}")
+                    response_message = f"I got a message from you saying \"{text}\""
+                    web_client.chat_postMessage(channel=channel_id, text=response_message)
+                    logging.info(f"Sent response in channel {channel_id}")

-                # Craft the response message
-                response_message = f"I got a message from you saying \"{text}\""
-                web_client.chat_postMessage(channel=channel_id, text=response_message)
-                logging.info(f"Sent response in channel {channel_id}")
+                client.send_socket_mode_response(SocketModeResponse(envelope_id=req.envelope_id))
+        except Exception as e:
+            logging.error(f"Error processing event: {e}", exc_info=True)

-            # Acknowledge the event to Slack
-            client.send_socket_mode_response(SocketModeResponse(envelope_id=req.envelope_id))
-    except Exception as e:
-        logging.error(f"Error processing event: {e}", exc_info=True)

+class SlackBot:
+    def __init__(self, token: str, app_token: str, bot_user_id: str, event_handler: SlackEventHandler):
+        self.web_client = WebClient(token=token)
+        self.socket_mode_client = SocketModeClient(app_token=app_token, web_client=self.web_client)
+        self.bot_user_id = bot_user_id
+        self.event_handler = event_handler

-# Initialize Socket Mode client
-socket_mode_client = SocketModeClient(app_token=slack_app_level_token, web_client=web_client)
+    def start(self):
+        from functools import partial
+        event_handler = partial(self.event_handler.handle, web_client=self.web_client, bot_user_id=self.bot_user_id)
+        self.socket_mode_client.socket_mode_request_listeners.append(event_handler)
+        self.socket_mode_client.connect()

-# Attach the event handler
-socket_mode_client.socket_mode_request_listeners.append(process)
+        try:
+            while True:
+                logging.debug("Bot is running...")
+                time.sleep(10)
+        except KeyboardInterrupt:
+            logging.info("Bot stopped by the user")
+        except Exception as e:
+            logging.critical("Bot stopped due to an exception", exc_info=True)

-# Start listening to Slack events
-socket_mode_client.connect()

-# Keep the process alive
-try:
-    while True:
-        logging.debug("Bot is running...")
-        time.sleep(10)  # Sleep for 10 seconds
-except KeyboardInterrupt:
-    logging.info("Bot stopped by the user")
-except Exception as e:
-    logging.critical("Bot stopped due to an exception", exc_info=True)
+if __name__ == "__main__":
+    logging.basicConfig(level=logging.DEBUG)
+    bot_user_id = "U069C17DCE5"  # Replace with your bot's actual user ID
+    event_handler = ChannelMessageHandler()
+    bot = SlackBot(slack_bot_user_oauth_token, slack_app_level_token, bot_user_id, event_handler)
+    bot.start()

1fc5cfe0abbe9b35fa7517984cb5366112e56f18
Author: Roland Abou Younes
Date: Thu Dec 7 11:50:11 2023 +0200
Message: Slack integration to respond to messages in channel

diff --git a/requirements.txt b/requirements.txt
index 508f2f3..b8913e6 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -6,4 +6,5 @@ beautifulsoup4
 chromadb
 langchain
 tiktoken
-prettytable
\ No newline at end of file
+prettytable
+slack_sdk
diff --git a/slack/__init__.py b/slack/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/slack/channel_reaction.py b/slack/channel_reaction.py
new file mode 100644
index 0000000..c8cd317
--- /dev/null
+++ b/slack/channel_reaction.py
@@ -0,0 +1,61 @@
+import logging
+import time
+from slack_sdk import WebClient
+from slack_sdk.socket_mode import SocketModeClient
+from slack_sdk.socket_mode.response import SocketModeResponse
+from slack_sdk.socket_mode.request import SocketModeRequest
+from credentials import slack_bot_user_oauth_token, slack_app_level_token
+
+# Configure detailed logging
+logging.basicConfig(level=logging.DEBUG)
+
+# Initialize a Web client
+web_client = WebClient(token=slack_bot_user_oauth_token)
+
+# Your bot's user ID
+bot_user_id = "U069C17DCE5"  # Replace with your bot's actual user ID
+
+
+# Function to handle events
+def process(client: SocketModeClient, req: SocketModeRequest):
+    try:
+        logging.debug(f"Received event: {req.payload}")
+        if req.type == "events_api":
+            event = req.payload.get("event", {})
+            logging.info(f"Event details: {event}")
+
+            # Respond to any message that is not from a bot
+            if event.get("type") == "message" and "subtype" not in event and event.get("user") != bot_user_id:
+                text = event.get("text", "")
+                channel_id = event["channel"]
+                logging.info(f"Message received: '{text}' in channel {channel_id}")
+
+                # Craft the response message
+                response_message = f"I got a message from you saying \"{text}\""
+                web_client.chat_postMessage(channel=channel_id, text=response_message)
+                logging.info(f"Sent response in channel {channel_id}")
+
+            # Acknowledge the event to Slack
+            client.send_socket_mode_response(SocketModeResponse(envelope_id=req.envelope_id))
+    except Exception as e:
+        logging.error(f"Error processing event: {e}", exc_info=True)
+
+
+# Initialize Socket Mode client
+socket_mode_client = SocketModeClient(app_token=slack_app_level_token, web_client=web_client)
+
+# Attach the event handler
+socket_mode_client.socket_mode_request_listeners.append(process)
+
+# Start listening to Slack events
+socket_mode_client.connect()
+
+# Keep the process alive
+try:
+    while True:
+        logging.debug("Bot is running...")
+        time.sleep(10)  # Sleep for 10 seconds
+except KeyboardInterrupt:
+    logging.info("Bot stopped by the user")
+except Exception as e:
+    logging.critical("Bot stopped due to an exception", exc_info=True)

ddf82b4407c1e5b27ce3b354f9b1bf842f9c1461
Author: Roland Abou Younes
Date: Wed Dec 6 21:11:58 2023 +0200
Message: Added configuration file

diff --git a/configuration.py b/configuration.py
new file mode 100644
index 0000000..aed219e
--- /dev/null
+++ b/configuration.py
@@ -0,0 +1,8 @@
+# /Users/roland/code/Nur/configuration.py
+
+project_path = "/Users/roland/code/Nur"
+# build file_system_path and database_path from project_path
+file_system_path = project_path + "/content/file_system"
+database_path = project_path + "/content/database"
+vector_folder_path = database_path + "/confluence_page_vectors"
+sql_file_path = database_path + "/confluence_pages_sql.db"

daaa35c04fca02e1338bbfcb7f48e991dd5f2784
Author: Roland Abou Younes
Date: Wed Dec 6 21:11:12 2023 +0200
Message: Clear content to erase database sql and vector and erase page txt files

diff --git a/clear_content.py b/clear_content.py
index d09e587..a8fc596 100644
--- a/clear_content.py
+++ b/clear_content.py
@@ -1,5 +1,4 @@
-# doesnt work needs fixing
-
+# ./clear_content.py
 import os
 import shutil
 from configuration import file_system_path, database_path, vector_folder_path

edab5b4006f0f91470afe584cc480c18c46cf669
Author: Roland Abou Younes
Date: Wed Dec 6 21:10:17 2023 +0200
Message: Clear content to erase database sql and vector and erase page txt files

diff --git a/clear_content.py b/clear_content.py
new file mode 100644
index 0000000..d09e587
--- /dev/null
+++ b/clear_content.py
@@ -0,0 +1,30 @@
+# doesnt work needs fixing
+
+import os
+import shutil
+from configuration import file_system_path, database_path, vector_folder_path
+
+
+def clear_directory(directory):
+    # Check if the directory exists
+    if not os.path.exists(directory):
+        print(f"Directory does not exist: {directory}")
+        return
+
+    for filename in os.listdir(directory):
+        file_path = os.path.join(directory, filename)
+        try:
+            if os.path.isfile(file_path) or os.path.islink(file_path):
+                os.unlink(file_path)
+            elif os.path.isdir(file_path):
+                shutil.rmtree(file_path)
+        except Exception as e:
+            print(f'Failed to delete {file_path}. Reason: {e}')
+
+
+# Clearing directories
+clear_directory(file_system_path)
+clear_directory(database_path)
+clear_directory(vector_folder_path)
+
+print("Content of specified directories has been cleared.")

9cbc775bb75eb8151e6346e5af221710cc725adb
Author: Roland Abou Younes
Date: Wed Dec 6 15:29:01 2023 +0200
Message: Updated setup and run

diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
index 513a780..43fd787 100755
--- a/setup/setup_and_run.sh
+++ b/setup/setup_and_run.sh
@@ -50,8 +50,10 @@ fi
 # Modify this depending on your shell compatibility
 source activate "$env_name" || conda activate "$env_name"

-# Install Python dependencies from the setup directory
-requirements_file="$setup_path/requirements.txt"
+
+
+# Install Python dependencies from the project root directory
+requirements_file="$project_root_path/requirements.txt"
 if [ -f "$requirements_file" ]; then
     echo "Installing Python dependencies from $requirements_file."
     pip install -r "$requirements_file"
@@ -59,8 +61,18 @@ else
     echo "No requirements.txt found in $setup_path. Skipping Python dependencies installation."
 fi

+python "$project_root_path/main.py"
+# Install Python dependencies from the setup directory
+# requirements_file="$setup_path/requirements.txt"
+# if [ -f "$requirements_file" ]; then
+#     echo "Installing Python dependencies from $requirements_file."
+#     pip install -r "$requirements_file"
+# else
+#     echo "No requirements.txt found in $setup_path. Skipping Python dependencies installation."
+# fi
+
 # Add run create_db.sh to this script
 # Start the Docker containers
 # echo "Starting Docker containers from $project_root_path."
-docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone
+# docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone


668bd336293d8b79567cf29ff75e6b5c6c3287e4
Author: Roland Abou Younes
Date: Wed Dec 6 14:48:35 2023 +0200
Message: Updating gitignore

diff --git a/.gitignore b/.gitignore
index 7cc2b73..e2512bc 100644
--- a/.gitignore
+++ b/.gitignore
@@ -10,3 +10,5 @@
 /content/file_system/
 /content/databaseconfluence_data.db
 /content/
+.DS_Store
+*.pyc

127ed152b9477ce37661908272f5091b47b29d7c
Author: Roland Abou Younes
Date: Wed Dec 6 14:46:16 2023 +0200
Message: Adding OAI Assistants

diff --git a/oai_assistants/__init__.py b/oai_assistants/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/oai_assistants/assistant_manager.py b/oai_assistants/assistant_manager.py
new file mode 100644
index 0000000..86d322f
--- /dev/null
+++ b/oai_assistants/assistant_manager.py
@@ -0,0 +1,188 @@
+# ./oai_assistants/assistant_manager.py
+import json
+from oai_assistants.file_manager import FileManager
+from oai_assistants.utility import initiate_client
+
+
+class AssistantManager:
+    """
+    AssistantManager provides functionalities to manage the lifecycle of assistants.
+    It includes creating, listing, loading, updating, and deleting assistants within the GPT-4-Turbo-Assistant environment.
+    """
+    def __init__(self, client):
+        """
+        Initialize the AssistantManager with a client to manage assistants.
+
+        Parameters:
+        client (OpenAI_Client): The client object used for assistant operations.
+        """
+        self.client = client.beta.assistants
+
+    def create_assistant(self, model, name, instructions, tools, description=None, metadata=None):
+        """
+        Create an assistant without files.
+
+        Args:
+            model: ID of the model to use for the assistant.
+            name (optional): The name of the assistant.
+            instructions (optional): Instructions for the system using the assistant.
+            description (optional): A descriptive text for the assistant.
+            metadata (optional): Metadata in key-value format for the assistant.
+            tools (optional): A list of tools enabled on the assistant.
+
+        Returns:
+            The newly created Assistant object.
+        """
+        response = self.client.create(
+            model=model,
+            name=name,
+            instructions=instructions,
+            description=description,
+            metadata=metadata,
+            tools=tools
+        )
+        return response
+
+    def clean_missing_files_from_assistant(self, assistant_id):
+        """
+        Identifies and removes missing file references from an assistant's configuration.
+
+        This function checks the files associated with the specified assistant and identifies any files
+        that are no longer present in the file system. It then updates the assistant's configuration
+        to remove these missing files.
+
+        Parameters:
+        assistant_id (str): The ID of the assistant to be cleaned.
+
+        Returns:
+        list: A list of file IDs that were identified as missing and removed from the assistant.
+        """
+        assistant = self.load_assistant(assistant_id)
+        assistant_file_ids = assistant.file_ids if assistant.file_ids is not None else []
+
+        # Check if the assistant has any file IDs associated with it
+        if not assistant_file_ids:
+            print("No files are associated with this assistant.")
+            return []
+
+        file_manager = FileManager(initiate_client())
+        all_files = file_manager.list()
+        file_ids = list(all_files.keys())
+
+        missing_files = [file_id for file_id in assistant_file_ids if file_id not in file_ids]
+
+        # Check if there are any missing files
+        if missing_files:
+            for missing_file in missing_files:
+                print(f"Deleting missing file {missing_file} from assistant {assistant_id}.")
+                self.client.update(
+                    assistant_id=assistant_id,
+                    file_ids=[file_id for file_id in assistant_file_ids if file_id != missing_file]
+                )
+        else:
+            print("There are no missing files.")
+
+        return missing_files
+
+    def add_file_to_assistant(self, assistant_id, file_id):
+        """
+        Add a file to an assistant's list of files.
+
+        Args:
+            assistant_id: The ID of the assistant being updated.
+            file_id: The ID of the file to add to the assistant.
+
+        Returns:
+            The updated Assistant object.
+        """
+        assistant = self.client.retrieve(assistant_id)
+        existing_file_ids = assistant.file_ids if assistant.file_ids is not None else []
+        updated_file_ids = existing_file_ids + [file_id]
+
+        response = self.client.update(
+            assistant_id=assistant_id,
+            file_ids=updated_file_ids
+        )
+        return response
+
+    def list_assistants(self):
+        """
+        List all assistants.
+
+        Returns:
+            A list of Assistant objects containing details about each assistant.
+        """
+        return self.client.list()
+
+    def load_assistant(self, assistant_id):
+        """
+        Load an assistant's parameters by ID.
+
+        Args:
+            assistant_id: The unique identifier for the assistant.
+
+        Returns:
+            An Assistant object or details about the assistant.
+        """
+        return self.client.retrieve(assistant_id=assistant_id)
+
+    def print_assistant_details(self, assistant_id):
+        """
+        Retrieve and display the parameters of a specific assistant by ID.
+
+        Args:
+            assistant_id: The unique identifier for the assistant.
+        """
+        assistant = self.load_assistant(assistant_id)
+        assistant_details = assistant.model_dump()
+        print(assistant_details)
+
+    def update_assistant_interactively(self, assistant_id):
+        """
+        Interactively update an assistant's parameters.
+
+        Args:
+            assistant_id: The unique identifier for the assistant.
+        """
+        assistant = self.load_assistant(assistant_id)
+        # Retrieve the assistant's existing parameters.
+        updatable_params = {
+            'name': assistant.name,
+            'model': assistant.model,
+            'instructions': assistant.instructions,
+            'description': assistant.description,
+            'metadata': assistant.metadata,
+            'tools': assistant.tools
+        }
+
+        # Interactive update process
+        for param_name, current_value in updatable_params.items():
+            print(f'Current value of {param_name}: {current_value}')
+            user_input = input(f'Press Enter to keep the current value or enter a new value for {param_name}: ').strip()
+            # If the user enters a new value, update the parameter
+            if user_input:
+                if param_name in ['metadata', 'tools']:
+                    # Convert the string input to a Python dictionary using json.loads()
+                    try:
+                        updatable_params[param_name] = json.loads(user_input)
+                    except json.JSONDecodeError as e:
+                        print(f'Error: Invalid JSON for {param_name}. Using the current value instead.')
+                else:
+                    updatable_params[param_name] = user_input
+
+        # After all parameters are reviewed, update the assistant details
+        update_response = self.client.update(assistant_id=assistant_id, **updatable_params)
+        return update_response
+
+    def delete_assistant(self, assistant_id):
+        """
+        Delete an assistant by ID.
+
+        Args:
+            assistant_id: The unique identifier for the assistant.
+
+        Returns:
+            A confirmation message indicating that the assistant was deleted.
+        """
+        response = self.client.delete(assistant_id=assistant_id)
+        return "Assistant deleted successfully."
diff --git a/oai_assistants/file_manager.py b/oai_assistants/file_manager.py
new file mode 100644
index 0000000..6deec71
--- /dev/null
+++ b/oai_assistants/file_manager.py
@@ -0,0 +1,52 @@
+# ./oai_assistants/file_manager.py
+class FileManager:
+    """
+        FileManager handles operations related to file management in the context of the GPT-4-Turbo-Assistant.
+        It provides functionalities to create, list, and delete files within the assistant's environment.
+        """
+    def __init__(self, client):
+        """
+        Initializes the FileManager with a client to manage files.
+
+        Parameters:
+        client (OpenAI_Client): The client object used for file operations.
+        """
+        self.client = client.files
+
+    def create(self, file_path, purpose):
+        """
+        Creates a new file in the assistant's environment.
+
+        Parameters:
+        file_path (str): The path to the file to be uploaded.
+        purpose (str): The purpose of the file.
+
+        Returns:
+        str: The ID of the created file.
+        """
+        with open(file_path, 'rb') as file_object:
+            response = self.client.create(file=file_object, purpose=purpose)
+            return response.id
+
+    def list(self):
+        """
+        Lists all files currently managed by the assistant.
+
+        Returns:
+        dict: A dictionary with file IDs as keys and file details (filename and purpose) as values.
+        """
+        files_data = self.client.list().data
+        return {file.id: {"filename": file.filename, "purpose": file.purpose} for file in files_data}
+
+    def delete(self, file_id):
+        """
+        Deletes a file based on its ID.
+
+        Parameters:
+        file_id (str): The ID of the file to be deleted.
+
+        Returns:
+        str: Confirmation message stating the file has been deleted.
+        """
+        self.client.delete(file_id)
+        return f"File with ID {file_id} has been deleted."
diff --git a/oai_assistants/openai_assistant.py b/oai_assistants/openai_assistant.py
new file mode 100644
index 0000000..c5db3f6
--- /dev/null
+++ b/oai_assistants/openai_assistant.py
@@ -0,0 +1,362 @@
+# ./oai_assistants/openai_assistant.py
+from oai_assistants.utility import new_assistant, select_file_for_upload, initiate_client
+from oai_assistants.file_manager import FileManager
+from oai_assistants.assistant_manager import AssistantManager
+from oai_assistants.thread_manager import ThreadManager
+
+
+def create_assistant(client, new_assistant=new_assistant):
+    """
+    Creates a new assistant based on the provided template.
+
+    Parameters:
+    client: OpenAI client instance used for assistant creation.
+    new_assistant (dict, optional): Template for the new assistant. Defaults to the global `new_assistant`.
+
+    Returns:
+    Assistant: An instance of the created assistant.
+    """
+    assistant_manager = AssistantManager(client)
+    return assistant_manager.create_assistant(
+        new_assistant['model'],
+        new_assistant['name'],
+        new_assistant['instructions'],
+        new_assistant['tools'],
+        new_assistant['description']
+    )
+
+
+def chat_with_assistant(thread_manager):
+    """
+    Facilitates a chat interaction with an assistant using the provided thread manager.
+
+    Parameters:
+    thread_manager (ThreadManager): An instance of ThreadManager to handle the chat thread.
+
+    Returns:
+    None
+    """
+    print("Welcome to the Assistant Chat!")
+    thread_manager.create_thread()
+
+    while True:  # Main chat loop
+        message_files = []
+
+        # File upload loop
+        while True:
+            action = input("1. Add a file\n2. Continue to add your message\nChoose an option: ")
+            if action == "1":
+                file_id = chose_and_upload_file(client, file_path='context_update')
+                if file_id is not None:
+                    message_files.append(file_id)
+                    print(f"File uploaded successfully with ID: {file_id}")
+                else:
+                    print(
+                        "File upload failed or was canceled. Please try again or choose to continue without a file.")
+            elif action == "2":
+                break
+            else:
+                print("Invalid option. Please choose 1 or 2.")
+
+        # Message input loop
+        user_message = ""
+        print("You: \nWrite your message, or write 'QUIT' to abort the chat.")
+        while True:
+            user_input = input()
+            if user_input.lower() == 'quit':
+                print("Exiting chat.")
+                return  # Exit the entire chat function
+            else:
+                user_message += user_input + "\n"
+                if user_input.lower() == 'done':
+                    break
+
+        if user_message.strip():
+            thread_manager.add_message_and_wait_for_reply(user_message, message_files)
+        else:
+            print("No message entered.")
+
+        # After processing, continue the main chat loop
+        print("\nContinue chatting, or type 'QUIT' to exit.")
+
+
+def chose_and_upload_file(client, file_path='context_update'):
+    """
+    Allows the user to select and upload a file from the specified path.
+
+    Parameters:
+    client: OpenAI client instance used for file operations.
+    file_path (str, optional): The path where files are located. Defaults to 'context_update'.
+
+    Returns:
+    str: The ID of the uploaded file, or None if the operation is unsuccessful or canceled.
+    """
+    file_manager = FileManager(client)
+    chosen_file_path = select_file_for_upload(file_path)
+    if chosen_file_path:
+        print("Select the purpose of the file:")
+        print("1. Fine-tune")
+        print("2. Assistants")
+        print("3. Fine-tune results")
+        print("4. Assistants output")
+        purpose_choice = input("Enter the number for the purpose (1, 2, 3 or 4):")
+        if purpose_choice == "1":
+            purpose = "fine-tune"
+        elif purpose_choice == "2":
+            purpose = "assistants"
+        elif purpose_choice == "3":
+            purpose = "fine-tune-results"
+        elif purpose_choice == "4":
+            purpose = "assistants_output"
+        else:
+            print("Invalid option.")
+            return
+        file_id = file_manager.create(chosen_file_path, purpose)
+        print(f"File uploaded successfully with ID: {file_id}")
+        return file_id
+
+
+def add_file_to_assistant(assistant_manager, assistant_id):
+    """
+    Adds a file to the specified assistant.
+
+    Parameters:
+    assistant_manager (AssistantManager): An instance of AssistantManager to handle file addition.
+    assistant_id (str): The ID of the assistant to add the file to.
+
+    Returns:
+    None
+    """
+    file_manager = FileManager(client)
+    files = file_manager.list()
+
+    print("Available Files:")
+    file_list = list(files.items())
+    for index, (file_id, file_data) in enumerate(file_list, start=1):
+        print(f"{index}. {file_data['filename']} (ID: {file_id})")
+
+    file_index = input("Select the number of the file you want to add or '0' to cancel: ")
+    if file_index == '0':
+        print("Operation canceled.")
+        return
+
+    try:
+        file_index = int(file_index) - 1
+        if 0 <= file_index < len(file_list):
+            file_id_to_add = file_list[file_index][0]
+            assistant_manager.add_file_to_assistant(assistant_id, file_id_to_add)
+            print("File added successfully.")
+            assistant_manager.print_assistant_details(assistant_id)
+        else:
+            print("Invalid file number.")
+    except ValueError:
+        print("Invalid input. Please enter a number.")
+
+
+def chose_assistant(assistant_manager, assistants):
+    """
+    Allows the user to select an assistant from a list.
+
+    Parameters:
+    assistant_manager (AssistantManager): An instance of AssistantManager for managing assistants.
+    assistants (list): A list of available assistants.
+
+    Returns:
+    str: The ID of the selected assistant, or None if the operation is canceled.
+    """
+    print("\nSelect an Assistant")
+    print("-------------------")
+    for index, assistant in enumerate(assistants, start=1):
+        print(f"{index}. {assistant.name} (ID: {assistant.id})")
+    print("0. Cancel - Return to the previous menu.")
+    print("-------------------")
+
+    assistant_index = input("Enter the number of the assistant you want to manage or '0' to cancel: ")
+    if assistant_index == '0':
+        print("Operation canceled.")
+        return None
+
+    try:
+        assistant_index = int(assistant_index) - 1
+        if 0 <= assistant_index < len(assistants):
+            selected_assistant = assistants[assistant_index]
+            assistant_id = selected_assistant.id
+            assistant_manager.print_assistant_details(assistant_id)
+            return assistant_id
+        else:
+            print("Invalid assistant number.")
+            return None
+    except ValueError:
+        print("Invalid input. Please enter a number.")
+        return None
+
+
+def chose_assistant_action():
+    """
+    Presents a menu for the user to choose an action to perform on an assistant.
+
+    Returns:
+    str: The selected action as a string.
+    """
+    print("\nChoose an Action for the Assistant")
+    print("------------------------------------")
+    print("1. Chat - Chat with this assistant.")
+    print("2. Add File - Add a file to this assistant.")
+    print("3. Update - Update this assistant's parameters.")
+    print("4. Delete - Delete this assistant.")
+    print("5. Check Files - Cleanup assistant files that are not available.")
+    print("0. Cancel - Return to the previous menu.")
+    print("------------------------------------")
+    action = input("Choose an option (0-5): ")
+    return action
+
+
+def manage_assistants(client):
+    """
+    Provides a management interface for assistants.
+
+    Parameters:
+    client: OpenAI client instance used for managing assistants.
+
+    Returns:
+    None
+    """
+    assistant_manager = AssistantManager(client)
+    assistants = assistant_manager.list_assistants().data
+
+    if not assistants:
+        print("No assistants available.")
+        return
+
+    assistant_id = chose_assistant(assistant_manager, assistants)
+
+    if assistant_id is None:
+        return
+
+    action = chose_assistant_action()
+    if action == '1':
+        # Chat with assistant
+        thread_manager = ThreadManager(client, assistant_id)
+        chat_with_assistant(thread_manager)
+    elif action == '2':
+        # Add file to assistant
+        assistant_manager.clean_missing_files_from_assistant(assistant_id)
+        add_file_to_assistant(assistant_manager, assistant_id)
+    elif action == '3':
+        # Update assistant parameters
+        # Call the interactive update method from AssistantManager
+        assistant_manager.clean_missing_files_from_assistant(assistant_id)
+        assistant_manager.update_assistant_interactively(assistant_id)
+        print("Assistant updated successfully.")
+    elif action == '4':
+        # Delete the assistant
+        delete_message = assistant_manager.delete_assistant(assistant_id)
+        print(delete_message)
+    elif action == '5':
+        assistant_manager.clean_missing_files_from_assistant(assistant_id)
+    elif action == '6':
+        # exit menu
+        print("Operation canceled.")
+    else:
+        print("Invalid action.")
+
+
+def manage_files(client):
+    """
+    Provides a file management interface for managing files associated with assistants.
+
+    Parameters:
+    client: OpenAI client instance used for file management.
+
+    Returns:
+    None
+    """
+    file_manager = FileManager(client)
+
+    while True:
+        files = file_manager.list()
+        print("\nFile Management Menu")
+        print("----------------------")
+        print("1. List Files - Display available files.")
+        print("2. Delete File - Remove a specific file.")
+        print("3. Upload File - Add a new file.")
+        print("0. Cancel - Return to the previous menu.")
+        print("----------------------")
+
+        choice = input("Select an option (0-3): ")
+
+        if choice == '1':
+            print("Available Files:")
+            for file_id, file_data in files.items():
+                print(f"ID: {file_id}, Filename: {file_data['filename']}, Purpose: {file_data['purpose']}")
+        elif choice == '2':
+            if not files:
+                print("No files available to delete.")
+                continue
+
+            print("Select the file to delete:")
+            file_list = list(files.items())
+            for index, (file_id, file_data) in enumerate(file_list, start=1):
+                print(f"{index}. {file_data['filename']} (ID: {file_id})")
+
+            file_index = input("Enter the number of the file you want to delete or '0' to cancel: ")
+            if file_index == '0':
+                continue
+
+            try:
+                file_index = int(file_index) - 1
+                if 0 <= file_index < len(file_list):
+                    file_id_to_delete = file_list[file_index][0]
+                    status = file_manager.delete(file_id_to_delete)
+                    print(f"File deleted: {status}")
+                else:
+                    print("Invalid file number.")
+            except ValueError:
+                print("Invalid input. Please enter a number.")
+        elif choice == '3':
+            file_id = chose_and_upload_file(client)
+            print(f"File uploaded: ID {file_id}")
+        elif choice == '0':
+            print("Exiting File Management Menu.")
+            break
+        else:
+            print("Invalid option. Please select a valid number.")
+
+
+def user_interaction(client):
+    """
+    Provides the main user interaction interface for managing files and assistants.
+
+    Parameters:
+    client: OpenAI client instance used for user interactions.
+
+    Returns:
+    None
+    """
+    while True:
+        print("\nUser Interaction Menu:")
+        print("--------------------------------")
+        print("1. Manage Files - Handle file-related operations.")
+        print("2. Manage Assistants - View, modify, or delete assistants.")
+        print("3. Create a New Assistant - Start the process of creating a new assistant.")
+        print("0. Exit - Exit the user interaction menu.")
+        print("--------------------------------")
+
+        choice = input("Enter your choice (0-3): ")
+        if choice == '1':
+            manage_files(client)
+        elif choice == '2':
+            manage_assistants(client)
+        elif choice == '3':
+            created_assistant = create_assistant(client, new_assistant)
+            print(f"New assistant created with ID: {created_assistant.id}")
+        elif choice == '0':
+            print("Exiting user interaction menu.")
+            break
+        else:
+            print("Invalid choice. Please select a valid option.")
+
+
+if __name__ == "__main__":
+    client = initiate_client()
+    user_interaction(client)
diff --git a/oai_assistants/query_assistant_from_documents.py b/oai_assistants/query_assistant_from_documents.py
new file mode 100644
index 0000000..8247e44
--- /dev/null
+++ b/oai_assistants/query_assistant_from_documents.py
@@ -0,0 +1,78 @@
+# ./oai_assistants/query_assistant_from_documents.py
+from oai_assistants.openai_assistant import create_assistant
+from oai_assistants.utility import initiate_client
+from oai_assistants.file_manager import FileManager
+from oai_assistants.thread_manager import ThreadManager
+from oai_assistants.assistant_manager import AssistantManager
+
+
+def create_new_assistant():
+    """
+    Creates a new assistant with the specified parameters.
+    """
+    client = initiate_client()
+
+    new_assistant = {
+        "model": "gpt-4-1106-preview",
+        "name": "Shams",
+        "instructions": """You are the Q&A Assistant,
+        you know everything about the uploaded files
+        and you review them and answer primarily from their content
+        if you ever answer from outside the files, you will be penalized
+        if you use your knowledge to explain some information from outside the file, you will clearly state that.
+        """,
+        "tools": [{"type": "code_interpreter"}, {"type": "retrieval"}],
+        "description": "The ultimate librarian",
+        "file_ids": []
+    }
+
+    assistant = create_assistant(client, new_assistant)
+    print(assistant)
+    return assistant
+
+
+def add_files_to_assistant(assistant, file_ids):
+    """
+    Adds multiple files to an assistant's list of files.
+    """
+    client = initiate_client()
+    file_manager = FileManager(client)
+    assistant_manager = AssistantManager(client)
+
+    for file_id in file_ids:
+        chosen_file_path = f"/Users/roland/code/Nur/content/file_system/{file_id}.txt"
+        purpose = "assistants"
+        uploaded_file_id = file_manager.create(chosen_file_path, purpose)
+        print(f"File uploaded successfully with ID: {uploaded_file_id}")
+
+        assistant_manager.add_file_to_assistant(assistant.id, uploaded_file_id)
+        print(f"File {chosen_file_path} added to assistant {assistant.id}")
+
+
+def ask_assistant(assistant, question):
+    """
+    Asks an assistant a question.
+    """
+    client = initiate_client()
+    thread_manager = ThreadManager(client, assistant.id)
+    thread_manager.create_thread()
+    question = (f"You will answer the following question with a summary, then provide a comprehensive answer, "
+                f"then provide the references aliasing them as Technical trace: {question}")
+    messages = thread_manager.add_message_and_wait_for_reply(question, [])
+    return messages
+
+
+def get_response_from_assistant(question, page_ids):
+    assistant = create_new_assistant()
+
+    if not isinstance(page_ids, list):
+        page_ids = [page_ids]
+
+    add_files_to_assistant(assistant, page_ids)
+    messages = ask_assistant(assistant, question)
+    # print(messages)
+    return messages
+
+
+if __name__ == "__main__":
+    get_response_from_assistant("Do we support payment matching in our solution? and if the payment is not matched do we already have a way to notify the client that they have a delayed payment?", ["458841", "491570"])
diff --git a/oai_assistants/thread_manager.py b/oai_assistants/thread_manager.py
new file mode 100644
index 0000000..087f6da
--- /dev/null
+++ b/oai_assistants/thread_manager.py
@@ -0,0 +1,119 @@
+# ./oai_assistants/thread_manager.py
+import time
+
+
+class ThreadManager:
+    """
+    Manages threads for asynchronous handling of conversations or operations in the GPT-4-Turbo-Assistant.
+
+    This class provides functionality to create and manage threads, allowing simultaneous operations and
+    conversations. It handles adding messages, waiting for replies, checking the status of operations,
+    and retrieving and displaying messages within these threads.
+
+    Attributes:
+    client (OpenAI_Client): An instance of the client used for handling thread operations.
+    """
+    def __init__(self, client, assistant_id):
+        """
+        Initializes the ThreadManager with a client to manage threads.
+
+        Parameters:
+        client (OpenAI_Client): The client object used for thread operations.
+        """
+        self.client = client
+        self.assistant_id = assistant_id
+        self.thread_id = None
+
+    def create_thread(self):
+        """
+        Creates a new thread with the specified thread ID.
+
+        Parameters:
+        thread_id (str): The identifier for the new thread.
+
+        Returns:
+        None
+        """
+        thread = self.client.beta.threads.create()
+        self.thread_id = thread.id
+
+    def add_message_and_wait_for_reply(self, user_message, message_files):
+        """
+        Adds a message to a thread and waits for the reply.
+
+        Parameters:
+        thread_id (str): The ID of the thread to add the message to.
+        user_message (str): The message from the user to add to the thread.
+        message_files (list): List of file IDs associated with the message.
+
+        Returns:
+        list: A list of messages constituting the conversation thread.
+        """
+        # Add the user's message to the thread
+        self.client.beta.threads.messages.create(
+            thread_id=self.thread_id,
+            role="user",
+            content=user_message,
+            file_ids=message_files
+        )
+
+        # Request the assistant to process the message
+        run = self.client.beta.threads.runs.create(
+            thread_id=self.thread_id,
+            assistant_id=self.assistant_id,
+        )
+
+        # Wait for a response from the assistant
+        run_status = self.check_run_status(run.id)
+        while run_status.status != "completed":
+            print("Waiting for assistant...")
+            time.sleep(20)
+            run_status = self.check_run_status(run.id)
+
+        # Retrieve and display the messages
+        messages = self.retrieve_messages()
+        self.display_messages(messages)
+        return messages
+
+    def check_run_status(self, run_id):
+        """
+        Checks the status of a thread run.
+
+        Parameters:
+        thread_id (str): The ID of the thread to check.
+
+        Returns:
+        str: The current status of the thread run.
+        """
+        return self.client.beta.threads.runs.retrieve(
+            thread_id=self.thread_id,
+            run_id=run_id
+        )
+
+    def retrieve_messages(self):
+        """
+        Retrieves all messages from the specified thread.
+
+        Parameters:
+        thread_id (str): The ID of the thread to retrieve messages from.
+
+        Returns:
+        list: A list of messages from the thread.
+        """
+        return self.client.beta.threads.messages.list(
+            thread_id=self.thread_id
+        )
+
+    def display_messages(self, messages):
+        """
+        Displays the messages from a thread.
+
+        Parameters:
+        messages (list): A list of messages to be displayed.
+
+        Returns:
+        None
+        """
+        for message in messages.data:
+            if message.role == "assistant":
+                print(f"Assistant: {message.content[0].text.value}")
diff --git a/oai_assistants/utility.py b/oai_assistants/utility.py
new file mode 100644
index 0000000..5b206a0
--- /dev/null
+++ b/oai_assistants/utility.py
@@ -0,0 +1,69 @@
+# ./oai_assistants/utility.py
+from openai import OpenAI
+from credentials import oai_api_key
+import os
+
+
+def initiate_client():
+    """
+    Initializes and returns an OpenAI client using the provided API key.
+
+    Returns:
+    OpenAI: An instance of the OpenAI client configured with the specified API key.
+    """
+    client = OpenAI(api_key=oai_api_key)
+    return client
+
+
+def get_all_files_in_path(file_path):
+    """
+    Returns a list of all file paths within the specified directory and its subdirectories.
+    Skips '.DS_Store' files common on macOS.
+
+    Args:
+    - directory (str): The path to the directory.
+
+    Returns:
+    - list: A list of file paths.
+    """
+    all_file_paths = []
+    for root, _, files in os.walk(file_path):
+        for file in files:
+            if file != '.DS_Store':
+                all_file_paths.append(os.path.join(root, file))
+    return all_file_paths
+
+
+# Additional function to select a file for upload
+def select_file_for_upload(file_path):
+    """
+    Presents a list of all files in the specified path and allows the user to select a file for upload.
+
+    Parameters:
+    file_path (str): The path where files are located.
+
+    Returns:
+    str: The path of the selected file, or None if the selection is invalid.
+    """
+    all_files = get_all_files_in_path(file_path)
+    print("Please select a file to upload:")
+    for idx, file_name in enumerate(all_files):
+        print(f"{idx + 1}. {file_name}")
+    selected_index = int(input("Enter the number of the file you want to upload: ")) - 1
+    if 0 <= selected_index < len(all_files):
+        return all_files[selected_index]
+    else:
+        print("Invalid selection.")
+        return None
+
+
+# Sample assistant template used for creating new assistants in the system.
+new_assistant = {
+    "model": "gpt-4-1106-preview",
+    "name": "Laura",
+    "instructions": """You are the ultimate librarian, you know everything about the files attached to you
+     and you review them and answer primarily from their content""",
+    "tools": [{"type": "code_interpreter"}, {"type": "retrieval"}],
+    "description": "The ultimate librarian",
+    "file_ids": []
+}

4b83c5c463a86e21abe609e76ae9b69ecfeab1e8
Author: Roland Abou Younes
Date: Wed Dec 6 13:53:41 2023 +0200
Message: removing old fiel system and schema

diff --git a/file_system_and_Schema.txt b/file_system_and_Schema.txt
new file mode 100644
index 0000000..ef4e2e2
--- /dev/null
+++ b/file_system_and_Schema.txt
@@ -0,0 +1,166 @@
+# Database .schema
+sqlite> .schema
+CREATE TABLE space_data (
+        id INTEGER NOT NULL,
+        space_key VARCHAR,
+        url VARCHAR,
+        login VARCHAR,
+        token VARCHAR,
+        PRIMARY KEY (id)
+);
+CREATE TABLE page_data (
+        id INTEGER NOT NULL,
+        page_id VARCHAR,
+        space_key VARCHAR,
+        title VARCHAR,
+        author VARCHAR,
+        "createdDate" DATETIME,
+        "lastUpdated" DATETIME,
+        content TEXT,
+        comments TEXT,
+        PRIMARY KEY (id)
+);
+CREATE TABLE sqlite_sequence(name,seq);
+
+
+# ls -R
+
+LICENSE
+README.md
+__pycache__
+configuration.py
+confluence_integration
+content
+context
+credentials.py
+database
+file_system
+file_system_and_Schema.txt
+main.py
+oai_assistants
+requirements.txt
+setup
+shams
+vector
+visual
+
+./__pycache__:
+configuration.cpython-38.pyc
+credentials.cpython-38.pyc
+
+./confluence_integration:
+__init__.py
+__pycache__
+context
+retrieve_space.py
+
+./confluence_integration/__pycache__:
+__init__.cpython-38.pyc
+retrieve_space.cpython-38.pyc
+
+./confluence_integration/context:
+confluence.py
+
+./content:
+database
+file_system
+
+./content/database:
+confluence_page_vectors
+confluence_pages_sql.db
+
+./content/database/confluence_page_vectors:
+850bbb99-cf0c-4ce3-890c-28474571b311
+chroma.sqlite3
+
+./content/database/confluence_page_vectors/850bbb99-cf0c-4ce3-890c-28474571b311:
+data_level0.bin
+header.bin
+length.bin
+link_lists.bin
+
+./content/file_system:
+1441795.txt
+458753.txt
+458841.txt
+491558.txt
+491570.txt
+98787.txt
+
+./context:
+
+./database:
+__init__.py
+__pycache__
+clear_database.sh
+confluence_database.py
+view_page_data.py
+view_space_data.py
+
+./database/__pycache__:
+__init__.cpython-38.pyc
+confluence_database.cpython-38.pyc
+
+./file_system:
+__init__.py
+__pycache__
+file_manager.py
+
+./file_system/__pycache__:
+__init__.cpython-38.pyc
+file_manager.cpython-38.pyc
+
+./oai_assistants:
+__init__.py
+__pycache__
+assistant_manager.py
+context_update
+file_manager.py
+openai_assistant.py
+query_assistant_from_documents.py
+thread_manager.py
+utility.py
+
+./oai_assistants/__pycache__:
+__init__.cpython-38.pyc
+assistant_manager.cpython-38.pyc
+file_manager.cpython-38.pyc
+openai_assistant.cpython-38.pyc
+query_assistant_from_documents.cpython-38.pyc
+thread_manager.cpython-38.pyc
+utility.cpython-38.pyc
+
+./oai_assistants/context_update:
+assistant_manager.py
+file_manager.py
+openai_assistant.py
+requirements.txt
+thread_manager.py
+utility.py
+
+./setup:
+__init__.py
+create_db.sh
+requirements.txt
+setup_and_run.sh
+
+./shams:
+NUR.png
+gpt.txt
+
+./vector:
+__init__.py
+__pycache__
+chroma.py
+context
+
+./vector/__pycache__:
+__init__.cpython-38.pyc
+chroma.cpython-38.pyc
+
+./vector/context:
+Chroma langchain documentation.pdf
+chroma.py
+
+./visual:
+NUR.png

c3f0bbaa0f3eab526fbbff82cdd4ee05c104a58e
Author: Roland Abou Younes
Date: Wed Dec 6 13:51:12 2023 +0200
Message: added main module where you cna chose if you want to load a new documentation space or as your question and get an answer.

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index b83b808..bfc9b46 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -1,4 +1,4 @@
-# //Users/roland/code/Nur/confluence_integration/retrieve_space.py
+# ./confluence_integration/retrieve_space.py
 from datetime import datetime
 from bs4 import BeautifulSoup
 from atlassian import Confluence
diff --git a/database/confluence_database.py b/database/confluence_database.py
index 46c64d8..3a885b7 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -1,4 +1,4 @@
-# /Users/roland/code/Nur/database/confluence_database.py
+# ./database/confluence_database.py
 from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, LargeBinary
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import sessionmaker
diff --git a/file_system/file_manager.py b/file_system/file_manager.py
index 063bf97..0a500a4 100644
--- a/file_system/file_manager.py
+++ b/file_system/file_manager.py
@@ -1,3 +1,4 @@
+# ./file_system/file_manager.py
 import os
 from configuration import file_system_path

diff --git a/file_system_and_Schema.txt b/file_system_and_Schema.txt
deleted file mode 100644
index e97fdbb..0000000
--- a/file_system_and_Schema.txt
+++ /dev/null
@@ -1,146 +0,0 @@
-sqlite> .schema
-CREATE TABLE space_data (
-    id INTEGER PRIMARY KEY AUTOINCREMENT,
-    space_key TEXT,
-    url TEXT,
-    login TEXT,
-    token TEXT
-);
-CREATE TABLE sqlite_sequence(name,seq);
-CREATE TABLE page_data (
-    id INTEGER PRIMARY KEY AUTOINCREMENT,
-    page_id TEXT,
-    space_key TEXT,
-    title TEXT,
-    author TEXT,
-    createdDate DATETIME,
-    lastUpdated DATETIME,
-    content TEXT,
-    comments TEXT
-);
-
-ls -R
-
-LICENSE
-OAIAssistants
-README.md
-__pycache__
-configuration.py
-confluence_integration
-content
-context
-credentials.py
-database
-file_system
-file_system_and_Schema.txt
-gpt
-requirements.txt
-setup
-vector
-visual
-
-./OAIAssistants:
-Install_GPT-4-Turbo-Assistant.sh
-README.md
-__pycache__
-chat_bot
-run_openai_assistant.sh
-
-./OAIAssistants/__pycache__:
-
-./OAIAssistants/chat_bot:
-__init__.py
-__pycache__
-assistant_manager.py
-context
-context_update
-file_manager.py
-openai_assistant.py
-thread_manager.py
-utility.py
-
-./OAIAssistants/chat_bot/__pycache__:
-__init__.cpython-38.pyc
-assistant_manager.cpython-38.pyc
-file_manager.cpython-38.pyc
-openai_assistant.cpython-38.pyc
-thread_manager.cpython-38.pyc
-utility.cpython-38.pyc
-
-./OAIAssistants/chat_bot/context:
-
-./OAIAssistants/chat_bot/context_update:
-assistant_manager.py
-file_manager.py
-file_structure.txt
-openai_assistant.py
-requirements.txt
-thread_manager.py
-utility.py
-
-./__pycache__:
-configuration.cpython-38.pyc
-credentials.cpython-38.pyc
-
-./confluence_integration:
-__init__.py
-context
-retrieve_space.py
-
-./confluence_integration/context:
-confluence.py
-
-./content:
-file_system
-
-./content/file_system:
-1441795.txt
-458753.txt
-458841.txt
-491558.txt
-491570.txt
-98787.txt
-
-./context:
-
-./database:
-__init__.py
-__pycache__
-clear_database.sh
-confluence_data.db
-confluence_database.py
-
-./database/__pycache__:
-__init__.cpython-38.pyc
-confluence_database.cpython-38.pyc
-
-./file_system:
-__init__.py
-__pycache__
-file_manager.py
-
-./file_system/__pycache__:
-__init__.cpython-38.pyc
-file_manager.cpython-38.pyc
-
-./gpt:
-NUR.png
-gpt.txt
-
-./setup:
-__init__.py
-create_db.sh
-requirements.txt
-setup_and_run.sh
-
-./vector:
-__init__.py
-chroma.py
-context
-
-./vector/context:
-Chroma langchain documentation.pdf
-chroma.py
-
-./visual:
-NUR.png
diff --git a/vector/chroma.py b/vector/chroma.py
index 08d95b0..66e8275 100644
--- a/vector/chroma.py
+++ b/vector/chroma.py
@@ -1,4 +1,4 @@
-# /Users/roland/code/Nur/vector/chroma.py
+# ./vector/chroma.py
 import sqlite3
 from langchain.embeddings.openai import OpenAIEmbeddings
 from langchain.vectorstores import Chroma
diff --git a/visual/NUR.png b/visual/NUR.png
deleted file mode 100644
index 2d5e1ee..0000000
Binary files a/visual/NUR.png and /dev/null differ

2a41bc8e219ae78d2e47b5cb5fd8118883a19030
Author: Roland Abou Younes
Date: Wed Dec 6 13:38:47 2023 +0200
Message: added main module where you cna chose if you want to load a new documentation space or as your question and get an answer.

diff --git a/README.md b/README.md
index f7b4c64..e5144d3 100644
--- a/README.md
+++ b/README.md
@@ -7,8 +7,8 @@ The self actualizing documentation framework that heals its knowledge gaps as na
 - Pulls the confluence space and stores it in a sqlite database
 - Vectorizes the confluence space pages and stores the embeds in a chroma db collection
 - Uses the vectorized embeds to find the most similar pages to a question
-### Todo:
 - Creates an assistant with the relevant pages and allows it to engage to provide the answer if confident enough
+### Todo:
 - Listens on specific slack channels for questions relevant to its domain
 - Gets user feedback to either increase confidence or decrease confidence
 - If confidence is below a certain threashold the assistant will add the question to a trivia quizz and runs it with the specialist team and recommends the update in a confluence comment
@@ -23,3 +23,9 @@ Familiarize yourself with the modules
 git clone https://github.com/MDGrey33/Nur.git
 ````
 Setup script not functional at this point.
+
+## Usage
+1. Add openai api key to credentials
+2. Add confluence credentials to ./credentials
+3. Add project absolute path to ./confiduration
+4. Run the main.py file
diff --git a/database/confluence_database.py b/database/confluence_database.py
index bbd5000..46c64d8 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -76,8 +76,3 @@ def store_pages_data(space_key, pages_data):
     print("Page content written to database.")


-# stores vector data for each page in the sql database
-def store_vector_data(page_id, vector, metadata):
-    new_vector_data = VectorData(page_id=page_id, vector=vector, metadata=metadata)
-    session.add(new_vector_data)
-    session.commit()
\ No newline at end of file
diff --git a/main.py b/main.py
new file mode 100644
index 0000000..a39b4f8
--- /dev/null
+++ b/main.py
@@ -0,0 +1,67 @@
+from confluence_integration.retrieve_space import get_space_content
+from vector.chroma import add_to_vector, retrieve_relevant_documents
+from oai_assistants.query_assistant_from_documents import get_response_from_assistant
+
+
+def add_space():
+    retrieved_page_ids = get_space_content()
+    indexed_page_ids = add_to_vector()
+    return retrieved_page_ids, indexed_page_ids
+
+
+def answer_question(question):
+    relevant_document_ids = retrieve_relevant_documents(question)
+    response = get_response_from_assistant(question, relevant_document_ids)
+    return response
+
+
+question1 = "Do we support payment matching in our solution? and if the payment is not matched do we already have a way to notify the client that they have a delayed payment?"
+
+"""# add_space()
+# print("#"*100 + "\nSpace retrieval and indexing complete\n" + "#"*100)
+print("Question 1: " + question1)
+answer = answer_question(question1)
+# print("Answer: " + answer)
+print(answer)
+print("#"*100 + "\nQuestion 1 answered\n" + "#"*100)
+"""
+
+
+def main_menu():
+    while True:
+        print("\nMain Menu:")
+        print("1. Load New Documentation Space")
+        print("2. Ask a Question to Existing Documentation")
+        print("0. Cancel/Quit")
+        choice = input("Enter your choice (0-2): ")
+
+        if choice == "1":
+            add_space()
+            print("\nSpace retrieval and indexing complete.")
+        elif choice == "2":
+            question = ask_question()
+            if question:
+                answer = answer_question(question)
+                print("\nAnswer:", answer)
+        elif choice == "0":
+            print("Exiting program.")
+            break
+        else:
+            print("Invalid choice. Please enter 0, 1, or 2.")
+
+
+def ask_question():
+    print("\nEnter your question (type 'done' on a new line to submit, 'quit' to cancel):")
+    lines = []
+    while True:
+        line = input()
+        if line.lower() == "done":
+            return "\n".join(lines)
+        elif line.lower() == "quit":
+            return None
+        else:
+            lines.append(line)
+
+
+if __name__ == "__main__":
+    main_menu()
\ No newline at end of file
diff --git a/gpt/NUR.png b/shams/NUR.png
similarity index 100%
rename from gpt/NUR.png
rename to shams/NUR.png
diff --git a/gpt/gpt.txt b/shams/gpt.txt
similarity index 100%
rename from gpt/gpt.txt
rename to shams/gpt.txt
diff --git a/vector/chroma.py b/vector/chroma.py
index 50bed92..08d95b0 100644
--- a/vector/chroma.py
+++ b/vector/chroma.py
@@ -24,13 +24,17 @@ def get_data_from_db():
             f"content: {record[7]}, comments: {record[8]}"
         )
         all_documents.append(document)
+    page_ids = [record[1] for record in records]

     # Close the SQLite connection
     conn.close()
-    return all_documents
+    print('###################################################')
+    print(page_ids)
+    print('###################################################')
+    return all_documents, page_ids


-def vectorize_documents(all_documents):
+def vectorize_documents(all_documents, page_ids):

     # Initialize OpenAI embeddings with the API key
     embedding = OpenAIEmbeddings(openai_api_key=oai_api_key)
@@ -38,19 +42,26 @@ def vectorize_documents(all_documents):
     # Create the Chroma vectorstore with the embedding function
     vectordb = Chroma(embedding_function=embedding, persist_directory=vector_folder_path)

+    # Prepare page_ids to be added as metadata
+    metadatas = [
+        {"page_id": page_id} for page_id in page_ids
+    ]
+
     # Add texts to the vectorstore
-    vectordb.add_texts(texts=all_documents)
+    vectordb.add_texts(texts=all_documents, metadatas=metadatas)
+    # vectordb.upsert_texts(texts=all_documents, metadatas=metadatas)

     # Persist the database
     vectordb.persist()


-def add():
-    all_documents = get_data_from_db()
-    vectorize_documents(all_documents)
+def add_to_vector():
+    all_documents, page_ids = get_data_from_db()
+    vectorize_documents(all_documents, page_ids)
+    return page_ids


-def retrieve(question):
+def retrieve_relevant_documents(question):
     # Initialize OpenAI embeddings with the API key
     embedding = OpenAIEmbeddings(openai_api_key=oai_api_key)

@@ -63,16 +74,24 @@ def retrieve(question):
     # Perform a similarity search in the vectorstore
     similar_documents = vectordb.similarity_search_by_vector(query_embedding)

-    # Process and return the results
-    return [doc.page_content for doc in similar_documents]
+    # Process and return the results along with their metadata
+    results = []
+    for doc in similar_documents:
+        result = {
+            "page_content": doc.page_content,
+            "metadata": doc.metadata
+        }
+        results.append(result)
+    document_ids = [doc.metadata.get('page_id') for doc in similar_documents if doc.metadata]
+
+    return document_ids


 if __name__ == '__main__':
-    add()
+    vectorized_page_ids = add_to_vector()
     question = "do we use any reminder functionality in our solution?"
-    results = retrieve(question)
-    for doc in results:
-        print(doc)
-
-
+    relevant_document_ids = retrieve_relevant_documents(question)
+    for result in relevant_document_ids:
+        print(result)
+        print("---------------------------------------------------")


dab24687bb9651c7a47a750043ab4de526bcbc23
Author: Roland Abou Younes
Date: Tue Dec 5 22:07:04 2023 +0200
Message: Added file path on top of most modules Added view space data and page data so we can see the content of the database.

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index d531ae0..b83b808 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -1,4 +1,4 @@
-import json
+# //Users/roland/code/Nur/confluence_integration/retrieve_space.py
 from datetime import datetime
 from bs4 import BeautifulSoup
 from atlassian import Confluence
diff --git a/database/confluence_data.db b/database/confluence_data.db
deleted file mode 100644
index 17c2547..0000000
Binary files a/database/confluence_data.db and /dev/null differ
diff --git a/database/confluence_database.py b/database/confluence_database.py
index f2e6794..bbd5000 100644
--- a/database/confluence_database.py
+++ b/database/confluence_database.py
@@ -1,7 +1,10 @@
-from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
+# /Users/roland/code/Nur/database/confluence_database.py
+from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text, LargeBinary
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import sessionmaker
 from datetime import datetime
+from configuration import sql_file_path
+

 # Define the base class
 Base = declarative_base()
@@ -34,7 +37,7 @@ class PageData(Base):


 # Setup database connection
-engine = create_engine('sqlite:///../database/confluence_data.db')
+engine = create_engine('sqlite:///' + sql_file_path)
 Base.metadata.bind = engine
 Base.metadata.create_all(engine)
 Session = sessionmaker(bind=engine)
@@ -72,3 +75,9 @@ def store_pages_data(space_key, pages_data):
     session.commit()
     print("Page content written to database.")

+
+# stores vector data for each page in the sql database
+def store_vector_data(page_id, vector, metadata):
+    new_vector_data = VectorData(page_id=page_id, vector=vector, metadata=metadata)
+    session.add(new_vector_data)
+    session.commit()
\ No newline at end of file
diff --git a/database/view_page_data.py b/database/view_page_data.py
new file mode 100644
index 0000000..5c3a54c
--- /dev/null
+++ b/database/view_page_data.py
@@ -0,0 +1,29 @@
+import sqlite3
+from prettytable import PrettyTable
+from configuration import sql_file_path
+
+
+def display_page_data():
+    # Connect to the SQLite database
+    conn = sqlite3.connect(sql_file_path)
+    cursor = conn.cursor()
+
+    # Retrieve all records from the "page_data" table
+    cursor.execute("SELECT * FROM page_data")
+    records = cursor.fetchall()
+
+    # Create a PrettyTable
+    table = PrettyTable()
+    table.field_names = ["ID", "Page ID", "Space Key", "Title", "Author", "Created Date", "Last Updated", "Content", "Comments"]
+
+    for record in records:
+        table.add_row(record)
+
+    # Close the SQLite connection
+    conn.close()
+
+    print(table)
+
+
+if __name__ == "__main__":
+    display_page_data()
diff --git a/database/view_space_data.py b/database/view_space_data.py
new file mode 100644
index 0000000..f163bfe
--- /dev/null
+++ b/database/view_space_data.py
@@ -0,0 +1,29 @@
+import sqlite3
+from prettytable import PrettyTable
+from configuration import sql_file_path
+
+
+def display_space_data():
+    # Connect to the SQLite database
+    conn = sqlite3.connect(sql_file_path)
+    cursor = conn.cursor()
+
+    # Retrieve all records from the "space_data" table
+    cursor.execute("SELECT * FROM space_data")
+    records = cursor.fetchall()
+
+    # Create a PrettyTable
+    table = PrettyTable()
+    table.field_names = ["ID", "Space Key", "URL", "Login", "Token"]
+
+    for record in records:
+        table.add_row(record)
+
+    # Close the SQLite connection
+    conn.close()
+
+    print(table)
+
+
+if __name__ == "__main__":
+    display_space_data()
diff --git a/requirements.txt b/requirements.txt
index 31be83b..508f2f3 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -6,3 +6,4 @@ beautifulsoup4
 chromadb
 langchain
 tiktoken
+prettytable
\ No newline at end of file
diff --git a/vector/chroma.py b/vector/chroma.py
index 56b4ef7..50bed92 100644
--- a/vector/chroma.py
+++ b/vector/chroma.py
@@ -1,12 +1,14 @@
+# /Users/roland/code/Nur/vector/chroma.py
 import sqlite3
 from langchain.embeddings.openai import OpenAIEmbeddings
 from langchain.vectorstores import Chroma
 from credentials import oai_api_key
+from configuration import sql_file_path, vector_folder_path


 def get_data_from_db():
     # Connect to the SQLite database
-    conn = sqlite3.connect('../database/confluence_data.db')
+    conn = sqlite3.connect(sql_file_path)
     cursor = conn.cursor()

     # Retrieve all records from the "page_data" table
@@ -14,12 +16,14 @@ def get_data_from_db():
     records = cursor.fetchall()

     # Process each record into a string
-    all_documents = [
-        f"Page id: {record[1]}, space key: {record[2]}, title: {record[3]}, "
-        f"author: {record[4]}, created date: {record[5]}, last updated: {record[6]}, "
-        f"content: {record[7]}, comments: {record[8]}"
-        for record in records
-    ]
+    all_documents = []
+    for record in records:
+        document = (
+            f"Page id: {record[1]}, space key: {record[2]}, title: {record[3]}, "
+            f"author: {record[4]}, created date: {record[5]}, last updated: {record[6]}, "
+            f"content: {record[7]}, comments: {record[8]}"
+        )
+        all_documents.append(document)

     # Close the SQLite connection
     conn.close()
@@ -32,7 +36,7 @@ def vectorize_documents(all_documents):
     embedding = OpenAIEmbeddings(openai_api_key=oai_api_key)

     # Create the Chroma vectorstore with the embedding function
-    vectordb = Chroma(embedding_function=embedding, persist_directory='db')
+    vectordb = Chroma(embedding_function=embedding, persist_directory=vector_folder_path)

     # Add texts to the vectorstore
     vectordb.add_texts(texts=all_documents)
@@ -51,7 +55,7 @@ def retrieve(question):
     embedding = OpenAIEmbeddings(openai_api_key=oai_api_key)

     # Create the Chroma vectorstore with the embedding function
-    vectordb = Chroma(embedding_function=embedding, persist_directory='db')
+    vectordb = Chroma(embedding_function=embedding, persist_directory=vector_folder_path)

     # Embed the query text using the embedding function
     query_embedding = embedding.embed_query(question)

12e7e77110654d8e39afd7c9b09fa587f9a70d03
Author: Roland Abou Younes
Date: Tue Dec 5 20:55:11 2023 +0200
Message: Query assistant can append many document.

diff --git a/fs.txt b/fs.txt
deleted file mode 100644
index 9ed2882..0000000
--- a/fs.txt
+++ /dev/null
@@ -1,12 +0,0 @@
-vector
-setup
-requirements.txt
-fs.txt
-database
-credentials.py
-context
-confluence_integration
-configuration.py
-__pycache__
-README.md
-LICENSE

ead9d5002473b48390ee2c927eec5163ec83c2c8
Author: Roland Abou Younes
Date: Mon Dec 4 01:49:52 2023 +0200
Message: Updated Readme

diff --git a/README.md b/README.md
index dd83c86..f7b4c64 100644
--- a/README.md
+++ b/README.md
@@ -4,7 +4,7 @@ The self actualizing documentation framework that heals its knowledge gaps as na
 ## Feature list
 ### Done:
 - add a confluence space (url credentials and update interval)
-- Pulls the confluence space and stores it in an sqlite database
+- Pulls the confluence space and stores it in a sqlite database
 - Vectorizes the confluence space pages and stores the embeds in a chroma db collection
 - Uses the vectorized embeds to find the most similar pages to a question
 ### Todo:

3682fc52c0cf5e2c166d93665838d55c5e55e787
Author: Roland Abou Younes
Date: Mon Dec 4 00:28:35 2023 +0200
Message: Updated Git Ignore

diff --git a/.gitignore b/.gitignore
index 45eff43..7cc2b73 100644
--- a/.gitignore
+++ b/.gitignore
@@ -7,3 +7,6 @@
 /confluence_integration/page_content.json
 /vector/db/
 /database/confluence_data.db
+/content/file_system/
+/content/databaseconfluence_data.db
+/content/

7f0d23a4a7c081354f0a8d209e099254474c8105
Author: Roland Abou Younes
Date: Mon Dec 4 00:27:07 2023 +0200
Message: Assistants added and tested working fine.

diff --git a/OAIAssistants b/OAIAssistants
deleted file mode 160000
index 10e1459..0000000
--- a/OAIAssistants
+++ /dev/null
@@ -1 +0,0 @@
-Subproject commit 10e1459d09c248a0c6b601cb85d7b79b38fbb748

0928ddf67e48812966538eb80a195c84c933458b
Author: Roland Abou Younes
Date: Mon Dec 4 00:26:10 2023 +0200
Message: Assistants added and tested working fine

diff --git a/file_system_and_Schema.txt b/file_system_and_Schema.txt
new file mode 100644
index 0000000..e97fdbb
--- /dev/null
+++ b/file_system_and_Schema.txt
@@ -0,0 +1,146 @@
+sqlite> .schema
+CREATE TABLE space_data (
+    id INTEGER PRIMARY KEY AUTOINCREMENT,
+    space_key TEXT,
+    url TEXT,
+    login TEXT,
+    token TEXT
+);
+CREATE TABLE sqlite_sequence(name,seq);
+CREATE TABLE page_data (
+    id INTEGER PRIMARY KEY AUTOINCREMENT,
+    page_id TEXT,
+    space_key TEXT,
+    title TEXT,
+    author TEXT,
+    createdDate DATETIME,
+    lastUpdated DATETIME,
+    content TEXT,
+    comments TEXT
+);
+
+ls -R
+
+LICENSE
+OAIAssistants
+README.md
+__pycache__
+configuration.py
+confluence_integration
+content
+context
+credentials.py
+database
+file_system
+file_system_and_Schema.txt
+gpt
+requirements.txt
+setup
+vector
+visual
+
+./OAIAssistants:
+Install_GPT-4-Turbo-Assistant.sh
+README.md
+__pycache__
+chat_bot
+run_openai_assistant.sh
+
+./OAIAssistants/__pycache__:
+
+./OAIAssistants/chat_bot:
+__init__.py
+__pycache__
+assistant_manager.py
+context
+context_update
+file_manager.py
+openai_assistant.py
+thread_manager.py
+utility.py
+
+./OAIAssistants/chat_bot/__pycache__:
+__init__.cpython-38.pyc
+assistant_manager.cpython-38.pyc
+file_manager.cpython-38.pyc
+openai_assistant.cpython-38.pyc
+thread_manager.cpython-38.pyc
+utility.cpython-38.pyc
+
+./OAIAssistants/chat_bot/context:
+
+./OAIAssistants/chat_bot/context_update:
+assistant_manager.py
+file_manager.py
+file_structure.txt
+openai_assistant.py
+requirements.txt
+thread_manager.py
+utility.py
+
+./__pycache__:
+configuration.cpython-38.pyc
+credentials.cpython-38.pyc
+
+./confluence_integration:
+__init__.py
+context
+retrieve_space.py
+
+./confluence_integration/context:
+confluence.py
+
+./content:
+file_system
+
+./content/file_system:
+1441795.txt
+458753.txt
+458841.txt
+491558.txt
+491570.txt
+98787.txt
+
+./context:
+
+./database:
+__init__.py
+__pycache__
+clear_database.sh
+confluence_data.db
+confluence_database.py
+
+./database/__pycache__:
+__init__.cpython-38.pyc
+confluence_database.cpython-38.pyc
+
+./file_system:
+__init__.py
+__pycache__
+file_manager.py
+
+./file_system/__pycache__:
+__init__.cpython-38.pyc
+file_manager.cpython-38.pyc
+
+./gpt:
+NUR.png
+gpt.txt
+
+./setup:
+__init__.py
+create_db.sh
+requirements.txt
+setup_and_run.sh
+
+./vector:
+__init__.py
+chroma.py
+context
+
+./vector/context:
+Chroma langchain documentation.pdf
+chroma.py
+
+./visual:
+NUR.png
diff --git a/gpt/NUR.png b/gpt/NUR.png
new file mode 100644
index 0000000..2d5e1ee
Binary files /dev/null and b/gpt/NUR.png differ
diff --git a/gpt/gpt.txt b/gpt/gpt.txt
new file mode 100644
index 0000000..3b0f6d9
--- /dev/null
+++ b/gpt/gpt.txt
@@ -0,0 +1,3 @@
+Name: Shams
+Instruction: Always refer to your documentation before answering questions.
+This GPT is designed to provide information about a specific project hosted on GitHub, called 'Nur'. The projects primary function is to integrate with a documentation system and a chat system. It will enable users to interact with the documentation in a conversational manner, answer questions, collect feedback to improve its credibility, and engage in trivia with knowledge owners to identify knowledge gaps and recommend updates to the documentation. This GPT will have access to the project's README and codebase for reference. It is intended for use by the development team to assist in developing the solution and by contributors to learn more about the project.

212128d29c09b2688246ce8f0649a07abcce7648
Author: Roland Abou Younes
Date: Sun Dec 3 02:34:04 2023 +0200
Message: Added database file to Git ignore

diff --git a/.gitignore b/.gitignore
index fd6b3fe..45eff43 100644
--- a/.gitignore
+++ b/.gitignore
@@ -6,3 +6,4 @@
 /confluence_integration/old/
 /confluence_integration/page_content.json
 /vector/db/
+/database/confluence_data.db

6c3e323417e566229900ffbdf5ebe0e0c8d6fe39
Author: Roland Abou Younes
Date: Sun Dec 3 02:32:38 2023 +0200
Message: Added file_manager module to manage file creation. Made confluence retrieve space, create a file for each page with its ID as the name.

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index cacfb95..d531ae0 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -4,6 +4,7 @@ from bs4 import BeautifulSoup
 from atlassian import Confluence
 from credentials import confluence_credentials
 from database.confluence_database import store_space_data, store_pages_data
+from file_system.file_manager import FileManager

 # Initialize Confluence API
 confluence = Confluence(
@@ -60,19 +61,6 @@ def get_all_comments_recursive(page_id):
     return all_comments


-"""
-def get_space_ids(space_key):
-    all_page_ids = get_all_pages_recursive(space_key)
-    page_comments_map = {}
-
-    for page_id in all_page_ids:
-        page_comments = get_all_comments_recursive(page_id)
-        page_comments_map[page_id] = page_comments
-
-    print("Page and Comments Map:", page_comments_map)
-"""
-
-
 def choose_space():
     spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
     for i, space in enumerate(spaces['results']):
@@ -103,32 +91,39 @@ def check_date_filter(update_date, all_page_ids):
     return updated_pages


+def format_page_content_for_llm(page_data):
+    """ Format page data into a string of key-value pairs for LLM context. """
+    content = ""
+    for key, value in page_data.items():
+        content += f"{key}: {value}\n"
+    return content
+

 def get_space_content(update_date=None):
     space_key = choose_space()
     all_page_ids = get_all_pages_recursive(space_key)
     if update_date is not None:
         all_page_ids = check_date_filter(update_date, all_page_ids)
-    page_content_map = {}
+
+    file_manager = FileManager()  # Initialize FileManager instance
+    page_content_map = {}  # For storing page data for database

     for page_id in all_page_ids:
         page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
         page_title = strip_html_tags(page['title'])
-
         page_author = page['history']['createdBy']['displayName']
         created_date = page['history']['createdDate']
         last_updated = page['version']['when']
-
         page_content = strip_html_tags(page.get('body', {}).get('storage', {}).get('value', ''))
-
         page_comments_content = ""
         page_comments = get_all_comments_recursive(page_id)
+
         for comment_id in page_comments:
             comment = confluence.get_page_by_id(comment_id, expand='body.storage')
             comment_content = comment.get('body', {}).get('storage', {}).get('value', '')
             page_comments_content += strip_html_tags(comment_content)

-        page_content_map[page_id] = {
+        page_data = {
             'title': page_title,
             'author': page_author,
             'createdDate': created_date,
@@ -137,13 +132,17 @@ def get_space_content(update_date=None):
             'comments': page_comments_content
         }

-    with open('page_content.json', 'w') as f:
-        json.dump(page_content_map, f)
+        # Store data for database
+        page_content_map[page_id] = page_data
+
+        formatted_content = format_page_content_for_llm(page_data)
+        file_manager.create(f"{page_id}.txt", formatted_content)  # Create a file for each page

+    # Store all page data in the database
     store_pages_data(space_key, page_content_map)
-    print("Page content written to JSON and database.")

-    return page_content_map
+    print("Page content written to individual files and database.")
+    return all_page_ids


 if __name__ == "__main__":
diff --git a/file_system/__init__.py b/file_system/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/file_system/file_manager.py b/file_system/file_manager.py
new file mode 100644
index 0000000..063bf97
--- /dev/null
+++ b/file_system/file_manager.py
@@ -0,0 +1,40 @@
+import os
+from configuration import file_system_path
+
+"""
+A module to manage file system operations.
+Creating files, deleting files, moving files, listing files in file_system_path directory.=
+"""
+
+
+class FileManager:
+    """ Class to manage file system operations."""
+    def __init__(self):
+        self.file_system_path = file_system_path
+
+    def create(self, file_name, file_content):
+        """ Create a file in file_system_path directory."""
+        with open(os.path.join(self.file_system_path, file_name), 'w') as file_object:
+            file_object.write(file_content)
+            return f"File {file_name} has been created."
+
+    def delete(self, file_name):
+        """ Delete a file in file_system_path directory."""
+        os.remove(os.path.join(self.file_system_path, file_name))
+        return f"File {file_name} has been deleted."
+
+    def list(self):
+        """ List all files in file_system_path directory."""
+        return os.listdir(self.file_system_path)
+
+    def add_content(self, file_name, file_content):
+        """ Add a file in file_system_path directory."""
+        with open(os.path.join(self.file_system_path, file_name), 'a') as file_object:
+            file_object.write(file_content)
+            return f"File {file_name} has been added."
+
+    def read(self, file_name):
+        """ Read a file in file_system_path directory."""
+        with open(os.path.join(self.file_system_path, file_name), 'r') as file_object:
+            return file_object.read()
+

4533c8fd4d5dea2d2660c4adb13dd42673055eaa
Author: Roland Abou Younes
Date: Sun Dec 3 01:09:07 2023 +0200
Message: Added OpenAI Assistant. Will be used to bring in context answers form OpenAI.

diff --git a/OAIAssistants b/OAIAssistants
new file mode 160000
index 0000000..10e1459
--- /dev/null
+++ b/OAIAssistants
@@ -0,0 +1 @@
+Subproject commit 10e1459d09c248a0c6b601cb85d7b79b38fbb748

469629581e48339baaddacd56a298a7c36357c04
Author: Roland Abou Younes
Date: Sun Dec 3 01:06:27 2023 +0200
Message: Introduced update space from specific date.

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index b68a024..cacfb95 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -1,4 +1,5 @@
 import json
+from datetime import datetime
 from bs4 import BeautifulSoup
 from atlassian import Confluence
 from credentials import confluence_credentials
@@ -12,11 +13,13 @@ confluence = Confluence(
 )


+# Get top level pages from a space
 def get_top_level_ids(space_key):
     top_level_pages = confluence.get_all_pages_from_space(space_key)
     return [page['id'] for page in top_level_pages]


+# Get child pages from a page
 def get_child_ids(item_id, content_type):
     child_items = confluence.get_page_child_by_type(item_id, type=content_type)
     return [child['id'] for child in child_items]
@@ -54,10 +57,10 @@ def get_all_comments_recursive(page_id):
     for comment_id in top_level_comment_ids:
         all_comments.append(comment_id)
         all_comments.extend(get_child_comments_recursively(comment_id))
-
     return all_comments


+"""
 def get_space_ids(space_key):
     all_page_ids = get_all_pages_recursive(space_key)
     page_comments_map = {}
@@ -67,6 +70,7 @@ def get_space_ids(space_key):
         page_comments_map[page_id] = page_comments

     print("Page and Comments Map:", page_comments_map)
+"""


 def choose_space():
@@ -89,9 +93,22 @@ def strip_html_tags(content):
     return soup.get_text()


-def get_space_content():
+def check_date_filter(update_date, all_page_ids):
+    updated_pages = []
+    for page_id in all_page_ids:
+        page_history = confluence.history(page_id)  # directly use page_id
+        last_updated = datetime.strptime(page_history['lastUpdated']['when'], '%Y-%m-%dT%H:%M:%S.%fZ')
+        if last_updated >= update_date:
+            updated_pages.append(page_id)  # append the page_id to the list
+    return updated_pages
+
+
+
+def get_space_content(update_date=None):
     space_key = choose_space()
     all_page_ids = get_all_pages_recursive(space_key)
+    if update_date is not None:
+        all_page_ids = check_date_filter(update_date, all_page_ids)
     page_content_map = {}

     for page_id in all_page_ids:
@@ -130,4 +147,8 @@ def get_space_content():


 if __name__ == "__main__":
+    # Initial space retrieve
     get_space_content()
+    # Space update retrieve
+    # get_space_content(update_date=datetime(2023, 12, 1, 0, 0, 0))
+
diff --git a/database/confluence_data.db b/database/confluence_data.db
index 5f65928..17c2547 100644
Binary files a/database/confluence_data.db and b/database/confluence_data.db differ
diff --git a/requirements.txt b/requirements.txt
index 47f5b9f..31be83b 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,5 +1,4 @@
 openai
-kafka-python
 requests
 schedule
 sqlalchemy
diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
index 14cb547..513a780 100755
--- a/setup/setup_and_run.sh
+++ b/setup/setup_and_run.sh
@@ -62,5 +62,5 @@ fi
 # Add run create_db.sh to this script
 # Start the Docker containers
 # echo "Starting Docker containers from $project_root_path."
-# docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone
+docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone

diff --git a/visual/NUR.png b/visual/NUR.png
new file mode 100644
index 0000000..2d5e1ee
Binary files /dev/null and b/visual/NUR.png differ

0c1582da0e93459da9d3830aea561f8e9ef5b77a
Author: Roland Abou Younes
Date: Sat Dec 2 10:55:35 2023 +0200
Message: Updated readme: -added the custom GPT for conversational documentation.

diff --git a/README.md b/README.md
index 67563f2..dd83c86 100644
--- a/README.md
+++ b/README.md
@@ -1,6 +1,6 @@
 # Nur
 The self actualizing documentation framework that heals its knowledge gaps as naturally as a ray of light
-
+[Custom GPT to discuss the code base](https://chat.openai.com/g/g-zKBLXtfrD-shams-nur)
 ## Feature list
 ### Done:
 - add a confluence space (url credentials and update interval)
diff --git a/fs.txt b/fs.txt
new file mode 100644
index 0000000..9ed2882
--- /dev/null
+++ b/fs.txt
@@ -0,0 +1,12 @@
+vector
+setup
+requirements.txt
+fs.txt
+database
+credentials.py
+context
+confluence_integration
+configuration.py
+__pycache__
+README.md
+LICENSE

e30f2dd4042d6c22c7a47a9aa9b66dbe541b6fb6
Author: Roland Abou Younes
Date: Sat Dec 2 10:49:32 2023 +0200
Message: Cleanup deleted useless files

diff --git a/README.md b/README.md
index 20d1a59..67563f2 100644
--- a/README.md
+++ b/README.md
@@ -15,22 +15,11 @@ The self actualizing documentation framework that heals its knowledge gaps as na


 ## Setup
-
-
+Still n forming phase
+For now clone the repo
+There is one module in each package
+Familiarize yourself with the modules
 ````
 git clone https://github.com/MDGrey33/Nur.git
 ````
-Download setup_and_run.sh from the repo and make it executable and execute it
-
-````
-cd Nur
-chmod +x ./setup/setup_and_run.sh
-./setup/setup_and_run.sh
-````
-
-The script will load 3 docker images for you 1 of which is the python environment and 2 are Kafka and Kafka Zookeeper
-
-or follow the steps in the script manually.
-
-Ref:
-API documentation: https://atlassian-python-api.readthedocs.io/confluence.html
+Setup script not functional at this point.
diff --git a/context/File Structure Setup and Schema.pdf b/context/File Structure Setup and Schema.pdf
deleted file mode 100644
index b7cd91a..0000000
Binary files a/context/File Structure Setup and Schema.pdf and /dev/null differ

e9983dcbc8a983b7284b3c6587047d83cbd9fb1d
Author: Roland Abou Younes
Date: Sat Dec 2 10:46:02 2023 +0200
Message: Cleanup deleted useless files

diff --git a/README.md b/README.md
index 31baeb2..20d1a59 100644
--- a/README.md
+++ b/README.md
@@ -1,20 +1,21 @@
 # Nur
 The self actualizing documentation framework that heals its knowledge gaps as naturally as a ray of light

-## Rough thoughts
-- add a confluence space (url credentials and update interval)
+## Feature list
+### Done:
+- add a confluence space (url credentials and update interval)
 - Pulls the confluence space and stores it in an sqlite database
 - Vectorizes the confluence space pages and stores the embeds in a chroma db collection
-- Listens on specific slack channels for questions relevant to its domain
 - Uses the vectorized embeds to find the most similar pages to a question
+### Todo:
 - Creates an assistant with the relevant pages and allows it to engage to provide the answer if confident enough
+- Listens on specific slack channels for questions relevant to its domain
 - Gets user feedback to either increase confidence or decrease confidence
 - If confidence is below a certain threashold the assistant will add the question to a trivia quizz and runs it with the specialist team and recommends the update in a confluence comment


 ## Setup
-1. Setup Docker
-2. Git clone the repo
+

 ````
 git clone https://github.com/MDGrey33/Nur.git
diff --git a/confluence_integration/old/comments_summary.py b/confluence_integration/old/comments_summary.py
deleted file mode 100644
index 5bfdfd9..0000000
--- a/confluence_integration/old/comments_summary.py
+++ /dev/null
@@ -1,20 +0,0 @@
-page_content = ""
-comments = ""
-
-summarize_comments_prompt = f"""
-Given the content of a page:
-
-{page_content}
-
-And the associated comments:
-
-{comments}
-
-Please provide a summary of the comments with the following considerations:
-
-1. Identify the key themes discussed in the comments.
-2. Highlight any new insights or additional information that the comments provide, which are not covered in the page content.
-3. Pay attention to any questions raised in the comments and the answers provided.
-4. Note any points of agreement or disagreement among the commenters.
-5. Summarize these elements concisely to enhance the understanding of the page's content.
-"""
\ No newline at end of file
diff --git a/confluence_integration/old/test_confluence.py b/confluence_integration/old/test_confluence.py
deleted file mode 100644
index 44ab7b8..0000000
--- a/confluence_integration/old/test_confluence.py
+++ /dev/null
@@ -1,127 +0,0 @@
-from atlassian import Confluence
-from credentials import confluence_credentials
-
-# Create a Confluence object
-confluence = Confluence(
-    url=confluence_credentials['base_url'],
-    username=confluence_credentials['username'],
-    password=confluence_credentials['api_token']
-    )
-
-
-def get_list_of_spaces(confluence):
-    spaces_response = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
-    for space in spaces_response['results']:
-        print(f"Space Name: {space['name']}, Space Key: {space['key']}")
-
-
-# Get a list of all ids of content items of all types in space and the ids of their children
-def get_all_ids(confluence, space_key):
-    # Recursively fetch child ids of a page
-    def get_child_ids(page_id):
-        child_ids = []
-        # Fetch child pages
-        children = confluence.get_page_child_by_type(page_id, type='page')
-        for child in children:
-            child_ids.append(child['id'])
-            # Recursively fetch children of the child page
-            child_ids.extend(get_child_ids(child['id']))
-
-        # Fetch comments
-        comments = confluence.get_page_child_by_type(page_id, type='comment')
-        for comment in comments:
-            child_ids.append(comment['id'])
-        return child_ids
-
-    all_ids = []
-    pages = confluence.get_all_pages_from_space(space_key)
-    for page in pages:
-        page_id = page['id']
-        all_ids.append(page_id)
-        all_ids.extend(get_child_ids(page_id))
-
-    return all_ids
-
-
-def get_page_details_with_comments(confluence, all_ids_with_comments):
-    pages_details = {}
-
-    for page_id, comment_ids in all_ids_with_comments.items():
-        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
-        title = page['title']
-        body = page['body']['storage']['value']
-
-        comments_content = []
-        for comment_id in comment_ids:
-            comment = confluence.get_content_by_id(comment_id, expand='body.storage,history.createdBy')
-            comment_text = comment['body']['storage']['value']
-            commenter = comment['history']['createdBy']['displayName']
-            created_date = comment['history']['createdDate']
-            comments_content.append(f"Comment by {commenter} on {created_date}: {comment_text}")
-
-        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
-
-    return pages_details
-
-
-
-# Get the list of pages in a space
-def get_pages_in_space(confluence, space_key):
-    pages = confluence.get_all_pages_from_space(space_key, start=0, limit=50, status='current')
-    for page in pages:
-        print(f"Page Title: {page['title']}, Page ID: {page['id']}")
-
-
-def get_page_content_and_comments(confluence, page_id):
-    page_data = confluence.get_page_by_id(page_id, expand='body.storage,title')
-    content = page_data['body']['storage']['value']
-    title = page_data['title']
-
-    print(f"title: {title}, \ncontent: {content}")
-
-
-def get_page_comments(confluence, page_id):
-    comments = confluence.get_page_child_by_type(page_id, type='comment')
-    print(comments)
-
-
-def create_page(confluence, space, title, body):
-    new_page = confluence.create_page(space, title, body)
-    print(new_page)
-
-
-def create_comment(confluence, page_id, comment):
-    confluence.add_comment(page_id, comment)
-    print(comment)
-
-
-def test_read_content():
-    get_list_of_spaces(confluence)
-
-    space_key = 'ST'
-
-    get_pages_in_space(confluence, space_key)
-
-    page_id = 33250  # Replace with your page ID
-
-    get_page_content_and_comments(confluence, page_id)
-
-    get_page_comments(confluence, page_id)
-
-    print(get_all_ids(confluence, space_key))
-
-
-def test_create_content():
-    space_key = 'ST'
-    page_id = 33250  # Replace with your page ID
-
-    for i in range(1, 10):
-        title = f"New Pages {i}"
-        body = "This is the body of our new page.{i}"
-        create_page(confluence, space_key, title, body)
-        for j in range(1, 3):
-            comment = f'Your comment text here.{i}'
-            print(create_comment(confluence, page_id, comment))
-
-
-test_read_content()
diff --git a/main.py b/main.py
deleted file mode 100644
index 5f381d1..0000000
--- a/main.py
+++ /dev/null
@@ -1,89 +0,0 @@
-class SpaceManager:
-    """Manages Confluence space configurations.
-
-    Handles adding new Confluence spaces with associated credentials and update intervals.
-    Publishes space configuration to Pulsar.
-    """
-
-    def add_new_space(self, space_name, credentials, update_interval):
-        pass
-
-
-class PulsarProducer:
-    """Facilitates sending messages to Pulsar.
-
-    Used for publishing various events, such as space additions, updates, and deletions, to Pulsar topics.
-    """
-
-    def send_message(self, topic, message):
-        pass
-
-
-class SpaceConsumer:
-    """Processes new space configurations from Pulsar.
-
-    Consumes messages from Pulsar related to new space additions and stores configurations in SQLite.
-    """
-
-    def consume_message(self):
-        pass
-
-
-class PagePuller:
-    """Retrieves pages and their comments from Confluence spaces.
-
-    Activated for newly added spaces or during scheduled updates, it fetches pages along with their comments and posts the data to Pulsar.
-    """
-
-    def pull_pages_and_comments(self, space_name):
-        pass
-
-
-class PageConsumer:
-    """Handles page data from Pulsar.
-
-    Consumes page data from Pulsar and extracts and stores in the database including metadata (space id, page id, user who created the page, last update date and time) and content (title text, body text, and comment text).
-    """
-
-    def consume_page_data(self):
-        pass
-
-
-class UpdateScheduler:
-    """Schedules update checks for Confluence spaces.
-
-    Triggers periodic checks for updates and deletions in Confluence spaces based on configured intervals.
-    """
-
-    def schedule_updates(self):
-        pass
-
-
-class UpdateProducer:
-    """Sends update and delete events to Pulsar.
-
-    Collects updated and deleted page information from Confluence and publishes it to Pulsar.
-    """
-
-    def send_update(self, update_info):
-        pass
-
-
-class UpdateConsumer:
-    """Processes updates and deletions from Pulsar.
-
-    Listens for update and delete events on Pulsar and updates or removes corresponding pages in SQLite.
-    """
-
-    def consume_updates(self):
-        pass
-
-
-class ChromaSync:
-    """Syncs page data from Pulsar to Chroma DB.
-
-    Listens to Pulsar topics for page data, including new, updated, and deleted pages, and syncs this data with Chroma DB.
-    """
-
-    def sync_with_chroma(self):
-        pass
diff --git a/old/consumer.py b/old/consumer.py
deleted file mode 100644
index 1a662ef..0000000
--- a/old/consumer.py
+++ /dev/null
@@ -1,57 +0,0 @@
-import logging
-from pulsar import Client
-
-# Set up logging
-logging.basicConfig(level=logging.INFO)
-logger = logging.getLogger(__name__)
-
-
-def send_message(service_url, topic, message_content):
-    logger.info("Creating Pulsar client.")
-    client = Client(service_url)
-
-    logger.info(f"Creating producer for topic: {topic}.")
-    producer = client.create_producer(topic)
-
-    logger.info(f"Sending message: {message_content}.")
-    producer.send(message_content.encode('utf-8'))
-
-    logger.info("Message sent. Closing producer.")
-    producer.close()
-
-    logger.info("Producer closed. Closing client.")
-    client.close()
-
-
-def receive_messages(service_url, topic, subscription_name, num_messages):
-    logger.info("Creating Pulsar client.")
-    client = Client(service_url)
-
-    logger.info(f"Subscribing to topic: {topic} with subscription name: {subscription_name}.")
-    consumer = client.subscribe(topic, subscription_name)
-
-    received_messages = []
-    for _ in range(num_messages):
-        logger.info("Waiting to receive a message.")
-        msg = consumer.receive()
-        message_data = msg.data().decode('utf-8')
-        received_messages.append(message_data)
-        logger.info(f"Received message: {message_data}")
-        consumer.acknowledge(msg)
-
-    logger.info("Messages received. Closing consumer.")
-    consumer.close()
-
-    logger.info("Consumer closed. Closing client.")
-    client.close()
-
-    return received_messages
-
-
-# Test sending and receiving a message
-service_url = 'pulsar://localhost:6650'
-topic = 'test-topic'
-message_content = 'Hello, Pulsar!'
-
-logger.info("Starting test: sending a message.")
-send_message(service_url, topic, message_content)
\ No newline at end of file
diff --git a/old/test_pulsar.py b/old/test_pulsar.py
deleted file mode 100644
index b2dd5e8..0000000
--- a/old/test_pulsar.py
+++ /dev/null
@@ -1,61 +0,0 @@
-import logging
-from pulsar import Client
-
-# Set up logging
-logging.basicConfig(level=logging.INFO)
-logger = logging.getLogger(__name__)
-
-
-def send_message(service_url, topic, message_content):
-    logger.info("Creating Pulsar client.")
-    client = Client(service_url)
-
-    logger.info(f"Creating producer for topic: {topic}.")
-    producer = client.create_producer(topic)
-
-    logger.info(f"Sending message: {message_content}.")
-    producer.send(message_content.encode('utf-8'))
-
-    logger.info("Message sent. Closing producer.")
-    producer.close()
-
-    logger.info("Producer closed. Closing client.")
-    client.close()
-
-
-def receive_messages(service_url, topic, subscription_name, num_messages):
-    logger.info("Creating Pulsar client.")
-    client = Client(service_url)
-
-    logger.info(f"Subscribing to topic: {topic} with subscription name: {subscription_name}.")
-    consumer = client.subscribe(topic, subscription_name)
-
-    received_messages = []
-    for _ in range(num_messages):
-        logger.info("Waiting to receive a message.")
-        msg = consumer.receive()
-        message_data = msg.data().decode('utf-8')
-        received_messages.append(message_data)
-        logger.info(f"Received message: {message_data}")
-        consumer.acknowledge(msg)
-
-    logger.info("Messages received. Closing consumer.")
-    consumer.close()
-
-    logger.info("Consumer closed. Closing client.")
-    client.close()
-
-    return received_messages
-
-
-# Test sending and receiving a message
-service_url = 'pulsar://localhost:6650'
-topic = 'test-topic'
-message_content = 'Hello, Pulsar!'
-
-logger.info("Starting test: sending a message.")
-send_message(service_url, topic, message_content)
-
-logger.info("Starting test: receiving messages.")
-messages = receive_messages(service_url, topic, 'test-subscription', 1)
-logger.info(f"Messages received: {messages}")
diff --git a/utility/__init__.py b/utility/__init__.py
deleted file mode 100644
index e69de29..0000000

81c15b6e7bbaccfcbf95b513cf352209111b74f1
Author: Roland Abou Younes
Date: Sat Dec 2 10:41:28 2023 +0200
Message: Minor refactor folder name change to vector

diff --git a/.gitignore b/.gitignore
index 423428a..fd6b3fe 100644
--- a/.gitignore
+++ b/.gitignore
@@ -5,3 +5,4 @@
 /confluence_integration/context/
 /confluence_integration/old/
 /confluence_integration/page_content.json
+/vector/db/
diff --git a/context/File Structure Setup and Schema.pdf b/context/File Structure Setup and Schema.pdf
new file mode 100644
index 0000000..b7cd91a
Binary files /dev/null and b/context/File Structure Setup and Schema.pdf differ
diff --git a/crhoma/__init__.py b/vector/__init__.py
similarity index 100%
rename from crhoma/__init__.py
rename to vector/__init__.py
diff --git a/crhoma/chroma.py b/vector/chroma.py
similarity index 100%
rename from crhoma/chroma.py
rename to vector/chroma.py
diff --git a/vector/context/Chroma langchain documentation.pdf b/vector/context/Chroma langchain documentation.pdf
new file mode 100644
index 0000000..a0239f5
Binary files /dev/null and b/vector/context/Chroma langchain documentation.pdf differ
diff --git a/vector/context/chroma.py b/vector/context/chroma.py
new file mode 100644
index 0000000..1bb5fc5
--- /dev/null
+++ b/vector/context/chroma.py
@@ -0,0 +1,790 @@
+from __future__ import annotations
+
+import base64
+import logging
+import uuid
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    Dict,
+    Iterable,
+    List,
+    Optional,
+    Tuple,
+    Type,
+)
+
+import numpy as np
+from langchain_core.documents import Document
+from langchain_core.embeddings import Embeddings
+from langchain_core.utils import xor_args
+from langchain_core.vectorstores import VectorStore
+
+from langchain.vectorstores.utils import maximal_marginal_relevance
+
+if TYPE_CHECKING:
+    import chromadb
+    import chromadb.config
+    from chromadb.api.types import ID, OneOrMany, Where, WhereDocument
+
+logger = logging.getLogger()
+DEFAULT_K = 4  # Number of Documents to return.
+
+
+def _results_to_docs(results: Any) -> List[Document]:
+    return [doc for doc, _ in _results_to_docs_and_scores(results)]
+
+
+def _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:
+    return [
+        # TODO: Chroma can do batch querying,
+        # we shouldn't hard code to the 1st result
+        (Document(page_content=result[0], metadata=result[1] or {}), result[2])
+        for result in zip(
+            results["documents"][0],
+            results["metadatas"][0],
+            results["distances"][0],
+        )
+    ]
+
+
+class Chroma(VectorStore):
+    """`ChromaDB` vector store.
+
+    To use, you should have the ``chromadb`` python package installed.
+
+    Example:
+        .. code-block:: python
+
+                from langchain.vectorstores import Chroma
+                from langchain.embeddings.openai import OpenAIEmbeddings
+
+                embeddings = OpenAIEmbeddings()
+                vectorstore = Chroma("langchain_store", embeddings)
+    """
+
+    _LANGCHAIN_DEFAULT_COLLECTION_NAME = "langchain"
+
+    def __init__(
+        self,
+        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
+        embedding_function: Optional[Embeddings] = None,
+        persist_directory: Optional[str] = None,
+        client_settings: Optional[chromadb.config.Settings] = None,
+        collection_metadata: Optional[Dict] = None,
+        client: Optional[chromadb.Client] = None,
+        relevance_score_fn: Optional[Callable[[float], float]] = None,
+    ) -> None:
+        """Initialize with a Chroma client."""
+        try:
+            import chromadb
+            import chromadb.config
+        except ImportError:
+            raise ImportError(
+                "Could not import chromadb python package. "
+                "Please install it with `pip install chromadb`."
+            )
+
+        if client is not None:
+            self._client_settings = client_settings
+            self._client = client
+            self._persist_directory = persist_directory
+        else:
+            if client_settings:
+                # If client_settings is provided with persist_directory specified,
+                # then it is "in-memory and persisting to disk" mode.
+                client_settings.persist_directory = (
+                    persist_directory or client_settings.persist_directory
+                )
+                if client_settings.persist_directory is not None:
+                    # Maintain backwards compatibility with chromadb < 0.4.0
+                    major, minor, _ = chromadb.__version__.split(".")
+                    if int(major) == 0 and int(minor) < 4:
+                        client_settings.chroma_db_impl = "duckdb+parquet"
+
+                _client_settings = client_settings
+            elif persist_directory:
+                # Maintain backwards compatibility with chromadb < 0.4.0
+                major, minor, _ = chromadb.__version__.split(".")
+                if int(major) == 0 and int(minor) < 4:
+                    _client_settings = chromadb.config.Settings(
+                        chroma_db_impl="duckdb+parquet",
+                    )
+                else:
+                    _client_settings = chromadb.config.Settings(is_persistent=True)
+                _client_settings.persist_directory = persist_directory
+            else:
+                _client_settings = chromadb.config.Settings()
+            self._client_settings = _client_settings
+            self._client = chromadb.Client(_client_settings)
+            self._persist_directory = (
+                _client_settings.persist_directory or persist_directory
+            )
+
+        self._embedding_function = embedding_function
+        self._collection = self._client.get_or_create_collection(
+            name=collection_name,
+            embedding_function=None,
+            metadata=collection_metadata,
+        )
+        self.override_relevance_score_fn = relevance_score_fn
+
+    @property
+    def embeddings(self) -> Optional[Embeddings]:
+        return self._embedding_function
+
+    @xor_args(("query_texts", "query_embeddings"))
+    def __query_collection(
+        self,
+        query_texts: Optional[List[str]] = None,
+        query_embeddings: Optional[List[List[float]]] = None,
+        n_results: int = 4,
+        where: Optional[Dict[str, str]] = None,
+        where_document: Optional[Dict[str, str]] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Query the chroma collection."""
+        try:
+            import chromadb  # noqa: F401
+        except ImportError:
+            raise ValueError(
+                "Could not import chromadb python package. "
+                "Please install it with `pip install chromadb`."
+            )
+        return self._collection.query(
+            query_texts=query_texts,
+            query_embeddings=query_embeddings,
+            n_results=n_results,
+            where=where,
+            where_document=where_document,
+            **kwargs,
+        )
+
+    def encode_image(self, uri: str) -> str:
+        """Get base64 string from image URI."""
+        with open(uri, "rb") as image_file:
+            return base64.b64encode(image_file.read()).decode("utf-8")
+
+    def add_images(
+        self,
+        uris: List[str],
+        metadatas: Optional[List[dict]] = None,
+        ids: Optional[List[str]] = None,
+        **kwargs: Any,
+    ) -> List[str]:
+        """Run more images through the embeddings and add to the vectorstore.
+
+        Args:
+            images (List[List[float]]): Images to add to the vectorstore.
+            metadatas (Optional[List[dict]], optional): Optional list of metadatas.
+            ids (Optional[List[str]], optional): Optional list of IDs.
+
+        Returns:
+            List[str]: List of IDs of the added images.
+        """
+        # Map from uris to b64 encoded strings
+        b64_texts = [self.encode_image(uri=uri) for uri in uris]
+        # Populate IDs
+        if ids is None:
+            ids = [str(uuid.uuid1()) for _ in uris]
+        embeddings = None
+        # Set embeddings
+        if self._embedding_function is not None and hasattr(
+            self._embedding_function, "embed_image"
+        ):
+            embeddings = self._embedding_function.embed_image(uris=uris)
+        if metadatas:
+            # fill metadatas with empty dicts if somebody
+            # did not specify metadata for all images
+            length_diff = len(uris) - len(metadatas)
+            if length_diff:
+                metadatas = metadatas + [{}] * length_diff
+            empty_ids = []
+            non_empty_ids = []
+            for idx, m in enumerate(metadatas):
+                if m:
+                    non_empty_ids.append(idx)
+                else:
+                    empty_ids.append(idx)
+            if non_empty_ids:
+                metadatas = [metadatas[idx] for idx in non_empty_ids]
+                images_with_metadatas = [uris[idx] for idx in non_empty_ids]
+                embeddings_with_metadatas = (
+                    [embeddings[idx] for idx in non_empty_ids] if embeddings else None
+                )
+                ids_with_metadata = [ids[idx] for idx in non_empty_ids]
+                try:
+                    self._collection.upsert(
+                        metadatas=metadatas,
+                        embeddings=embeddings_with_metadatas,
+                        documents=images_with_metadatas,
+                        ids=ids_with_metadata,
+                    )
+                except ValueError as e:
+                    if "Expected metadata value to be" in str(e):
+                        msg = (
+                            "Try filtering complex metadata using "
+                            "langchain.vectorstores.utils.filter_complex_metadata."
+                        )
+                        raise ValueError(e.args[0] + "\n\n" + msg)
+                    else:
+                        raise e
+            if empty_ids:
+                images_without_metadatas = [uris[j] for j in empty_ids]
+                embeddings_without_metadatas = (
+                    [embeddings[j] for j in empty_ids] if embeddings else None
+                )
+                ids_without_metadatas = [ids[j] for j in empty_ids]
+                self._collection.upsert(
+                    embeddings=embeddings_without_metadatas,
+                    documents=images_without_metadatas,
+                    ids=ids_without_metadatas,
+                )
+        else:
+            self._collection.upsert(
+                embeddings=embeddings,
+                documents=b64_texts,
+                ids=ids,
+            )
+        return ids
+
+    def add_texts(
+        self,
+        texts: Iterable[str],
+        metadatas: Optional[List[dict]] = None,
+        ids: Optional[List[str]] = None,
+        **kwargs: Any,
+    ) -> List[str]:
+        """Run more texts through the embeddings and add to the vectorstore.
+
+        Args:
+            texts (Iterable[str]): Texts to add to the vectorstore.
+            metadatas (Optional[List[dict]], optional): Optional list of metadatas.
+            ids (Optional[List[str]], optional): Optional list of IDs.
+
+        Returns:
+            List[str]: List of IDs of the added texts.
+        """
+        # TODO: Handle the case where the user doesn't provide ids on the Collection
+        if ids is None:
+            ids = [str(uuid.uuid1()) for _ in texts]
+        embeddings = None
+        texts = list(texts)
+        if self._embedding_function is not None:
+            embeddings = self._embedding_function.embed_documents(texts)
+        if metadatas:
+            # fill metadatas with empty dicts if somebody
+            # did not specify metadata for all texts
+            length_diff = len(texts) - len(metadatas)
+            if length_diff:
+                metadatas = metadatas + [{}] * length_diff
+            empty_ids = []
+            non_empty_ids = []
+            for idx, m in enumerate(metadatas):
+                if m:
+                    non_empty_ids.append(idx)
+                else:
+                    empty_ids.append(idx)
+            if non_empty_ids:
+                metadatas = [metadatas[idx] for idx in non_empty_ids]
+                texts_with_metadatas = [texts[idx] for idx in non_empty_ids]
+                embeddings_with_metadatas = (
+                    [embeddings[idx] for idx in non_empty_ids] if embeddings else None
+                )
+                ids_with_metadata = [ids[idx] for idx in non_empty_ids]
+                try:
+                    self._collection.upsert(
+                        metadatas=metadatas,
+                        embeddings=embeddings_with_metadatas,
+                        documents=texts_with_metadatas,
+                        ids=ids_with_metadata,
+                    )
+                except ValueError as e:
+                    if "Expected metadata value to be" in str(e):
+                        msg = (
+                            "Try filtering complex metadata from the document using "
+                            "langchain.vectorstores.utils.filter_complex_metadata."
+                        )
+                        raise ValueError(e.args[0] + "\n\n" + msg)
+                    else:
+                        raise e
+            if empty_ids:
+                texts_without_metadatas = [texts[j] for j in empty_ids]
+                embeddings_without_metadatas = (
+                    [embeddings[j] for j in empty_ids] if embeddings else None
+                )
+                ids_without_metadatas = [ids[j] for j in empty_ids]
+                self._collection.upsert(
+                    embeddings=embeddings_without_metadatas,
+                    documents=texts_without_metadatas,
+                    ids=ids_without_metadatas,
+                )
+        else:
+            self._collection.upsert(
+                embeddings=embeddings,
+                documents=texts,
+                ids=ids,
+            )
+        return ids
+
+    def similarity_search(
+        self,
+        query: str,
+        k: int = DEFAULT_K,
+        filter: Optional[Dict[str, str]] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Run similarity search with Chroma.
+
+        Args:
+            query (str): Query text to search for.
+            k (int): Number of results to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+
+        Returns:
+            List[Document]: List of documents most similar to the query text.
+        """
+        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)
+        return [doc for doc, _ in docs_and_scores]
+
+    def similarity_search_by_vector(
+        self,
+        embedding: List[float],
+        k: int = DEFAULT_K,
+        filter: Optional[Dict[str, str]] = None,
+        where_document: Optional[Dict[str, str]] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs most similar to embedding vector.
+        Args:
+            embedding (List[float]): Embedding to look up documents similar to.
+            k (int): Number of Documents to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+        Returns:
+            List of Documents most similar to the query vector.
+        """
+        results = self.__query_collection(
+            query_embeddings=embedding,
+            n_results=k,
+            where=filter,
+            where_document=where_document,
+        )
+        return _results_to_docs(results)
+
+    def similarity_search_by_vector_with_relevance_scores(
+        self,
+        embedding: List[float],
+        k: int = DEFAULT_K,
+        filter: Optional[Dict[str, str]] = None,
+        where_document: Optional[Dict[str, str]] = None,
+        **kwargs: Any,
+    ) -> List[Tuple[Document, float]]:
+        """
+        Return docs most similar to embedding vector and similarity score.
+
+        Args:
+            embedding (List[float]): Embedding to look up documents similar to.
+            k (int): Number of Documents to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+
+        Returns:
+            List[Tuple[Document, float]]: List of documents most similar to
+            the query text and cosine distance in float for each.
+            Lower score represents more similarity.
+        """
+        results = self.__query_collection(
+            query_embeddings=embedding,
+            n_results=k,
+            where=filter,
+            where_document=where_document,
+        )
+        return _results_to_docs_and_scores(results)
+
+    def similarity_search_with_score(
+        self,
+        query: str,
+        k: int = DEFAULT_K,
+        filter: Optional[Dict[str, str]] = None,
+        where_document: Optional[Dict[str, str]] = None,
+        **kwargs: Any,
+    ) -> List[Tuple[Document, float]]:
+        """Run similarity search with Chroma with distance.
+
+        Args:
+            query (str): Query text to search for.
+            k (int): Number of results to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+
+        Returns:
+            List[Tuple[Document, float]]: List of documents most similar to
+            the query text and cosine distance in float for each.
+            Lower score represents more similarity.
+        """
+        if self._embedding_function is None:
+            results = self.__query_collection(
+                query_texts=[query],
+                n_results=k,
+                where=filter,
+                where_document=where_document,
+            )
+        else:
+            query_embedding = self._embedding_function.embed_query(query)
+            results = self.__query_collection(
+                query_embeddings=[query_embedding],
+                n_results=k,
+                where=filter,
+                where_document=where_document,
+            )
+
+        return _results_to_docs_and_scores(results)
+
+    def _select_relevance_score_fn(self) -> Callable[[float], float]:
+        """
+        The 'correct' relevance function
+        may differ depending on a few things, including:
+        - the distance / similarity metric used by the VectorStore
+        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)
+        - embedding dimensionality
+        - etc.
+        """
+        if self.override_relevance_score_fn:
+            return self.override_relevance_score_fn
+
+        distance = "l2"
+        distance_key = "hnsw:space"
+        metadata = self._collection.metadata
+
+        if metadata and distance_key in metadata:
+            distance = metadata[distance_key]
+
+        if distance == "cosine":
+            return self._cosine_relevance_score_fn
+        elif distance == "l2":
+            return self._euclidean_relevance_score_fn
+        elif distance == "ip":
+            return self._max_inner_product_relevance_score_fn
+        else:
+            raise ValueError(
+                "No supported normalization function"
+                f" for distance metric of type: {distance}."
+                "Consider providing relevance_score_fn to Chroma constructor."
+            )
+
+    def max_marginal_relevance_search_by_vector(
+        self,
+        embedding: List[float],
+        k: int = DEFAULT_K,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        filter: Optional[Dict[str, str]] = None,
+        where_document: Optional[Dict[str, str]] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs selected using the maximal marginal relevance.
+        Maximal marginal relevance optimizes for similarity to query AND diversity
+        among selected documents.
+
+        Args:
+            embedding: Embedding to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
+            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            lambda_mult: Number between 0 and 1 that determines the degree
+                        of diversity among the results with 0 corresponding
+                        to maximum diversity and 1 to minimum diversity.
+                        Defaults to 0.5.
+            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+
+        Returns:
+            List of Documents selected by maximal marginal relevance.
+        """
+
+        results = self.__query_collection(
+            query_embeddings=embedding,
+            n_results=fetch_k,
+            where=filter,
+            where_document=where_document,
+            include=["metadatas", "documents", "distances", "embeddings"],
+        )
+        mmr_selected = maximal_marginal_relevance(
+            np.array(embedding, dtype=np.float32),
+            results["embeddings"][0],
+            k=k,
+            lambda_mult=lambda_mult,
+        )
+
+        candidates = _results_to_docs(results)
+
+        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]
+        return selected_results
+
+    def max_marginal_relevance_search(
+        self,
+        query: str,
+        k: int = DEFAULT_K,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        filter: Optional[Dict[str, str]] = None,
+        where_document: Optional[Dict[str, str]] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs selected using the maximal marginal relevance.
+        Maximal marginal relevance optimizes for similarity to query AND diversity
+        among selected documents.
+
+        Args:
+            query: Text to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
+            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            lambda_mult: Number between 0 and 1 that determines the degree
+                        of diversity among the results with 0 corresponding
+                        to maximum diversity and 1 to minimum diversity.
+                        Defaults to 0.5.
+            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+
+        Returns:
+            List of Documents selected by maximal marginal relevance.
+        """
+        if self._embedding_function is None:
+            raise ValueError(
+                "For MMR search, you must specify an embedding function on" "creation."
+            )
+
+        embedding = self._embedding_function.embed_query(query)
+        docs = self.max_marginal_relevance_search_by_vector(
+            embedding,
+            k,
+            fetch_k,
+            lambda_mult=lambda_mult,
+            filter=filter,
+            where_document=where_document,
+        )
+        return docs
+
+    def delete_collection(self) -> None:
+        """Delete the collection."""
+        self._client.delete_collection(self._collection.name)
+
+    def get(
+        self,
+        ids: Optional[OneOrMany[ID]] = None,
+        where: Optional[Where] = None,
+        limit: Optional[int] = None,
+        offset: Optional[int] = None,
+        where_document: Optional[WhereDocument] = None,
+        include: Optional[List[str]] = None,
+    ) -> Dict[str, Any]:
+        """Gets the collection.
+
+        Args:
+            ids: The ids of the embeddings to get. Optional.
+            where: A Where type dict used to filter results by.
+                   E.g. `{"color" : "red", "price": 4.20}`. Optional.
+            limit: The number of documents to return. Optional.
+            offset: The offset to start returning results from.
+                    Useful for paging results with limit. Optional.
+            where_document: A WhereDocument type dict used to filter by the documents.
+                            E.g. `{$contains: "hello"}`. Optional.
+            include: A list of what to include in the results.
+                     Can contain `"embeddings"`, `"metadatas"`, `"documents"`.
+                     Ids are always included.
+                     Defaults to `["metadatas", "documents"]`. Optional.
+        """
+        kwargs = {
+            "ids": ids,
+            "where": where,
+            "limit": limit,
+            "offset": offset,
+            "where_document": where_document,
+        }
+
+        if include is not None:
+            kwargs["include"] = include
+
+        return self._collection.get(**kwargs)
+
+    def persist(self) -> None:
+        """Persist the collection.
+
+        This can be used to explicitly persist the data to disk.
+        It will also be called automatically when the object is destroyed.
+        """
+        if self._persist_directory is None:
+            raise ValueError(
+                "You must specify a persist_directory on"
+                "creation to persist the collection."
+            )
+        import chromadb
+
+        # Maintain backwards compatibility with chromadb < 0.4.0
+        major, minor, _ = chromadb.__version__.split(".")
+        if int(major) == 0 and int(minor) < 4:
+            self._client.persist()
+
+    def update_document(self, document_id: str, document: Document) -> None:
+        """Update a document in the collection.
+
+        Args:
+            document_id (str): ID of the document to update.
+            document (Document): Document to update.
+        """
+        return self.update_documents([document_id], [document])
+
+    def update_documents(self, ids: List[str], documents: List[Document]) -> None:
+        """Update a document in the collection.
+
+        Args:
+            ids (List[str]): List of ids of the document to update.
+            documents (List[Document]): List of documents to update.
+        """
+        text = [document.page_content for document in documents]
+        metadata = [document.metadata for document in documents]
+        if self._embedding_function is None:
+            raise ValueError(
+                "For update, you must specify an embedding function on creation."
+            )
+        embeddings = self._embedding_function.embed_documents(text)
+
+        if hasattr(
+            self._collection._client, "max_batch_size"
+        ):  # for Chroma 0.4.10 and above
+            from chromadb.utils.batch_utils import create_batches
+
+            for batch in create_batches(
+                api=self._collection._client,
+                ids=ids,
+                metadatas=metadata,
+                documents=text,
+                embeddings=embeddings,
+            ):
+                self._collection.update(
+                    ids=batch[0],
+                    embeddings=batch[1],
+                    documents=batch[3],
+                    metadatas=batch[2],
+                )
+        else:
+            self._collection.update(
+                ids=ids,
+                embeddings=embeddings,
+                documents=text,
+                metadatas=metadata,
+            )
+
+    @classmethod
+    def from_texts(
+        cls: Type[Chroma],
+        texts: List[str],
+        embedding: Optional[Embeddings] = None,
+        metadatas: Optional[List[dict]] = None,
+        ids: Optional[List[str]] = None,
+        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
+        persist_directory: Optional[str] = None,
+        client_settings: Optional[chromadb.config.Settings] = None,
+        client: Optional[chromadb.Client] = None,
+        collection_metadata: Optional[Dict] = None,
+        **kwargs: Any,
+    ) -> Chroma:
+        """Create a Chroma vectorstore from a raw documents.
+
+        If a persist_directory is specified, the collection will be persisted there.
+        Otherwise, the data will be ephemeral in-memory.
+
+        Args:
+            texts (List[str]): List of texts to add to the collection.
+            collection_name (str): Name of the collection to create.
+            persist_directory (Optional[str]): Directory to persist the collection.
+            embedding (Optional[Embeddings]): Embedding function. Defaults to None.
+            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.
+            ids (Optional[List[str]]): List of document IDs. Defaults to None.
+            client_settings (Optional[chromadb.config.Settings]): Chroma client settings
+            collection_metadata (Optional[Dict]): Collection configurations.
+                                                  Defaults to None.
+
+        Returns:
+            Chroma: Chroma vectorstore.
+        """
+        chroma_collection = cls(
+            collection_name=collection_name,
+            embedding_function=embedding,
+            persist_directory=persist_directory,
+            client_settings=client_settings,
+            client=client,
+            collection_metadata=collection_metadata,
+            **kwargs,
+        )
+        if ids is None:
+            ids = [str(uuid.uuid1()) for _ in texts]
+        if hasattr(
+            chroma_collection._client, "max_batch_size"
+        ):  # for Chroma 0.4.10 and above
+            from chromadb.utils.batch_utils import create_batches
+
+            for batch in create_batches(
+                api=chroma_collection._client,
+                ids=ids,
+                metadatas=metadatas,
+                documents=texts,
+            ):
+                chroma_collection.add_texts(
+                    texts=batch[3] if batch[3] else [],
+                    metadatas=batch[2] if batch[2] else None,
+                    ids=batch[0],
+                )
+        else:
+            chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)
+        return chroma_collection
+
+    @classmethod
+    def from_documents(
+        cls: Type[Chroma],
+        documents: List[Document],
+        embedding: Optional[Embeddings] = None,
+        ids: Optional[List[str]] = None,
+        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
+        persist_directory: Optional[str] = None,
+        client_settings: Optional[chromadb.config.Settings] = None,
+        client: Optional[chromadb.Client] = None,  # Add this line
+        collection_metadata: Optional[Dict] = None,
+        **kwargs: Any,
+    ) -> Chroma:
+        """Create a Chroma vectorstore from a list of documents.
+
+        If a persist_directory is specified, the collection will be persisted there.
+        Otherwise, the data will be ephemeral in-memory.
+
+        Args:
+            collection_name (str): Name of the collection to create.
+            persist_directory (Optional[str]): Directory to persist the collection.
+            ids (Optional[List[str]]): List of document IDs. Defaults to None.
+            documents (List[Document]): List of documents to add to the vectorstore.
+            embedding (Optional[Embeddings]): Embedding function. Defaults to None.
+            client_settings (Optional[chromadb.config.Settings]): Chroma client settings
+            collection_metadata (Optional[Dict]): Collection configurations.
+                                                  Defaults to None.
+
+        Returns:
+            Chroma: Chroma vectorstore.
+        """
+        texts = [doc.page_content for doc in documents]
+        metadatas = [doc.metadata for doc in documents]
+        return cls.from_texts(
+            texts=texts,
+            embedding=embedding,
+            metadatas=metadatas,
+            ids=ids,
+            collection_name=collection_name,
+            persist_directory=persist_directory,
+            client_settings=client_settings,
+            client=client,
+            collection_metadata=collection_metadata,
+            **kwargs,
+        )
+
+    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:
+        """Delete by vector IDs.
+
+        Args:
+            ids: List of ids to delete.
+        """
+        self._collection.delete(ids=ids)

f9da0606601f98a15a8ff9fb7c46e02c50549d53
Author: Roland Abou Younes
Date: Sat Dec 2 10:37:01 2023 +0200
Message: Pages adding to vector and retrieval working based in similarity

diff --git a/.gitignore b/.gitignore
index ff00bd7..423428a 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,3 +4,4 @@
 /old/
 /confluence_integration/context/
 /confluence_integration/old/
+/confluence_integration/page_content.json
diff --git a/README.md b/README.md
index 8b0ede7..31baeb2 100644
--- a/README.md
+++ b/README.md
@@ -29,4 +29,7 @@ chmod +x ./setup/setup_and_run.sh

 The script will load 3 docker images for you 1 of which is the python environment and 2 are Kafka and Kafka Zookeeper

-or follow the steps in the script manually.
\ No newline at end of file
+or follow the steps in the script manually.
+
+Ref:
+API documentation: https://atlassian-python-api.readthedocs.io/confluence.html
diff --git a/crhoma/__init__.py b/crhoma/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/crhoma/chroma.py b/crhoma/chroma.py
new file mode 100644
index 0000000..56b4ef7
--- /dev/null
+++ b/crhoma/chroma.py
@@ -0,0 +1,74 @@
+import sqlite3
+from langchain.embeddings.openai import OpenAIEmbeddings
+from langchain.vectorstores import Chroma
+from credentials import oai_api_key
+
+
+def get_data_from_db():
+    # Connect to the SQLite database
+    conn = sqlite3.connect('../database/confluence_data.db')
+    cursor = conn.cursor()
+
+    # Retrieve all records from the "page_data" table
+    cursor.execute("SELECT * FROM page_data")
+    records = cursor.fetchall()
+
+    # Process each record into a string
+    all_documents = [
+        f"Page id: {record[1]}, space key: {record[2]}, title: {record[3]}, "
+        f"author: {record[4]}, created date: {record[5]}, last updated: {record[6]}, "
+        f"content: {record[7]}, comments: {record[8]}"
+        for record in records
+    ]
+
+    # Close the SQLite connection
+    conn.close()
+    return all_documents
+
+
+def vectorize_documents(all_documents):
+
+    # Initialize OpenAI embeddings with the API key
+    embedding = OpenAIEmbeddings(openai_api_key=oai_api_key)
+
+    # Create the Chroma vectorstore with the embedding function
+    vectordb = Chroma(embedding_function=embedding, persist_directory='db')
+
+    # Add texts to the vectorstore
+    vectordb.add_texts(texts=all_documents)
+
+    # Persist the database
+    vectordb.persist()
+
+
+def add():
+    all_documents = get_data_from_db()
+    vectorize_documents(all_documents)
+
+
+def retrieve(question):
+    # Initialize OpenAI embeddings with the API key
+    embedding = OpenAIEmbeddings(openai_api_key=oai_api_key)
+
+    # Create the Chroma vectorstore with the embedding function
+    vectordb = Chroma(embedding_function=embedding, persist_directory='db')
+
+    # Embed the query text using the embedding function
+    query_embedding = embedding.embed_query(question)
+
+    # Perform a similarity search in the vectorstore
+    similar_documents = vectordb.similarity_search_by_vector(query_embedding)
+
+    # Process and return the results
+    return [doc.page_content for doc in similar_documents]
+
+
+if __name__ == '__main__':
+    add()
+    question = "do we use any reminder functionality in our solution?"
+    results = retrieve(question)
+    for doc in results:
+        print(doc)
+
+
+
diff --git a/requirements.txt b/requirements.txt
index f76ffb2..47f5b9f 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -4,3 +4,6 @@ requests
 schedule
 sqlalchemy
 beautifulsoup4
+chromadb
+langchain
+tiktoken

00164142ef36ca79a7e2144c72252d14a7c4334a
Author: Roland Abou Younes
Date: Fri Dec 1 22:38:02 2023 +0200
Message: Space retrieval data writing to database

diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
index 9b59292..b68a024 100644
--- a/confluence_integration/retrieve_space.py
+++ b/confluence_integration/retrieve_space.py
@@ -1,7 +1,8 @@
-import requests
 import json
+from bs4 import BeautifulSoup
 from atlassian import Confluence
 from credentials import confluence_credentials
+from database.confluence_database import store_space_data, store_pages_data

 # Initialize Confluence API
 confluence = Confluence(
@@ -10,14 +11,17 @@ confluence = Confluence(
     password=confluence_credentials['api_token']
 )

+
 def get_top_level_ids(space_key):
     top_level_pages = confluence.get_all_pages_from_space(space_key)
     return [page['id'] for page in top_level_pages]

+
 def get_child_ids(item_id, content_type):
     child_items = confluence.get_page_child_by_type(item_id, type=content_type)
     return [child['id'] for child in child_items]

+
 def get_all_pages_recursive(space_key):
     def get_child_pages_recursively(page_id):
         child_pages = []
@@ -36,7 +40,6 @@ def get_all_pages_recursive(space_key):
     return all_pages


-
 def get_all_comments_recursive(page_id):
     def get_child_comments_recursively(comment_id):
         child_comments = []
@@ -66,32 +69,48 @@ def get_space_ids(space_key):
     print("Page and Comments Map:", page_comments_map)


-def get_all_space_content(space_key):
+def choose_space():
+    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
+    for i, space in enumerate(spaces['results']):
+        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
+    choice = int(input("Choose a space (number): ")) - 1
+    space_key = spaces['results'][choice]['key']
+    space_data={'space_key': space_key,
+                'url': confluence_credentials['base_url'],
+                'login': confluence_credentials['username'],
+                'token': confluence_credentials['api_token']
+                }
+    store_space_data(space_data)
+    return spaces['results'][choice]['key']
+
+
+def strip_html_tags(content):
+    soup = BeautifulSoup(content, 'html.parser')
+    return soup.get_text()
+
+
+def get_space_content():
+    space_key = choose_space()
     all_page_ids = get_all_pages_recursive(space_key)
     page_content_map = {}

     for page_id in all_page_ids:
-        # Include history and version details to get author and date/time information
         page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
-        page_title = page['title']
+        page_title = strip_html_tags(page['title'])

-        # Fetch author and date/time information
         page_author = page['history']['createdBy']['displayName']
         created_date = page['history']['createdDate']
         last_updated = page['version']['when']

-        # Fetch page content
-        page_content = page.get('body', {}).get('storage', {}).get('value', '')
+        page_content = strip_html_tags(page.get('body', {}).get('storage', {}).get('value', ''))

-        # Fetch and concatenate comments
         page_comments_content = ""
         page_comments = get_all_comments_recursive(page_id)
         for comment_id in page_comments:
             comment = confluence.get_page_by_id(comment_id, expand='body.storage')
             comment_content = comment.get('body', {}).get('storage', {}).get('value', '')
-            page_comments_content += comment_content
+            page_comments_content += strip_html_tags(comment_content)

-        # Map the required details to the page ID
         page_content_map[page_id] = {
             'title': page_title,
             'author': page_author,
@@ -101,22 +120,14 @@ def get_all_space_content(space_key):
             'comments': page_comments_content
         }

-    # Write to JSON file
     with open('page_content.json', 'w') as f:
         json.dump(page_content_map, f)

-    return page_content_map
-
+    store_pages_data(space_key, page_content_map)
+    print("Page content written to JSON and database.")

-def choose_space():
-    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
-    for i, space in enumerate(spaces['results']):
-        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
-    choice = int(input("Choose a space (number): ")) - 1
-    return spaces['results'][choice]['key']
+    return page_content_map


 if __name__ == "__main__":
-    space_key = choose_space()
-    get_all_space_content(space_key)
-
+    get_space_content()
diff --git a/database/clear_database.sh b/database/clear_database.sh
new file mode 100644
index 0000000..449e112
--- /dev/null
+++ b/database/clear_database.sh
@@ -0,0 +1,12 @@
+#!/bin/bash
+
+# Path to the SQLite database file
+DATABASE_PATH="./database/confluence_data.db"
+
+# SQL command to delete all records from the tables
+SQL_COMMAND="DELETE FROM space_data; DELETE FROM page_data;"
+
+# Execute the SQL command
+sqlite3 $DATABASE_PATH "$SQL_COMMAND"
+
+echo "Database contents cleared."
diff --git a/database/confluence_data.db b/database/confluence_data.db
index db678e0..5f65928 100644
Binary files a/database/confluence_data.db and b/database/confluence_data.db differ
diff --git a/database/confluence_database.py b/database/confluence_database.py
new file mode 100644
index 0000000..f2e6794
--- /dev/null
+++ b/database/confluence_database.py
@@ -0,0 +1,74 @@
+from sqlalchemy import create_engine, Column, Integer, String, DateTime, Text
+from sqlalchemy.ext.declarative import declarative_base
+from sqlalchemy.orm import sessionmaker
+from datetime import datetime
+
+# Define the base class
+Base = declarative_base()
+
+
+# Define the SpaceData model
+class SpaceData(Base):
+    __tablename__ = 'space_data'
+
+    id = Column(Integer, primary_key=True)
+    space_key = Column(String)
+    url = Column(String)
+    login = Column(String)
+    token = Column(String)
+
+
+# Define the PageData model
+class PageData(Base):
+    __tablename__ = 'page_data'
+
+    id = Column(Integer, primary_key=True)
+    page_id = Column(String)
+    space_key = Column(String)
+    title = Column(String)
+    author = Column(String)
+    createdDate = Column(DateTime)
+    lastUpdated = Column(DateTime)
+    content = Column(Text)
+    comments = Column(Text)
+
+
+# Setup database connection
+engine = create_engine('sqlite:///../database/confluence_data.db')
+Base.metadata.bind = engine
+Base.metadata.create_all(engine)
+Session = sessionmaker(bind=engine)
+session = Session()
+
+
+def store_space_data(space_data):
+    new_space = SpaceData(space_key=space_data['space_key'],
+                          url=space_data['url'],
+                          login=space_data['login'],
+                          token=space_data['token'])
+    session.add(new_space)
+    session.commit()
+    print(f"Space data written to database")
+
+
+def parse_datetime(date_string):
+    return datetime.fromisoformat(date_string.replace('Z', '+00:00'))
+
+
+def store_pages_data(space_key, pages_data):
+    for page_id, page_info in pages_data.items():
+        created_date = parse_datetime(page_info['createdDate'])
+        last_updated = parse_datetime(page_info['lastUpdated'])
+
+        new_page = PageData(page_id=page_id,
+                            space_key=space_key,
+                            title=page_info['title'],
+                            author=page_info['author'],
+                            createdDate=created_date,
+                            lastUpdated=last_updated,
+                            content=page_info['content'],
+                            comments=page_info['comments'])
+        session.add(new_page)
+    session.commit()
+    print("Page content written to database.")
+
diff --git a/requirements.txt b/requirements.txt
index 81ff3dd..f76ffb2 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -2,3 +2,5 @@ openai
 kafka-python
 requests
 schedule
+sqlalchemy
+beautifulsoup4

300da4e9129479470df72cba68d1c65ce98d8d97
Author: Roland Abou Younes
Date: Fri Dec 1 18:35:19 2023 +0200
Message: Merge remote-tracking branch 'origin/main'

b1f0c526a373a2c683675d7cc99588d2292ac682
Author: Roland Abou Younes
Date: Fri Dec 1 18:34:39 2023 +0200
Message: Working retrieve space with comments, stores in json. Database creation script and empty database included.

diff --git a/.gitignore b/.gitignore
index f74efbe..ff00bd7 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,6 @@
 /.idea/
 /credentials.py
-/configuration.py
\ No newline at end of file
+/configuration.py
+/old/
+/confluence_integration/context/
+/confluence_integration/old/
diff --git a/Dockerfile b/Dockerfile
deleted file mode 100644
index 9f92435..0000000
--- a/Dockerfile
+++ /dev/null
@@ -1,14 +0,0 @@
-# Use Python 3.8 image
-FROM python:3.8
-
-# Set the working directory in the container
-WORKDIR /app
-
-# Copy the application files into the container at /app
-COPY . /app
-
-# Install any dependencies
-RUN pip install -r requirements.txt
-
-# Command to run on container start
-CMD ["python", "main.py"]
diff --git a/README.md b/README.md
index 38730e8..8b0ede7 100644
--- a/README.md
+++ b/README.md
@@ -4,7 +4,6 @@ The self actualizing documentation framework that heals its knowledge gaps as na
 ## Rough thoughts
 - add a confluence space (url credentials and update interval)
 - Pulls the confluence space and stores it in an sqlite database
-- Uses Kafka for all operations
 - Vectorizes the confluence space pages and stores the embeds in a chroma db collection
 - Listens on specific slack channels for questions relevant to its domain
 - Uses the vectorized embeds to find the most similar pages to a question
diff --git a/confluence_integration/get_new_space.py b/confluence_integration/get_new_space.py
deleted file mode 100644
index f632b41..0000000
--- a/confluence_integration/get_new_space.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import requests
-import json
-from atlassian import Confluence
-from credentials import confluence_credentials
-
-# Initialize Confluence API
-confluence = Confluence(
-    url=confluence_credentials['base_url'],
-    username=confluence_credentials['username'],
-    password=confluence_credentials['api_token']
-)
-
-
-def get_all_ids(confluence, space_key):
-    # Function to fetch child IDs for a page
-    def get_child_ids(page_id):
-        child_ids = []
-        # Fetch comments
-        comments = confluence.get_page_child_by_type(page_id, type='comment')
-        for comment in comments:
-            child_ids.append(comment['id'])
-        return child_ids
-
-    all_ids = []
-    # Fetch top-level pages
-    pages = confluence.get_all_pages_from_space(space_key, content_type='page')
-    for page in pages:
-        page_id = page['id']
-        all_ids.append((page_id, get_child_ids(page_id)))
-
-    return all_ids
-
-
-def get_page_details_with_comments(confluence, all_ids):
-    pages_details = {}
-    for page_id, comment_ids in all_ids:
-        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
-        title = page['title']
-        body = page['body']['storage']['value']
-
-        comments_content = []
-        for comment_id in comment_ids:
-            comment = confluence.get_page_by_id(comment_id, expand='body.storage,history.createdBy')
-            comment_text = comment['body']['storage']['value']
-            comments_content.append(comment_text)
-
-        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
-
-    return pages_details
-
-
-def summarize_comments_with_llm(page_content, comments):
-    system_prompt = """
-    Focus strictly on the content provided. Avoid assumptions or adding information not present in the comments. Maintain the style and language used in the comments.
-    """
-    user_prompt = f"""
-    Given the content of a page:
-    {page_content}
-    And the associated comments:
-    {comments}
-    Please provide a summary of the comments with the following considerations:
-
-    1. Identify the key themes discussed in the comments.
-    2. Highlight any new insights or additional information that the comments provide, which are not covered in the page content.
-    3. Pay attention to any questions raised in the comments and the answers provided.
-    4. Note any points of agreement or disagreement among the commenters.
-    5. Summarize these elements concisely to enhance the understanding of the page's content.
-    """
-    print(f"System Prompt: {system_prompt}")
-    print(f"User Prompt: {user_prompt}")
-    # API call to local LLM
-    response = requests.post(
-        'http://localhost:1234/v1/chat/completions',
-        headers={"Content-Type": "application/json"},
-        data=json.dumps({
-            "messages": [
-                {"role": "system", "content": system_prompt},
-                {"role": "user", "content": user_prompt}
-            ],
-            "temperature": 0.7,
-            "max_tokens": -1,
-            "stream": False
-        })
-    )
-
-    return response.json()
-
-
-def choose_space(confluence):
-    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
-    for i, space in enumerate(spaces['results']):
-        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
-    choice = int(input("Choose a space (number): ")) - 1
-    return spaces['results'][choice]['key']
-
-
-# Example Usage
-chosen_space_key = choose_space(confluence)
-all_ids_with_comments = get_all_ids(confluence, chosen_space_key)
-pages_details = get_page_details_with_comments(confluence, all_ids_with_comments)
-
-for page_id, details in pages_details.items():
-    if details['comments']:  # Check if there are comments
-        page_content = details['body']
-        comments = " ".join(details['comments'])
-        summarized_comments = summarize_comments_with_llm(page_content, comments)
-        print(f"Page: {details['title']}")
-        print(f"Summarized Comments: {summarized_comments['choices'][0]['message']['content']}")
-    else:
-        print(f"Page: {details['title']}")
-        print("No comments to summarize.")
diff --git a/confluence_integration/comments_summary.py b/confluence_integration/old/comments_summary.py
similarity index 100%
rename from confluence_integration/comments_summary.py
rename to confluence_integration/old/comments_summary.py
diff --git a/test_confluence.py b/confluence_integration/old/test_confluence.py
similarity index 100%
rename from test_confluence.py
rename to confluence_integration/old/test_confluence.py
diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
new file mode 100644
index 0000000..9b59292
--- /dev/null
+++ b/confluence_integration/retrieve_space.py
@@ -0,0 +1,122 @@
+import requests
+import json
+from atlassian import Confluence
+from credentials import confluence_credentials
+
+# Initialize Confluence API
+confluence = Confluence(
+    url=confluence_credentials['base_url'],
+    username=confluence_credentials['username'],
+    password=confluence_credentials['api_token']
+)
+
+def get_top_level_ids(space_key):
+    top_level_pages = confluence.get_all_pages_from_space(space_key)
+    return [page['id'] for page in top_level_pages]
+
+def get_child_ids(item_id, content_type):
+    child_items = confluence.get_page_child_by_type(item_id, type=content_type)
+    return [child['id'] for child in child_items]
+
+def get_all_pages_recursive(space_key):
+    def get_child_pages_recursively(page_id):
+        child_pages = []
+        child_page_ids = get_child_ids(page_id, content_type='page')
+        for child_id in child_page_ids:
+            child_pages.append(child_id)
+            child_pages.extend(get_child_pages_recursively(child_id))
+        return child_pages
+
+    all_pages = []
+    top_level_ids = get_top_level_ids(space_key)
+    for top_level_id in top_level_ids:
+        all_pages.append(top_level_id)
+        all_pages.extend(get_child_pages_recursively(top_level_id))
+
+    return all_pages
+
+
+
+def get_all_comments_recursive(page_id):
+    def get_child_comments_recursively(comment_id):
+        child_comments = []
+        child_comment_ids = get_child_ids(comment_id, content_type='comment')
+        for child_id in child_comment_ids:
+            child_comments.append(child_id)
+            child_comments.extend(get_child_comments_recursively(child_id))
+        return child_comments
+
+    all_comments = []
+    top_level_comment_ids = get_child_ids(page_id, content_type='comment')
+    for comment_id in top_level_comment_ids:
+        all_comments.append(comment_id)
+        all_comments.extend(get_child_comments_recursively(comment_id))
+
+    return all_comments
+
+
+def get_space_ids(space_key):
+    all_page_ids = get_all_pages_recursive(space_key)
+    page_comments_map = {}
+
+    for page_id in all_page_ids:
+        page_comments = get_all_comments_recursive(page_id)
+        page_comments_map[page_id] = page_comments
+
+    print("Page and Comments Map:", page_comments_map)
+
+
+def get_all_space_content(space_key):
+    all_page_ids = get_all_pages_recursive(space_key)
+    page_content_map = {}
+
+    for page_id in all_page_ids:
+        # Include history and version details to get author and date/time information
+        page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
+        page_title = page['title']
+
+        # Fetch author and date/time information
+        page_author = page['history']['createdBy']['displayName']
+        created_date = page['history']['createdDate']
+        last_updated = page['version']['when']
+
+        # Fetch page content
+        page_content = page.get('body', {}).get('storage', {}).get('value', '')
+
+        # Fetch and concatenate comments
+        page_comments_content = ""
+        page_comments = get_all_comments_recursive(page_id)
+        for comment_id in page_comments:
+            comment = confluence.get_page_by_id(comment_id, expand='body.storage')
+            comment_content = comment.get('body', {}).get('storage', {}).get('value', '')
+            page_comments_content += comment_content
+
+        # Map the required details to the page ID
+        page_content_map[page_id] = {
+            'title': page_title,
+            'author': page_author,
+            'createdDate': created_date,
+            'lastUpdated': last_updated,
+            'content': page_content,
+            'comments': page_comments_content
+        }
+
+    # Write to JSON file
+    with open('page_content.json', 'w') as f:
+        json.dump(page_content_map, f)
+
+    return page_content_map
+
+
+def choose_space():
+    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
+    for i, space in enumerate(spaces['results']):
+        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
+    choice = int(input("Choose a space (number): ")) - 1
+    return spaces['results'][choice]['key']
+
+
+if __name__ == "__main__":
+    space_key = choose_space()
+    get_all_space_content(space_key)
+
diff --git a/confluence_integration/test_confluence.py b/confluence_integration/test_confluence.py
deleted file mode 100644
index 66188f6..0000000
--- a/confluence_integration/test_confluence.py
+++ /dev/null
@@ -1,65 +0,0 @@
-from atlassian import Confluence
-from credentials import confluence_credentials
-
-# Create a Confluence object
-confluence = Confluence(
-    url=confluence_credentials['base_url'],
-    username=confluence_credentials['username'],
-    password=confluence_credentials['api_token']
-    )
-
-
-def get_list_of_spaces(confluence):
-    spaces_response = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
-    for space in spaces_response['results']:
-        print(f"Space Name: {space['name']}, Space Key: {space['key']}")
-
-
-# Get a list of all ids of content items of all types in space and the ids of their children
-def get_all_ids(confluence, space_key):
-    # Recursively fetch child ids of a page
-    def get_child_ids(page_id):
-        child_ids = []
-        # Fetch child pages
-        children = confluence.get_page_child_by_type(page_id, type='page')
-        for child in children:
-            child_ids.append(child['id'])
-            # Recursively fetch children of the child page
-            child_ids.extend(get_child_ids(child['id']))
-
-        # Fetch comments
-        comments = confluence.get_page_child_by_type(page_id, type='comment')
-        for comment in comments:
-            child_ids.append(comment['id'])
-        return child_ids
-
-    all_ids = []
-    pages = confluence.get_all_pages_from_space(space_key)
-    for page in pages:
-        page_id = page['id']
-        all_ids.append(page_id)
-        all_ids.extend(get_child_ids(page_id))
-
-    return all_ids
-
-
-def get_page_details_with_comments(confluence, all_ids_with_comments):
-    pages_details = {}
-
-    for page_id, comment_ids in all_ids_with_comments.items():
-        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
-        title = page['title']
-        body = page['body']['storage']['value']
-
-        comments_content = []
-        for comment_id in comment_ids:
-            comment = confluence.get_content_by_id(comment_id, expand='body.storage,history.createdBy')
-            comment_text = comment['body']['storage']['value']
-            commenter = comment['history']['createdBy']['displayName']
-            created_date = comment['history']['createdDate']
-            comments_content.append(f"Comment by {commenter} on {created_date}: {comment_text}")
-
-        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
-
-    return pages_details
-
diff --git a/database/__init__.py b/database/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/database/confluence_data.db b/database/confluence_data.db
new file mode 100644
index 0000000..db678e0
Binary files /dev/null and b/database/confluence_data.db differ
diff --git a/docker-compose.yml b/docker-compose.yml
deleted file mode 100644
index 1139b26..0000000
--- a/docker-compose.yml
+++ /dev/null
@@ -1,9 +0,0 @@
-
-
-#  python-docker-compose up -dapp:
-#    build: .
-#    volumes:
-#      - .:/app
-#    depends_on:
-#      - kafka
-#      - zookeeper
diff --git a/consumer.py b/old/consumer.py
similarity index 100%
rename from consumer.py
rename to old/consumer.py
diff --git a/test_pulsar.py b/old/test_pulsar.py
similarity index 100%
rename from test_pulsar.py
rename to old/test_pulsar.py
diff --git a/setup/create_db.sh b/setup/create_db.sh
new file mode 100644
index 0000000..4f1b124
--- /dev/null
+++ b/setup/create_db.sh
@@ -0,0 +1,28 @@
+#!/bin/bash
+
+# File name for the SQLite database
+DB_FILE="confluence_data.db"
+
+# SQL commands to create tables
+SQL="
+CREATE TABLE IF NOT EXISTS space_data (
+    space_name TEXT,
+    url TEXT,
+    login TEXT,
+    token TEXT
+);
+
+CREATE TABLE IF NOT EXISTS pages_data (
+    page_id INTEGER PRIMARY KEY,
+    space_key TEXT,
+    title TEXT,
+    content TEXT,
+    author TEXT,
+    created_date TEXT,
+    last_updated TEXT,
+    comments TEXT
+);
+"
+
+# Execute SQL commands using SQLite
+sqlite3 $DB_FILE "$SQL"
diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
index 78be758..14cb547 100755
--- a/setup/setup_and_run.sh
+++ b/setup/setup_and_run.sh
@@ -59,7 +59,8 @@ else
     echo "No requirements.txt found in $setup_path. Skipping Python dependencies installation."
 fi

+# Add run create_db.sh to this script
 # Start the Docker containers
-echo "Starting Docker containers from $project_root_path."
-docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone
+# echo "Starting Docker containers from $project_root_path."
+# docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone


2b11d5073b21aca6ad75b44342159b70ce4c600e
Author: Roland Abou Younes
Date: Fri Dec 1 18:34:39 2023 +0200
Message: Working retrieve space with comments, stores in json.

diff --git a/.gitignore b/.gitignore
index f74efbe..ff00bd7 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,6 @@
 /.idea/
 /credentials.py
-/configuration.py
\ No newline at end of file
+/configuration.py
+/old/
+/confluence_integration/context/
+/confluence_integration/old/
diff --git a/Dockerfile b/Dockerfile
deleted file mode 100644
index 9f92435..0000000
--- a/Dockerfile
+++ /dev/null
@@ -1,14 +0,0 @@
-# Use Python 3.8 image
-FROM python:3.8
-
-# Set the working directory in the container
-WORKDIR /app
-
-# Copy the application files into the container at /app
-COPY . /app
-
-# Install any dependencies
-RUN pip install -r requirements.txt
-
-# Command to run on container start
-CMD ["python", "main.py"]
diff --git a/README.md b/README.md
index 38730e8..8b0ede7 100644
--- a/README.md
+++ b/README.md
@@ -4,7 +4,6 @@ The self actualizing documentation framework that heals its knowledge gaps as na
 ## Rough thoughts
 - add a confluence space (url credentials and update interval)
 - Pulls the confluence space and stores it in an sqlite database
-- Uses Kafka for all operations
 - Vectorizes the confluence space pages and stores the embeds in a chroma db collection
 - Listens on specific slack channels for questions relevant to its domain
 - Uses the vectorized embeds to find the most similar pages to a question
diff --git a/confluence_integration/get_new_space.py b/confluence_integration/get_new_space.py
deleted file mode 100644
index f632b41..0000000
--- a/confluence_integration/get_new_space.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import requests
-import json
-from atlassian import Confluence
-from credentials import confluence_credentials
-
-# Initialize Confluence API
-confluence = Confluence(
-    url=confluence_credentials['base_url'],
-    username=confluence_credentials['username'],
-    password=confluence_credentials['api_token']
-)
-
-
-def get_all_ids(confluence, space_key):
-    # Function to fetch child IDs for a page
-    def get_child_ids(page_id):
-        child_ids = []
-        # Fetch comments
-        comments = confluence.get_page_child_by_type(page_id, type='comment')
-        for comment in comments:
-            child_ids.append(comment['id'])
-        return child_ids
-
-    all_ids = []
-    # Fetch top-level pages
-    pages = confluence.get_all_pages_from_space(space_key, content_type='page')
-    for page in pages:
-        page_id = page['id']
-        all_ids.append((page_id, get_child_ids(page_id)))
-
-    return all_ids
-
-
-def get_page_details_with_comments(confluence, all_ids):
-    pages_details = {}
-    for page_id, comment_ids in all_ids:
-        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
-        title = page['title']
-        body = page['body']['storage']['value']
-
-        comments_content = []
-        for comment_id in comment_ids:
-            comment = confluence.get_page_by_id(comment_id, expand='body.storage,history.createdBy')
-            comment_text = comment['body']['storage']['value']
-            comments_content.append(comment_text)
-
-        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
-
-    return pages_details
-
-
-def summarize_comments_with_llm(page_content, comments):
-    system_prompt = """
-    Focus strictly on the content provided. Avoid assumptions or adding information not present in the comments. Maintain the style and language used in the comments.
-    """
-    user_prompt = f"""
-    Given the content of a page:
-    {page_content}
-    And the associated comments:
-    {comments}
-    Please provide a summary of the comments with the following considerations:
-
-    1. Identify the key themes discussed in the comments.
-    2. Highlight any new insights or additional information that the comments provide, which are not covered in the page content.
-    3. Pay attention to any questions raised in the comments and the answers provided.
-    4. Note any points of agreement or disagreement among the commenters.
-    5. Summarize these elements concisely to enhance the understanding of the page's content.
-    """
-    print(f"System Prompt: {system_prompt}")
-    print(f"User Prompt: {user_prompt}")
-    # API call to local LLM
-    response = requests.post(
-        'http://localhost:1234/v1/chat/completions',
-        headers={"Content-Type": "application/json"},
-        data=json.dumps({
-            "messages": [
-                {"role": "system", "content": system_prompt},
-                {"role": "user", "content": user_prompt}
-            ],
-            "temperature": 0.7,
-            "max_tokens": -1,
-            "stream": False
-        })
-    )
-
-    return response.json()
-
-
-def choose_space(confluence):
-    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
-    for i, space in enumerate(spaces['results']):
-        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
-    choice = int(input("Choose a space (number): ")) - 1
-    return spaces['results'][choice]['key']
-
-
-# Example Usage
-chosen_space_key = choose_space(confluence)
-all_ids_with_comments = get_all_ids(confluence, chosen_space_key)
-pages_details = get_page_details_with_comments(confluence, all_ids_with_comments)
-
-for page_id, details in pages_details.items():
-    if details['comments']:  # Check if there are comments
-        page_content = details['body']
-        comments = " ".join(details['comments'])
-        summarized_comments = summarize_comments_with_llm(page_content, comments)
-        print(f"Page: {details['title']}")
-        print(f"Summarized Comments: {summarized_comments['choices'][0]['message']['content']}")
-    else:
-        print(f"Page: {details['title']}")
-        print("No comments to summarize.")
diff --git a/confluence_integration/comments_summary.py b/confluence_integration/old/comments_summary.py
similarity index 100%
rename from confluence_integration/comments_summary.py
rename to confluence_integration/old/comments_summary.py
diff --git a/test_confluence.py b/confluence_integration/old/test_confluence.py
similarity index 100%
rename from test_confluence.py
rename to confluence_integration/old/test_confluence.py
diff --git a/confluence_integration/retrieve_space.py b/confluence_integration/retrieve_space.py
new file mode 100644
index 0000000..9b59292
--- /dev/null
+++ b/confluence_integration/retrieve_space.py
@@ -0,0 +1,122 @@
+import requests
+import json
+from atlassian import Confluence
+from credentials import confluence_credentials
+
+# Initialize Confluence API
+confluence = Confluence(
+    url=confluence_credentials['base_url'],
+    username=confluence_credentials['username'],
+    password=confluence_credentials['api_token']
+)
+
+def get_top_level_ids(space_key):
+    top_level_pages = confluence.get_all_pages_from_space(space_key)
+    return [page['id'] for page in top_level_pages]
+
+def get_child_ids(item_id, content_type):
+    child_items = confluence.get_page_child_by_type(item_id, type=content_type)
+    return [child['id'] for child in child_items]
+
+def get_all_pages_recursive(space_key):
+    def get_child_pages_recursively(page_id):
+        child_pages = []
+        child_page_ids = get_child_ids(page_id, content_type='page')
+        for child_id in child_page_ids:
+            child_pages.append(child_id)
+            child_pages.extend(get_child_pages_recursively(child_id))
+        return child_pages
+
+    all_pages = []
+    top_level_ids = get_top_level_ids(space_key)
+    for top_level_id in top_level_ids:
+        all_pages.append(top_level_id)
+        all_pages.extend(get_child_pages_recursively(top_level_id))
+
+    return all_pages
+
+
+
+def get_all_comments_recursive(page_id):
+    def get_child_comments_recursively(comment_id):
+        child_comments = []
+        child_comment_ids = get_child_ids(comment_id, content_type='comment')
+        for child_id in child_comment_ids:
+            child_comments.append(child_id)
+            child_comments.extend(get_child_comments_recursively(child_id))
+        return child_comments
+
+    all_comments = []
+    top_level_comment_ids = get_child_ids(page_id, content_type='comment')
+    for comment_id in top_level_comment_ids:
+        all_comments.append(comment_id)
+        all_comments.extend(get_child_comments_recursively(comment_id))
+
+    return all_comments
+
+
+def get_space_ids(space_key):
+    all_page_ids = get_all_pages_recursive(space_key)
+    page_comments_map = {}
+
+    for page_id in all_page_ids:
+        page_comments = get_all_comments_recursive(page_id)
+        page_comments_map[page_id] = page_comments
+
+    print("Page and Comments Map:", page_comments_map)
+
+
+def get_all_space_content(space_key):
+    all_page_ids = get_all_pages_recursive(space_key)
+    page_content_map = {}
+
+    for page_id in all_page_ids:
+        # Include history and version details to get author and date/time information
+        page = confluence.get_page_by_id(page_id, expand='body.storage,history,version')
+        page_title = page['title']
+
+        # Fetch author and date/time information
+        page_author = page['history']['createdBy']['displayName']
+        created_date = page['history']['createdDate']
+        last_updated = page['version']['when']
+
+        # Fetch page content
+        page_content = page.get('body', {}).get('storage', {}).get('value', '')
+
+        # Fetch and concatenate comments
+        page_comments_content = ""
+        page_comments = get_all_comments_recursive(page_id)
+        for comment_id in page_comments:
+            comment = confluence.get_page_by_id(comment_id, expand='body.storage')
+            comment_content = comment.get('body', {}).get('storage', {}).get('value', '')
+            page_comments_content += comment_content
+
+        # Map the required details to the page ID
+        page_content_map[page_id] = {
+            'title': page_title,
+            'author': page_author,
+            'createdDate': created_date,
+            'lastUpdated': last_updated,
+            'content': page_content,
+            'comments': page_comments_content
+        }
+
+    # Write to JSON file
+    with open('page_content.json', 'w') as f:
+        json.dump(page_content_map, f)
+
+    return page_content_map
+
+
+def choose_space():
+    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
+    for i, space in enumerate(spaces['results']):
+        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
+    choice = int(input("Choose a space (number): ")) - 1
+    return spaces['results'][choice]['key']
+
+
+if __name__ == "__main__":
+    space_key = choose_space()
+    get_all_space_content(space_key)
+
diff --git a/confluence_integration/test_confluence.py b/confluence_integration/test_confluence.py
deleted file mode 100644
index 66188f6..0000000
--- a/confluence_integration/test_confluence.py
+++ /dev/null
@@ -1,65 +0,0 @@
-from atlassian import Confluence
-from credentials import confluence_credentials
-
-# Create a Confluence object
-confluence = Confluence(
-    url=confluence_credentials['base_url'],
-    username=confluence_credentials['username'],
-    password=confluence_credentials['api_token']
-    )
-
-
-def get_list_of_spaces(confluence):
-    spaces_response = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
-    for space in spaces_response['results']:
-        print(f"Space Name: {space['name']}, Space Key: {space['key']}")
-
-
-# Get a list of all ids of content items of all types in space and the ids of their children
-def get_all_ids(confluence, space_key):
-    # Recursively fetch child ids of a page
-    def get_child_ids(page_id):
-        child_ids = []
-        # Fetch child pages
-        children = confluence.get_page_child_by_type(page_id, type='page')
-        for child in children:
-            child_ids.append(child['id'])
-            # Recursively fetch children of the child page
-            child_ids.extend(get_child_ids(child['id']))
-
-        # Fetch comments
-        comments = confluence.get_page_child_by_type(page_id, type='comment')
-        for comment in comments:
-            child_ids.append(comment['id'])
-        return child_ids
-
-    all_ids = []
-    pages = confluence.get_all_pages_from_space(space_key)
-    for page in pages:
-        page_id = page['id']
-        all_ids.append(page_id)
-        all_ids.extend(get_child_ids(page_id))
-
-    return all_ids
-
-
-def get_page_details_with_comments(confluence, all_ids_with_comments):
-    pages_details = {}
-
-    for page_id, comment_ids in all_ids_with_comments.items():
-        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
-        title = page['title']
-        body = page['body']['storage']['value']
-
-        comments_content = []
-        for comment_id in comment_ids:
-            comment = confluence.get_content_by_id(comment_id, expand='body.storage,history.createdBy')
-            comment_text = comment['body']['storage']['value']
-            commenter = comment['history']['createdBy']['displayName']
-            created_date = comment['history']['createdDate']
-            comments_content.append(f"Comment by {commenter} on {created_date}: {comment_text}")
-
-        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
-
-    return pages_details
-
diff --git a/database/__init__.py b/database/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/database/confluence_data.db b/database/confluence_data.db
new file mode 100644
index 0000000..db678e0
Binary files /dev/null and b/database/confluence_data.db differ
diff --git a/docker-compose.yml b/docker-compose.yml
deleted file mode 100644
index 1139b26..0000000
--- a/docker-compose.yml
+++ /dev/null
@@ -1,9 +0,0 @@
-
-
-#  python-docker-compose up -dapp:
-#    build: .
-#    volumes:
-#      - .:/app
-#    depends_on:
-#      - kafka
-#      - zookeeper
diff --git a/consumer.py b/old/consumer.py
similarity index 100%
rename from consumer.py
rename to old/consumer.py
diff --git a/test_pulsar.py b/old/test_pulsar.py
similarity index 100%
rename from test_pulsar.py
rename to old/test_pulsar.py
diff --git a/setup/create_db.sh b/setup/create_db.sh
new file mode 100644
index 0000000..4f1b124
--- /dev/null
+++ b/setup/create_db.sh
@@ -0,0 +1,28 @@
+#!/bin/bash
+
+# File name for the SQLite database
+DB_FILE="confluence_data.db"
+
+# SQL commands to create tables
+SQL="
+CREATE TABLE IF NOT EXISTS space_data (
+    space_name TEXT,
+    url TEXT,
+    login TEXT,
+    token TEXT
+);
+
+CREATE TABLE IF NOT EXISTS pages_data (
+    page_id INTEGER PRIMARY KEY,
+    space_key TEXT,
+    title TEXT,
+    content TEXT,
+    author TEXT,
+    created_date TEXT,
+    last_updated TEXT,
+    comments TEXT
+);
+"
+
+# Execute SQL commands using SQLite
+sqlite3 $DB_FILE "$SQL"
diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
index 78be758..14cb547 100755
--- a/setup/setup_and_run.sh
+++ b/setup/setup_and_run.sh
@@ -59,7 +59,8 @@ else
     echo "No requirements.txt found in $setup_path. Skipping Python dependencies installation."
 fi

+# Add run create_db.sh to this script
 # Start the Docker containers
-echo "Starting Docker containers from $project_root_path."
-docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone
+# echo "Starting Docker containers from $project_root_path."
+# docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone


e6f5d023d52f0e85d5935212cfdf2ea49f2663da
Author: Roland Abou Younes
Date: Wed Nov 29 18:18:18 2023 +0200
Message: Working get new space.py missing getting nested comments

diff --git a/confluence_integration/__init__.py b/confluence_integration/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/confluence_integration/comments_summary.py b/confluence_integration/comments_summary.py
new file mode 100644
index 0000000..5bfdfd9
--- /dev/null
+++ b/confluence_integration/comments_summary.py
@@ -0,0 +1,20 @@
+page_content = ""
+comments = ""
+
+summarize_comments_prompt = f"""
+Given the content of a page:
+
+{page_content}
+
+And the associated comments:
+
+{comments}
+
+Please provide a summary of the comments with the following considerations:
+
+1. Identify the key themes discussed in the comments.
+2. Highlight any new insights or additional information that the comments provide, which are not covered in the page content.
+3. Pay attention to any questions raised in the comments and the answers provided.
+4. Note any points of agreement or disagreement among the commenters.
+5. Summarize these elements concisely to enhance the understanding of the page's content.
+"""
\ No newline at end of file
diff --git a/confluence_integration/get_new_space.py b/confluence_integration/get_new_space.py
new file mode 100644
index 0000000..f632b41
--- /dev/null
+++ b/confluence_integration/get_new_space.py
@@ -0,0 +1,111 @@
+import requests
+import json
+from atlassian import Confluence
+from credentials import confluence_credentials
+
+# Initialize Confluence API
+confluence = Confluence(
+    url=confluence_credentials['base_url'],
+    username=confluence_credentials['username'],
+    password=confluence_credentials['api_token']
+)
+
+
+def get_all_ids(confluence, space_key):
+    # Function to fetch child IDs for a page
+    def get_child_ids(page_id):
+        child_ids = []
+        # Fetch comments
+        comments = confluence.get_page_child_by_type(page_id, type='comment')
+        for comment in comments:
+            child_ids.append(comment['id'])
+        return child_ids
+
+    all_ids = []
+    # Fetch top-level pages
+    pages = confluence.get_all_pages_from_space(space_key, content_type='page')
+    for page in pages:
+        page_id = page['id']
+        all_ids.append((page_id, get_child_ids(page_id)))
+
+    return all_ids
+
+
+def get_page_details_with_comments(confluence, all_ids):
+    pages_details = {}
+    for page_id, comment_ids in all_ids:
+        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
+        title = page['title']
+        body = page['body']['storage']['value']
+
+        comments_content = []
+        for comment_id in comment_ids:
+            comment = confluence.get_page_by_id(comment_id, expand='body.storage,history.createdBy')
+            comment_text = comment['body']['storage']['value']
+            comments_content.append(comment_text)
+
+        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
+
+    return pages_details
+
+
+def summarize_comments_with_llm(page_content, comments):
+    system_prompt = """
+    Focus strictly on the content provided. Avoid assumptions or adding information not present in the comments. Maintain the style and language used in the comments.
+    """
+    user_prompt = f"""
+    Given the content of a page:
+    {page_content}
+    And the associated comments:
+    {comments}
+    Please provide a summary of the comments with the following considerations:
+
+    1. Identify the key themes discussed in the comments.
+    2. Highlight any new insights or additional information that the comments provide, which are not covered in the page content.
+    3. Pay attention to any questions raised in the comments and the answers provided.
+    4. Note any points of agreement or disagreement among the commenters.
+    5. Summarize these elements concisely to enhance the understanding of the page's content.
+    """
+    print(f"System Prompt: {system_prompt}")
+    print(f"User Prompt: {user_prompt}")
+    # API call to local LLM
+    response = requests.post(
+        'http://localhost:1234/v1/chat/completions',
+        headers={"Content-Type": "application/json"},
+        data=json.dumps({
+            "messages": [
+                {"role": "system", "content": system_prompt},
+                {"role": "user", "content": user_prompt}
+            ],
+            "temperature": 0.7,
+            "max_tokens": -1,
+            "stream": False
+        })
+    )
+
+    return response.json()
+
+
+def choose_space(confluence):
+    spaces = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
+    for i, space in enumerate(spaces['results']):
+        print(f"{i + 1}. {space['name']} (Key: {space['key']})")
+    choice = int(input("Choose a space (number): ")) - 1
+    return spaces['results'][choice]['key']
+
+
+# Example Usage
+chosen_space_key = choose_space(confluence)
+all_ids_with_comments = get_all_ids(confluence, chosen_space_key)
+pages_details = get_page_details_with_comments(confluence, all_ids_with_comments)
+
+for page_id, details in pages_details.items():
+    if details['comments']:  # Check if there are comments
+        page_content = details['body']
+        comments = " ".join(details['comments'])
+        summarized_comments = summarize_comments_with_llm(page_content, comments)
+        print(f"Page: {details['title']}")
+        print(f"Summarized Comments: {summarized_comments['choices'][0]['message']['content']}")
+    else:
+        print(f"Page: {details['title']}")
+        print("No comments to summarize.")
diff --git a/confluence_integration/test_confluence.py b/confluence_integration/test_confluence.py
new file mode 100644
index 0000000..66188f6
--- /dev/null
+++ b/confluence_integration/test_confluence.py
@@ -0,0 +1,65 @@
+from atlassian import Confluence
+from credentials import confluence_credentials
+
+# Create a Confluence object
+confluence = Confluence(
+    url=confluence_credentials['base_url'],
+    username=confluence_credentials['username'],
+    password=confluence_credentials['api_token']
+    )
+
+
+def get_list_of_spaces(confluence):
+    spaces_response = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
+    for space in spaces_response['results']:
+        print(f"Space Name: {space['name']}, Space Key: {space['key']}")
+
+
+# Get a list of all ids of content items of all types in space and the ids of their children
+def get_all_ids(confluence, space_key):
+    # Recursively fetch child ids of a page
+    def get_child_ids(page_id):
+        child_ids = []
+        # Fetch child pages
+        children = confluence.get_page_child_by_type(page_id, type='page')
+        for child in children:
+            child_ids.append(child['id'])
+            # Recursively fetch children of the child page
+            child_ids.extend(get_child_ids(child['id']))
+
+        # Fetch comments
+        comments = confluence.get_page_child_by_type(page_id, type='comment')
+        for comment in comments:
+            child_ids.append(comment['id'])
+        return child_ids
+
+    all_ids = []
+    pages = confluence.get_all_pages_from_space(space_key)
+    for page in pages:
+        page_id = page['id']
+        all_ids.append(page_id)
+        all_ids.extend(get_child_ids(page_id))
+
+    return all_ids
+
+
+def get_page_details_with_comments(confluence, all_ids_with_comments):
+    pages_details = {}
+
+    for page_id, comment_ids in all_ids_with_comments.items():
+        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
+        title = page['title']
+        body = page['body']['storage']['value']
+
+        comments_content = []
+        for comment_id in comment_ids:
+            comment = confluence.get_content_by_id(comment_id, expand='body.storage,history.createdBy')
+            comment_text = comment['body']['storage']['value']
+            commenter = comment['history']['createdBy']['displayName']
+            created_date = comment['history']['createdDate']
+            comments_content.append(f"Comment by {commenter} on {created_date}: {comment_text}")
+
+        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
+
+    return pages_details
+
diff --git a/main.py b/main.py
new file mode 100644
index 0000000..5f381d1
--- /dev/null
+++ b/main.py
@@ -0,0 +1,89 @@
+class SpaceManager:
+    """Manages Confluence space configurations.
+
+    Handles adding new Confluence spaces with associated credentials and update intervals.
+    Publishes space configuration to Pulsar.
+    """
+
+    def add_new_space(self, space_name, credentials, update_interval):
+        pass
+
+
+class PulsarProducer:
+    """Facilitates sending messages to Pulsar.
+
+    Used for publishing various events, such as space additions, updates, and deletions, to Pulsar topics.
+    """
+
+    def send_message(self, topic, message):
+        pass
+
+
+class SpaceConsumer:
+    """Processes new space configurations from Pulsar.
+
+    Consumes messages from Pulsar related to new space additions and stores configurations in SQLite.
+    """
+
+    def consume_message(self):
+        pass
+
+
+class PagePuller:
+    """Retrieves pages and their comments from Confluence spaces.
+
+    Activated for newly added spaces or during scheduled updates, it fetches pages along with their comments and posts the data to Pulsar.
+    """
+
+    def pull_pages_and_comments(self, space_name):
+        pass
+
+
+class PageConsumer:
+    """Handles page data from Pulsar.
+
+    Consumes page data from Pulsar and extracts and stores in the database including metadata (space id, page id, user who created the page, last update date and time) and content (title text, body text, and comment text).
+    """
+
+    def consume_page_data(self):
+        pass
+
+
+class UpdateScheduler:
+    """Schedules update checks for Confluence spaces.
+
+    Triggers periodic checks for updates and deletions in Confluence spaces based on configured intervals.
+    """
+
+    def schedule_updates(self):
+        pass
+
+
+class UpdateProducer:
+    """Sends update and delete events to Pulsar.
+
+    Collects updated and deleted page information from Confluence and publishes it to Pulsar.
+    """
+
+    def send_update(self, update_info):
+        pass
+
+
+class UpdateConsumer:
+    """Processes updates and deletions from Pulsar.
+
+    Listens for update and delete events on Pulsar and updates or removes corresponding pages in SQLite.
+    """
+
+    def consume_updates(self):
+        pass
+
+
+class ChromaSync:
+    """Syncs page data from Pulsar to Chroma DB.
+
+    Listens to Pulsar topics for page data, including new, updated, and deleted pages, and syncs this data with Chroma DB.
+    """
+
+    def sync_with_chroma(self):
+        pass
diff --git a/test_confluence.py b/test_confluence.py
index da88165..44ab7b8 100644
--- a/test_confluence.py
+++ b/test_confluence.py
@@ -15,6 +15,56 @@ def get_list_of_spaces(confluence):
         print(f"Space Name: {space['name']}, Space Key: {space['key']}")


+# Get a list of all ids of content items of all types in space and the ids of their children
+def get_all_ids(confluence, space_key):
+    # Recursively fetch child ids of a page
+    def get_child_ids(page_id):
+        child_ids = []
+        # Fetch child pages
+        children = confluence.get_page_child_by_type(page_id, type='page')
+        for child in children:
+            child_ids.append(child['id'])
+            # Recursively fetch children of the child page
+            child_ids.extend(get_child_ids(child['id']))
+
+        # Fetch comments
+        comments = confluence.get_page_child_by_type(page_id, type='comment')
+        for comment in comments:
+            child_ids.append(comment['id'])
+        return child_ids
+
+    all_ids = []
+    pages = confluence.get_all_pages_from_space(space_key)
+    for page in pages:
+        page_id = page['id']
+        all_ids.append(page_id)
+        all_ids.extend(get_child_ids(page_id))
+
+    return all_ids
+
+
+def get_page_details_with_comments(confluence, all_ids_with_comments):
+    pages_details = {}
+
+    for page_id, comment_ids in all_ids_with_comments.items():
+        page = confluence.get_page_by_id(page_id, expand='body.storage,title')
+        title = page['title']
+        body = page['body']['storage']['value']
+
+        comments_content = []
+        for comment_id in comment_ids:
+            comment = confluence.get_content_by_id(comment_id, expand='body.storage,history.createdBy')
+            comment_text = comment['body']['storage']['value']
+            commenter = comment['history']['createdBy']['displayName']
+            created_date = comment['history']['createdDate']
+            comments_content.append(f"Comment by {commenter} on {created_date}: {comment_text}")
+
+        pages_details[page_id] = {'title': title, 'body': body, 'comments': comments_content}
+
+    return pages_details
+
+
+
 # Get the list of pages in a space
 def get_pages_in_space(confluence, space_key):
     pages = confluence.get_all_pages_from_space(space_key, start=0, limit=50, status='current')
@@ -30,12 +80,48 @@ def get_page_content_and_comments(confluence, page_id):
     print(f"title: {title}, \ncontent: {content}")


-get_list_of_spaces(confluence)
+def get_page_comments(confluence, page_id):
+    comments = confluence.get_page_child_by_type(page_id, type='comment')
+    print(comments)
+
+
+def create_page(confluence, space, title, body):
+    new_page = confluence.create_page(space, title, body)
+    print(new_page)
+
+
+def create_comment(confluence, page_id, comment):
+    confluence.add_comment(page_id, comment)
+    print(comment)
+
+
+def test_read_content():
+    get_list_of_spaces(confluence)
+
+    space_key = 'ST'
+
+    get_pages_in_space(confluence, space_key)
+
+    page_id = 33250  # Replace with your page ID
+
+    get_page_content_and_comments(confluence, page_id)
+
+    get_page_comments(confluence, page_id)
+
+    print(get_all_ids(confluence, space_key))
+

-space_key = 'ST'
+def test_create_content():
+    space_key = 'ST'
+    page_id = 33250  # Replace with your page ID

-get_pages_in_space(confluence, space_key)
+    for i in range(1, 10):
+        title = f"New Pages {i}"
+        body = "This is the body of our new page.{i}"
+        create_page(confluence, space_key, title, body)
+        for j in range(1, 3):
+            comment = f'Your comment text here.{i}'
+            print(create_comment(confluence, page_id, comment))

-page_id = 33250  # Replace with your page ID

-get_page_content_and_comments(confluence, page_id)
+test_read_content()

9338905861ca46381b2002d860270d8b2123fa4a
Author: Roland Abou Younes
Date: Wed Nov 29 02:13:54 2023 +0200
Message: Using pulsar and confluence

diff --git a/consumer.py b/consumer.py
new file mode 100644
index 0000000..1a662ef
--- /dev/null
+++ b/consumer.py
@@ -0,0 +1,57 @@
+import logging
+from pulsar import Client
+
+# Set up logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+def send_message(service_url, topic, message_content):
+    logger.info("Creating Pulsar client.")
+    client = Client(service_url)
+
+    logger.info(f"Creating producer for topic: {topic}.")
+    producer = client.create_producer(topic)
+
+    logger.info(f"Sending message: {message_content}.")
+    producer.send(message_content.encode('utf-8'))
+
+    logger.info("Message sent. Closing producer.")
+    producer.close()
+
+    logger.info("Producer closed. Closing client.")
+    client.close()
+
+
+def receive_messages(service_url, topic, subscription_name, num_messages):
+    logger.info("Creating Pulsar client.")
+    client = Client(service_url)
+
+    logger.info(f"Subscribing to topic: {topic} with subscription name: {subscription_name}.")
+    consumer = client.subscribe(topic, subscription_name)
+
+    received_messages = []
+    for _ in range(num_messages):
+        logger.info("Waiting to receive a message.")
+        msg = consumer.receive()
+        message_data = msg.data().decode('utf-8')
+        received_messages.append(message_data)
+        logger.info(f"Received message: {message_data}")
+        consumer.acknowledge(msg)
+
+    logger.info("Messages received. Closing consumer.")
+    consumer.close()
+
+    logger.info("Consumer closed. Closing client.")
+    client.close()
+
+    return received_messages
+
+
+# Test sending and receiving a message
+service_url = 'pulsar://localhost:6650'
+topic = 'test-topic'
+message_content = 'Hello, Pulsar!'
+
+logger.info("Starting test: sending a message.")
+send_message(service_url, topic, message_content)
\ No newline at end of file
diff --git a/docker-compose.yml b/docker-compose.yml
index 354b9e2..1139b26 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -1,24 +1,9 @@
-version: '3'
-services:
-  zookeeper:
-    image: wurstmeister/zookeeper
-    ports:
-      - "2181:2181"

-  kafka:
-    image: wurstmeister/kafka
-    ports:
-      - "9092:9092"
-    environment:
-      KAFKA_ADVERTISED_HOST_NAME: kafka
-      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
-    volumes:
-      - /var/run/docker.sock:/var/run/docker.sock

-  python-app:
-    build: .
-    volumes:
-      - .:/app
-    depends_on:
-      - kafka
-      - zookeeper
+#  python-docker-compose up -dapp:
+#    build: .
+#    volumes:
+#      - .:/app
+#    depends_on:
+#      - kafka
+#      - zookeeper
diff --git a/main.py b/main.py
deleted file mode 100644
index cceb525..0000000
--- a/main.py
+++ /dev/null
@@ -1 +0,0 @@
-print("Let there be light")
diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
index bd3c86f..78be758 100755
--- a/setup/setup_and_run.sh
+++ b/setup/setup_and_run.sh
@@ -61,4 +61,5 @@ fi

 # Start the Docker containers
 echo "Starting Docker containers from $project_root_path."
-(cd "$project_root_path" && docker-compose up --build)
+docker run -it -p 6650:6650 -p 8080:8080 --mount source=pulsardata,target=/pulsar/data --mount source=pulsarconf,target=/pulsar/conf apachepulsar/pulsar:3.1.1 bin/pulsar standalone
+
diff --git a/test_confluence.py b/test_confluence.py
new file mode 100644
index 0000000..da88165
--- /dev/null
+++ b/test_confluence.py
@@ -0,0 +1,41 @@
+from atlassian import Confluence
+from credentials import confluence_credentials
+
+# Create a Confluence object
+confluence = Confluence(
+    url=confluence_credentials['base_url'],
+    username=confluence_credentials['username'],
+    password=confluence_credentials['api_token']
+    )
+
+
+def get_list_of_spaces(confluence):
+    spaces_response = confluence.get_all_spaces(start=0, limit=50, expand='description.plain,body.view,value')
+    for space in spaces_response['results']:
+        print(f"Space Name: {space['name']}, Space Key: {space['key']}")
+
+
+# Get the list of pages in a space
+def get_pages_in_space(confluence, space_key):
+    pages = confluence.get_all_pages_from_space(space_key, start=0, limit=50, status='current')
+    for page in pages:
+        print(f"Page Title: {page['title']}, Page ID: {page['id']}")
+
+
+def get_page_content_and_comments(confluence, page_id):
+    page_data = confluence.get_page_by_id(page_id, expand='body.storage,title')
+    content = page_data['body']['storage']['value']
+    title = page_data['title']
+
+    print(f"title: {title}, \ncontent: {content}")
+
+
+get_list_of_spaces(confluence)
+
+space_key = 'ST'
+
+get_pages_in_space(confluence, space_key)
+
+page_id = 33250  # Replace with your page ID
+
+get_page_content_and_comments(confluence, page_id)
diff --git a/test_pulsar.py b/test_pulsar.py
new file mode 100644
index 0000000..b2dd5e8
--- /dev/null
+++ b/test_pulsar.py
@@ -0,0 +1,61 @@
+import logging
+from pulsar import Client
+
+# Set up logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+
+def send_message(service_url, topic, message_content):
+    logger.info("Creating Pulsar client.")
+    client = Client(service_url)
+
+    logger.info(f"Creating producer for topic: {topic}.")
+    producer = client.create_producer(topic)
+
+    logger.info(f"Sending message: {message_content}.")
+    producer.send(message_content.encode('utf-8'))
+
+    logger.info("Message sent. Closing producer.")
+    producer.close()
+
+    logger.info("Producer closed. Closing client.")
+    client.close()
+
+
+def receive_messages(service_url, topic, subscription_name, num_messages):
+    logger.info("Creating Pulsar client.")
+    client = Client(service_url)
+
+    logger.info(f"Subscribing to topic: {topic} with subscription name: {subscription_name}.")
+    consumer = client.subscribe(topic, subscription_name)
+
+    received_messages = []
+    for _ in range(num_messages):
+        logger.info("Waiting to receive a message.")
+        msg = consumer.receive()
+        message_data = msg.data().decode('utf-8')
+        received_messages.append(message_data)
+        logger.info(f"Received message: {message_data}")
+        consumer.acknowledge(msg)
+
+    logger.info("Messages received. Closing consumer.")
+    consumer.close()
+
+    logger.info("Consumer closed. Closing client.")
+    client.close()
+
+    return received_messages
+
+
+# Test sending and receiving a message
+service_url = 'pulsar://localhost:6650'
+topic = 'test-topic'
+message_content = 'Hello, Pulsar!'
+
+logger.info("Starting test: sending a message.")
+send_message(service_url, topic, message_content)
+
+logger.info("Starting test: receiving messages.")
+messages = receive_messages(service_url, topic, 'test-subscription', 1)
+logger.info(f"Messages received: {messages}")

4a8ceae9c707b0ded79ee966e8f810c680470890
Author: Roland Abou Younes
Date: Mon Nov 27 16:41:02 2023 +0200
Message: Updated setup and run to be idempotent Updated README.md changed install to utility

diff --git a/Dockerfile b/Dockerfile
index 795af35..9f92435 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -8,7 +8,7 @@ WORKDIR /app
 COPY . /app

 # Install any dependencies
-# RUN pip install -r requirements.txt
+RUN pip install -r requirements.txt

 # Command to run on container start
 CMD ["python", "main.py"]
diff --git a/README.md b/README.md
index 5e506c5..38730e8 100644
--- a/README.md
+++ b/README.md
@@ -14,63 +14,20 @@ The self actualizing documentation framework that heals its knowledge gaps as na


 ## Setup
-Create setup_and_run.sh
+1. Setup Docker
+2. Git clone the repo

-chmod +x setup_and_run.sh
-
-````bash
-#!/bin/bash
-
-# Function to check and install Miniconda if necessary
-check_miniconda() {
-    if ! [ -x "$(command -v conda)" ]; then
-        echo "Miniconda is not installed. Installing Miniconda..."
-        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
-        bash miniconda.sh -b -p $HOME/miniconda
-        export PATH="$HOME/miniconda/bin:$PATH"
-        echo "Miniconda installed."
-    else
-        echo "Miniconda is already installed."
-    fi
-}
-
-# Check if already in Nur directory with main.py
-if [ -d "Nur" ] && [ -f "Nur/main.py" ]; then
-    echo "Nur directory with main.py already exists. Skipping cloning."
-    cd Nur
-else
-    # Clone the GitHub repository if Nur directory doesn't exist
-    git clone https://github.com/MDGrey33/Nur.git
-    cd Nur
-fi
-
-# Check if the Miniconda environment already exists
-env_name="myenv"
-if conda info --envs | grep -q "$env_name"; then
-    echo "Miniconda environment '$env_name' already exists. Activating it."
-else
-    echo "Creating Miniconda environment '$env_name'."
-    conda create -n "$env_name" python=3.8 -y
-fi
-
-# Activate the Miniconda environment
-# Modify this depending on your shell compatibility
-source activate "$env_name"
-
-# Install Python dependencies
-if [ -f "requirements.txt" ]; then
-    echo "Installing Python dependencies."
-    pip install -r requirements.txt
-else
-    echo "No requirements.txt found. Skipping Python dependencies installation."
-fi
-
-# Start the Docker containers
-echo "Starting Docker containers."
-docker-compose up --build
 ````
+git clone https://github.com/MDGrey33/Nur.git
+````
+Download setup_and_run.sh from the repo and make it executable and execute it

-## Run
-````bash
-./utility/setup_and_run.sh
 ````
+cd Nur
+chmod +x ./setup/setup_and_run.sh
+./setup/setup_and_run.sh
+````
+
+The script will load 3 docker images for you 1 of which is the python environment and 2 are Kafka and Kafka Zookeeper
+
+or follow the steps in the script manually.
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..81ff3dd
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,4 @@
+openai
+kafka-python
+requests
+schedule
diff --git a/setup/__init__.py b/setup/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/setup/requirements.txt b/setup/requirements.txt
new file mode 100644
index 0000000..e69de29
diff --git a/setup/setup_and_run.sh b/setup/setup_and_run.sh
new file mode 100755
index 0000000..bd3c86f
--- /dev/null
+++ b/setup/setup_and_run.sh
@@ -0,0 +1,64 @@
+#!/bin/bash
+
+# Function to check and install Miniconda if necessary
+check_miniconda() {
+    if ! [ -x "$(command -v conda)" ]; then
+        echo "Miniconda is not installed. Installing Miniconda..."
+        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
+        bash miniconda.sh -b -p $HOME/miniconda
+        export PATH="$HOME/miniconda/bin:$PATH"
+        echo "Miniconda installed."
+    else
+        echo "Miniconda is already installed."
+    fi
+}
+
+# Determine the project root path
+current_dir="$(basename "$PWD")"
+parent_dir="$(basename "$(dirname "$PWD")")"
+
+if [ "$current_dir" = "setup" ] && [ "$parent_dir" = "Nur" ]; then
+    # In /Nur/setup, navigate to /Nur and set project root path
+    project_root_path="$(dirname "$PWD")"
+    echo "In /Nur/setup. Project root is: $project_root_path"
+    cd ..
+elif [ "$current_dir" = "Nur" ]; then
+    # Already in /Nur, set project root path
+    project_root_path="$PWD"
+    echo "Already in /Nur. Project root is: $project_root_path"
+else
+    # Not in /Nur or /Nur/setup, clone the repo and set project root path
+    git clone https://github.com/MDGrey33/Nur.git
+    cd Nur
+    project_root_path="$PWD"
+    echo "Cloned repository. Project root is: $project_root_path"
+fi
+
+# Define path for setup directory
+setup_path="$project_root_path/setup"
+
+# Check if the Miniconda environment already exists
+env_name="myenv"
+if conda info --envs | grep -q "$env_name"; then
+    echo "Miniconda environment '$env_name' already exists. Activating it."
+else
+    echo "Creating Miniconda environment '$env_name'."
+    conda create -n "$env_name" python=3.8 -y
+fi
+
+# Activate the Miniconda environment
+# Modify this depending on your shell compatibility
+source activate "$env_name" || conda activate "$env_name"
+
+# Install Python dependencies from the setup directory
+requirements_file="$setup_path/requirements.txt"
+if [ -f "$requirements_file" ]; then
+    echo "Installing Python dependencies from $requirements_file."
+    pip install -r "$requirements_file"
+else
+    echo "No requirements.txt found in $setup_path. Skipping Python dependencies installation."
+fi
+
+# Start the Docker containers
+echo "Starting Docker containers from $project_root_path."
+(cd "$project_root_path" && docker-compose up --build)
diff --git a/utility/setup_and_run.sh b/utility/setup_and_run.sh
deleted file mode 100755
index 7188dc9..0000000
--- a/utility/setup_and_run.sh
+++ /dev/null
@@ -1,49 +0,0 @@
-#!/bin/bash
-
-# Function to check and install Miniconda if necessary
-check_miniconda() {
-    if ! [ -x "$(command -v conda)" ]; then
-        echo "Miniconda is not installed. Installing Miniconda..."
-        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
-        bash miniconda.sh -b -p $HOME/miniconda
-        export PATH="$HOME/miniconda/bin:$PATH"
-        echo "Miniconda installed."
-    else
-        echo "Miniconda is already installed."
-    fi
-}
-
-# Check if already in Nur directory with main.py
-if [ -d "Nur" ] && [ -f "Nur/main.py" ]; then
-    echo "Nur directory with main.py already exists. Skipping cloning."
-    cd Nur
-else
-    # Clone the GitHub repository if Nur directory doesn't exist
-    git clone https://github.com/MDGrey33/Nur.git
-    cd Nur
-fi
-
-# Check if the Miniconda environment already exists
-env_name="myenv"
-if conda info --envs | grep -q "$env_name"; then
-    echo "Miniconda environment '$env_name' already exists. Activating it."
-else
-    echo "Creating Miniconda environment '$env_name'."
-    conda create -n "$env_name" python=3.8 -y
-fi
-
-# Activate the Miniconda environment
-# Modify this depending on your shell compatibility
-source activate "$env_name"
-
-# Install Python dependencies
-if [ -f "requirements.txt" ]; then
-    echo "Installing Python dependencies."
-    pip install -r requirements.txt
-else
-    echo "No requirements.txt found. Skipping Python dependencies installation."
-fi
-
-# Start the Docker containers
-echo "Starting Docker containers."
-docker-compose up --build

02a6ffaf4f16b4f2135c928920c5a9f7b7712d37
Author: Roland Abou Younes
Date: Mon Nov 27 13:31:50 2023 +0200
Message: Updated setup and run to be idempotent Updated README.md changed install to utility

diff --git a/README.md b/README.md
index d0037d6..5e506c5 100644
--- a/README.md
+++ b/README.md
@@ -1,2 +1,76 @@
 # Nur
 The self actualizing documentation framework that heals its knowledge gaps as naturally as a ray of light
+
+## Rough thoughts
+- add a confluence space (url credentials and update interval)
+- Pulls the confluence space and stores it in an sqlite database
+- Uses Kafka for all operations
+- Vectorizes the confluence space pages and stores the embeds in a chroma db collection
+- Listens on specific slack channels for questions relevant to its domain
+- Uses the vectorized embeds to find the most similar pages to a question
+- Creates an assistant with the relevant pages and allows it to engage to provide the answer if confident enough
+- Gets user feedback to either increase confidence or decrease confidence
+- If confidence is below a certain threashold the assistant will add the question to a trivia quizz and runs it with the specialist team and recommends the update in a confluence comment
+
+
+## Setup
+Create setup_and_run.sh
+
+chmod +x setup_and_run.sh
+
+````bash
+#!/bin/bash
+
+# Function to check and install Miniconda if necessary
+check_miniconda() {
+    if ! [ -x "$(command -v conda)" ]; then
+        echo "Miniconda is not installed. Installing Miniconda..."
+        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
+        bash miniconda.sh -b -p $HOME/miniconda
+        export PATH="$HOME/miniconda/bin:$PATH"
+        echo "Miniconda installed."
+    else
+        echo "Miniconda is already installed."
+    fi
+}
+
+# Check if already in Nur directory with main.py
+if [ -d "Nur" ] && [ -f "Nur/main.py" ]; then
+    echo "Nur directory with main.py already exists. Skipping cloning."
+    cd Nur
+else
+    # Clone the GitHub repository if Nur directory doesn't exist
+    git clone https://github.com/MDGrey33/Nur.git
+    cd Nur
+fi
+
+# Check if the Miniconda environment already exists
+env_name="myenv"
+if conda info --envs | grep -q "$env_name"; then
+    echo "Miniconda environment '$env_name' already exists. Activating it."
+else
+    echo "Creating Miniconda environment '$env_name'."
+    conda create -n "$env_name" python=3.8 -y
+fi
+
+# Activate the Miniconda environment
+# Modify this depending on your shell compatibility
+source activate "$env_name"
+
+# Install Python dependencies
+if [ -f "requirements.txt" ]; then
+    echo "Installing Python dependencies."
+    pip install -r requirements.txt
+else
+    echo "No requirements.txt found. Skipping Python dependencies installation."
+fi
+
+# Start the Docker containers
+echo "Starting Docker containers."
+docker-compose up --build
+````
+
+## Run
+````bash
+./utility/setup_and_run.sh
+````
diff --git a/install/setup_and_run.sh b/install/setup_and_run.sh
deleted file mode 100755
index 37cf46e..0000000
--- a/install/setup_and_run.sh
+++ /dev/null
@@ -1,36 +0,0 @@
-#!/bin/bash
-
-# Function to check if Miniconda is installed
-check_miniconda() {
-    if ! [ -x "$(command -v conda)" ]; then
-        echo "Miniconda is not installed. Installing Miniconda..."
-        # Replace this URL with the latest Miniconda installer for your platform
-        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
-        bash miniconda.sh -b -p $HOME/miniconda
-        export PATH="$HOME/miniconda/bin:$PATH"
-        echo "Miniconda installed."
-    else
-        echo "Miniconda is already installed."
-    fi
-}
-
-# Step 1: Check and install Miniconda if necessary
-check_miniconda
-
-# Step 2: Clone the GitHub repository
-git clone https://github.com/MDGrey33/Nur.git
-cd Nur
-
-# Step 3: Create a Miniconda environment
-conda create -n myenv python=3.8 -y
-
-# Step 4: Activate the Miniconda environment
-# You may need to modify this depending on how your shell handles script execution
-source activate myenv
-
-# Step 5: Install Python dependencies
-pip install -r requirements.txt
-
-# Step 6: Start the Docker containers
-docker-compose up --build
-
diff --git a/install/__init__.py b/utility/__init__.py
similarity index 100%
rename from install/__init__.py
rename to utility/__init__.py
diff --git a/utility/setup_and_run.sh b/utility/setup_and_run.sh
new file mode 100755
index 0000000..7188dc9
--- /dev/null
+++ b/utility/setup_and_run.sh
@@ -0,0 +1,49 @@
+#!/bin/bash
+
+# Function to check and install Miniconda if necessary
+check_miniconda() {
+    if ! [ -x "$(command -v conda)" ]; then
+        echo "Miniconda is not installed. Installing Miniconda..."
+        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
+        bash miniconda.sh -b -p $HOME/miniconda
+        export PATH="$HOME/miniconda/bin:$PATH"
+        echo "Miniconda installed."
+    else
+        echo "Miniconda is already installed."
+    fi
+}
+
+# Check if already in Nur directory with main.py
+if [ -d "Nur" ] && [ -f "Nur/main.py" ]; then
+    echo "Nur directory with main.py already exists. Skipping cloning."
+    cd Nur
+else
+    # Clone the GitHub repository if Nur directory doesn't exist
+    git clone https://github.com/MDGrey33/Nur.git
+    cd Nur
+fi
+
+# Check if the Miniconda environment already exists
+env_name="myenv"
+if conda info --envs | grep -q "$env_name"; then
+    echo "Miniconda environment '$env_name' already exists. Activating it."
+else
+    echo "Creating Miniconda environment '$env_name'."
+    conda create -n "$env_name" python=3.8 -y
+fi
+
+# Activate the Miniconda environment
+# Modify this depending on your shell compatibility
+source activate "$env_name"
+
+# Install Python dependencies
+if [ -f "requirements.txt" ]; then
+    echo "Installing Python dependencies."
+    pip install -r requirements.txt
+else
+    echo "No requirements.txt found. Skipping Python dependencies installation."
+fi
+
+# Start the Docker containers
+echo "Starting Docker containers."
+docker-compose up --build

a23fe7f91d985aeb9bad263e4193982d0a6de429
Author: Roland Abou Younes
Date: Mon Nov 27 13:15:23 2023 +0200
Message: removed app

diff --git a/app/__init__.py b/app/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/app/main.py b/app/main.py
deleted file mode 100644
index a3852f8..0000000
--- a/app/main.py
+++ /dev/null
@@ -1 +0,0 @@
-print("Fiat Lux")

ef5603ad6769d71d39f8dbdcf4aecfa512378dba
Author: Roland Abou Younes
Date: Mon Nov 27 12:46:46 2023 +0200
Message: adding git ignore

diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..f74efbe
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,3 @@
+/.idea/
+/credentials.py
+/configuration.py
\ No newline at end of file

a6eae1574aea6b068e0b10b40ecf82680f155570
Author: Roland Abou Younes
Date: Mon Nov 27 12:46:36 2023 +0200
Message: adding the setup script

diff --git a/install/setup_and_run.sh b/install/setup_and_run.sh
new file mode 100755
index 0000000..37cf46e
--- /dev/null
+++ b/install/setup_and_run.sh
@@ -0,0 +1,36 @@
+#!/bin/bash
+
+# Function to check if Miniconda is installed
+check_miniconda() {
+    if ! [ -x "$(command -v conda)" ]; then
+        echo "Miniconda is not installed. Installing Miniconda..."
+        # Replace this URL with the latest Miniconda installer for your platform
+        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
+        bash miniconda.sh -b -p $HOME/miniconda
+        export PATH="$HOME/miniconda/bin:$PATH"
+        echo "Miniconda installed."
+    else
+        echo "Miniconda is already installed."
+    fi
+}
+
+# Step 1: Check and install Miniconda if necessary
+check_miniconda
+
+# Step 2: Clone the GitHub repository
+git clone https://github.com/MDGrey33/Nur.git
+cd Nur
+
+# Step 3: Create a Miniconda environment
+conda create -n myenv python=3.8 -y
+
+# Step 4: Activate the Miniconda environment
+# You may need to modify this depending on how your shell handles script execution
+source activate myenv
+
+# Step 5: Install Python dependencies
+pip install -r requirements.txt
+
+# Step 6: Start the Docker containers
+docker-compose up --build
+

c2ea88d39cbc943f16c79c3a8184770e32002d70
Author: Roland Abou Younes
Date: Mon Nov 27 12:20:37 2023 +0200
Message: Added git ignore

diff --git a/.idea/.gitignore b/.idea/.gitignore
new file mode 100644
index 0000000..26d3352
--- /dev/null
+++ b/.idea/.gitignore
@@ -0,0 +1,3 @@
+# Default ignored files
+/shelf/
+/workspace.xml

ce609f6638345fb200e44993d61aaec3c9e9ba83
Author: Roland Abou Younes
Date: Mon Nov 27 12:20:03 2023 +0200
Message: Created main package and install package and moved the files

diff --git a/app/__init__.py b/app/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/app/main.py b/app/main.py
new file mode 100644
index 0000000..a3852f8
--- /dev/null
+++ b/app/main.py
@@ -0,0 +1 @@
+print("Fiat Lux")
diff --git a/install/__init__.py b/install/__init__.py
new file mode 100644
index 0000000..e69de29

7e0735741d86de307ea223b0208ae441142acd9b
Author: Roland Jones
Date: Mon Nov 27 12:07:47 2023 +0200
Message: Create docker-compose.yml

diff --git a/docker-compose.yml b/docker-compose.yml
new file mode 100644
index 0000000..354b9e2
--- /dev/null
+++ b/docker-compose.yml
@@ -0,0 +1,24 @@
+version: '3'
+services:
+  zookeeper:
+    image: wurstmeister/zookeeper
+    ports:
+      - "2181:2181"
+
+  kafka:
+    image: wurstmeister/kafka
+    ports:
+      - "9092:9092"
+    environment:
+      KAFKA_ADVERTISED_HOST_NAME: kafka
+      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
+    volumes:
+      - /var/run/docker.sock:/var/run/docker.sock
+
+  python-app:
+    build: .
+    volumes:
+      - .:/app
+    depends_on:
+      - kafka
+      - zookeeper

01c32c20655733e62983952a02cb01c02dc2db03
Author: Roland Jones
Date: Mon Nov 27 12:06:14 2023 +0200
Message: Create Dockerfile

diff --git a/Dockerfile b/Dockerfile
new file mode 100644
index 0000000..795af35
--- /dev/null
+++ b/Dockerfile
@@ -0,0 +1,14 @@
+# Use Python 3.8 image
+FROM python:3.8
+
+# Set the working directory in the container
+WORKDIR /app
+
+# Copy the application files into the container at /app
+COPY . /app
+
+# Install any dependencies
+# RUN pip install -r requirements.txt
+
+# Command to run on container start
+CMD ["python", "main.py"]

8a54868cc439342797f31a53a486093394993083
Author: Roland Jones
Date: Mon Nov 27 10:08:44 2023 +0200
Message: Create main.py

diff --git a/main.py b/main.py
new file mode 100644
index 0000000..cceb525
--- /dev/null
+++ b/main.py
@@ -0,0 +1 @@
+print("Let there be light")

7d81b628035afc8d2a22c0e4a3d0e2b7d498b7e2
Author: Roland Jones
Date: Mon Nov 27 10:05:15 2023 +0200
Message: Initial commit

diff --git a/LICENSE b/LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/README.md b/README.md
new file mode 100644
index 0000000..d0037d6
--- /dev/null
+++ b/README.md
@@ -0,0 +1,2 @@
+# Nur
+The self actualizing documentation framework that heals its knowledge gaps as naturally as a ray of light